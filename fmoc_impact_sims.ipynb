{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Gradient Boosting and Stochastic Models for Predicting FOMC Meetings Impact\n\n<a href=\"https://www.kaggle.com/code/addarm/bond-valuations\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a><a href=\"https://colab.research.google.com/github/adamd1985/quant_research/blob/main/bond_valuations.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>","metadata":{}},{"cell_type":"markdown","source":"![Bonds 2024.07.24](https://raw.githubusercontent.com/adamd1985/quant_research/main/images/bondbanner.png)\n\n<!-- @import \"[TOC]\" {cmd=\"toc\" depthFrom=1 depthTo=6 orderedList=false} -->","metadata":{}},{"cell_type":"markdown","source":"In this study, we use stochastic financial modeling and machine learning to estimated the impact of Federal Open Market Committee (FOMC) announcements on the market. Specifically, we implement the Heston Stochastic Volatility Model and Ornstein-Uhlenbeck process to simulate market movements and analyze their efficacy in forecasting market reactions. \n\nAdditionally, we utilize machine learning techniques, including Gradient Boosting Classifiers and Bayesian Optimization, to enhance prediction accuracy.","metadata":{}},{"cell_type":"markdown","source":"## Notebook Setup","metadata":{}},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime, timedelta\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport logging\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nINSTALL_DEPS = False\nif INSTALL_DEPS:\n    # If Kaggle or Colab, you have to manage these. If local, install all\n    %pip install numpy==1.23.4\n    %pip install pandas==2.2.0\n\n    IN_KAGGLE = IN_COLAB = False\ntry:\n    import google.colab\n    from google.colab import drive\n\n    drive.mount(\"/content/drive\")\n    DATA_PATH = \"/content/drive/MyDrive/fomc_dataset\"\n    MODEL_PATH = \"/content/drive/MyDrive/models\"\n    IN_COLAB = True\n    print(\"Colab!\")\nexcept:\n    IN_COLAB = False\nif \"KAGGLE_KERNEL_RUN_TYPE\" in os.environ and not IN_COLAB:\n    print(\"Running in Kaggle...\")\n    for dirname, _, filenames in os.walk(\"/kaggle/input\"):\n        for filename in filenames:\n            print(os.path.join(dirname, filename))\n    DATA_PATH = \"/kaggle/input/fomc-dataset-2000-2024\"\n    MODEL_PATH = \".\"\n    IN_KAGGLE = True\n    print(\"Kaggle!\")\nelif not IN_COLAB:\n    IN_KAGGLE = False\n    DATA_PATH = \"./data/macro\"\n    MODEL_PATH = \"./models/fomc/\"\n    print(\"running localhost!\")","metadata":{"execution":{"iopub.status.busy":"2024-07-29T12:28:41.296817Z","iopub.execute_input":"2024-07-29T12:28:41.297304Z","iopub.status.idle":"2024-07-29T12:28:41.316394Z","shell.execute_reply.started":"2024-07-29T12:28:41.297262Z","shell.execute_reply":"2024-07-29T12:28:41.315165Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Running in Kaggle...\n/kaggle/input/fomc-dataset-2000-2024/FOMC_dataset.csv\nKaggle!\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Heston Stochastic Volatility Model\n\nThe Heston model is a popular stochastic volatility model used to describe the evolution of an underlying asset with stochastic volatility. The model assumes that the underlying asset price $S_t$ and its variance $V_t$ follow the stochastic differential equations:\n\n$$\n\\begin{aligned}\n    dS_t &= \\mu S_t dt + \\sqrt{V_t} S_t dW_{1,t}, \\\\\n    dV_t &= \\kappa (\\theta - V_t) dt + \\sigma \\sqrt{V_t} dW_{2,t}, \\\\\n    dW_{1,t} dW_{2,t} &= \\rho dt,\n\\end{aligned}\n$$\n\nwhere:\n- $S_t$ is the asset price at time $t$,\n- $V_t$ is the variance at time $t$,\n- $\\mu$ is the drift rate of the asset,\n- $\\kappa$ is the rate of mean reversion of variance,\n- $\\gamma$ is the long-term variance,\n- $\\sigma$ is the volatility of the variance process,\n- $W_{1,t}$ and $W_{2,t}$ are Wiener processes with correlation $\\rho$.\n \nThis continuous-time Equations has 2 theoretical SDEs that describe how the price and variance evolve continuously over time, though it needs to be descritized to be applied in a numerical solution","metadata":{}},{"cell_type":"markdown","source":"## Discretized Heston Model\n\nUsing the Euler-Maruyama method to discretize the model:\n\n$$\nX_t = X_{t-1} + \\mu X_{t-1} \\, dt + \\sqrt{\\zeta_{t-1}} X_{t-1} \\sqrt{dt} Z_{t1}\n$$\n\nWhere:\n- $X_t$ is the stock price at time $t$.\n- $\\mu$ is the drift term, representing the expected return of the stock.\n- $dt$ is the size of the time step.\n- $\\zeta_{t-1}$ is the variance at time $t-1$.\n- $Z_{t1}$ is a standard normal random variable representing the random shock to the stock price.\n\nSame for the variance equation:\n\n$$\n\\zeta_t = \\zeta_{t-1} + \\kappa (\\gamma - \\zeta_{t-1}) \\, dt + \\sigma_\\zeta \\sqrt{\\zeta_{t-1}} \\sqrt{dt} Z_{t2}\n$$\n\nWhere:\n- $\\zeta_t$ is the variance at time $t$.\n- $\\kappa$ is the rate of mean reversion, dictating how quickly the variance reverts to the long-run average $\\gamma$.\n- $\\gamma$ is the long-run average variance.\n- $\\sigma_\\zeta$ is the volatility of the variance process.\n- $Z_{t2}$ is a standard normal random variable representing the random shock to the variance.\n\nin both equations $dt$ is the timestep, calculated as:\n\n$$\ndt = \\frac{T}{N}\n$$\n\nWhere:\n- $T$ is the annualized Time Horizon, in years.\n- $N$ is the number of discrete steps (in days) the time horizon is divided.\n\nThe random variables $Z_{t1}$ and $Z_{t2}$ are correlated with correlation coefficient $\\rho$. We can generate correlated random variables as follows:\n\n$$\nZ_{t1} = G_{t1}\n$$\n\n$$\nZ_{t2} = \\rho G_{t1} + \\sqrt{1 - \\rho^2} G_{t2}\n$$\n\nWhere:\n- $\\rho$ is the correlation between the stock price and variance processes.\n- $Z_{t2}$ is constructed to have the desired correlation with $Z_{t1}$.\n- $G_{t2}$ ensures that $Z_{t2}$ is also a standard normal random variable.","metadata":{}},{"cell_type":"markdown","source":"### Jumps with  Poisson Process\n\nWe are simulating market impact, therefore there will be a shock in the Heston process which we should simulate using Poisson. The Poisson process is used to model the occurrence of random events over time:\n\n1. The probability of exactly one event occurring in a small interval of length $dt$ is $\\lambda dt + o(dt)$.\n2. The probability of more than one event occurring in a small interval of length $dt$ is $o(dt)$.\n3. The probability of no events occurring in a small interval of length $dt$ is $1 - \\lambda dt + o(dt)$.\n\nThe number of events $N(t)$ occurring up to time $t$ follows the Poisson distribution:\n$$\nP(N(t) = k) = \\frac{(\\lambda t)^k e^{-\\lambda t}}{k!}, \\quad k = 0, 1, 2, \\ldots\n$$\nwhere $k$ represents the number of events (or jumps) that occur up to time $t$, and $\\lambda$ is the frequency of jumps\n\nIn our Heston model, we include 2 new paramaters:\n\n1. **Jump Occurrence**: The Poisson process determines if a jump occurs during each time step.\n2. **Jump Magnitude**: When a jump occurs, the magnitude is typically modeled using a normal distribution.","metadata":{}},{"cell_type":"code","source":"def heston_model(X0, v0, mu, kappa, gamma, sigma_variance, rho, T, N, pos_jump_proba=0.05, neg_jump_proba=0.05, jump_mean=0.005, jump_std=0.015):\n    dt = T / N\n    X = np.zeros(N+1)\n    v = np.zeros(N+1)\n    X[0] = X0\n    v[0] = v0\n    pos_lambda = pos_jump_proba / dt\n    neg_lambda = neg_jump_proba / dt\n    for t in range(1, N+1):\n        Gt1 = np.random.normal(0, 1)\n        Gt2 = np.random.normal(0, 1)\n        Zt1 = Gt1\n        Zt2 = rho * Gt1 + np.sqrt(1 - rho**2) * Gt2\n\n        v[t] = v[t-1] + kappa * (gamma - v[t-1]) * dt + sigma_variance * np.sqrt(v[t-1]) * np.sqrt(dt) * Zt2\n        v[t] = max(v[t], 0)  # Ensure variance stays non-negative\n        X[t] = X[t-1] * np.exp((mu - 0.5 * v[t-1]) * dt + np.sqrt(v[t-1] * dt) * Zt1)\n\n        # Convert into rates per unit time interval\n        pos_jump_occurs = np.random.poisson(pos_lambda) > 0\n        neg_jump_occurs = np.random.poisson(neg_lambda) > 0\n\n        if pos_jump_occurs:\n            pos_jump_size = np.random.normal(jump_mean, jump_std)\n            X[t] *= np.exp(pos_jump_size)\n        elif neg_jump_occurs:\n            neg_jump_size = np.random.normal(jump_mean, jump_std)\n            X[t] *= np.exp(-neg_jump_size)\n\n    # Don't return X0 and V0\n    return X[1:], v[1:]","metadata":{"execution":{"iopub.status.busy":"2024-07-29T12:28:41.318169Z","iopub.execute_input":"2024-07-29T12:28:41.318493Z","iopub.status.idle":"2024-07-29T12:28:41.330410Z","shell.execute_reply.started":"2024-07-29T12:28:41.318467Z","shell.execute_reply":"2024-07-29T12:28:41.329328Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"Let's do some tests to see that our Heston gives sensible results:","metadata":{}},{"cell_type":"code","source":"X0 = 100  # Initial stock price\nvariance0 = 0.04  # Initial variance (square of initial volatility)\nmu = 0.05  # Drift of stock price\nkappa = 1.0  # Speed of mean reversion of variance\ntheta = 0.04  # Long run average variance\nsigma_variance = 0.2  # Volatility of variance\nrho = -0.7  # Correlation between the two Wiener processes\nT = 5 / 252  # Time horizon (1 week, represented as 5/252 years)\nN = 5  # Number of time steps (5 trading days)\n\n# Simulate without shocks\nX, variance = heston_model(\n    X0, variance0, mu, kappa, theta, sigma_variance, rho, T, N\n)\n\nX, variance","metadata":{"execution":{"iopub.status.busy":"2024-07-29T12:28:41.332842Z","iopub.execute_input":"2024-07-29T12:28:41.333310Z","iopub.status.idle":"2024-07-29T12:28:41.350123Z","shell.execute_reply.started":"2024-07-29T12:28:41.333267Z","shell.execute_reply":"2024-07-29T12:28:41.348729Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"(array([100.58354351, 102.67465019, 103.17057781, 104.78208805,\n        108.51415422]),\n array([0.03721972, 0.03743579, 0.03515271, 0.03453851, 0.03357025]))"},"metadata":{}}]},{"cell_type":"markdown","source":"# FOMC and Macro Datasets\n\nNote the probabilities for jumps in the Heston parameters. We need to supply those through historic data. Our aim is to use machine learning (ML) to supply such probabilities. \n\nThrough various data repositories, we built a dataset and did feature engineering to give an edge to our models:","metadata":{}},{"cell_type":"code","source":"macro_df = pd.read_csv(f\"{DATA_PATH}/FOMC_dataset.csv\")\n\nmacro_df['Date'] = pd.to_datetime(macro_df['Date'])\nmacro_df['Year'] = macro_df['Date'].dt.year\nmacro_df['Month'] = macro_df['Date'].dt.month\nmacro_df['Quarter'] = macro_df['Date'].dt.quarter\n\nmacro_df['Month_Sin'] = np.sin(2 * np.pi * macro_df['Month'] / 12)\nmacro_df['Month_Cos'] = np.cos(2 * np.pi * macro_df['Month'] / 12)\n\nmacro_df['YoY Industry Production'] = macro_df['Industry Production'].pct_change(periods=12).bfill()\nmacro_df['YoY PCE'] = macro_df['PCE'].pct_change(periods=12).bfill()\nmacro_df['YoY Retail Sales'] = macro_df['Retail Sales'].pct_change(periods=12).bfill()\nmacro_df['YoY Unemployment'] = macro_df['Unemployment'].pct_change(periods=12).bfill()\nmacro_df['YoY Wage Increase'] = macro_df['Wage Increase'].pct_change(periods=12).bfill()\nmacro_df['YoY Home Sales'] = macro_df['Home Sales'].pct_change(periods=12).bfill()\nmacro_df['YoY Retail Trade'] = macro_df['Retail Trade'].pct_change(periods=12).bfill()\nmacro_df['YoY Real GDP'] = macro_df['Real GDP'].pct_change(periods=12).bfill()\n\nmacro_df['MoM Industry Production'] = macro_df['Industry Production'].pct_change(periods=1).bfill()\nmacro_df['MoM PCE'] = macro_df['PCE'].pct_change(periods=1).bfill()\nmacro_df['MoM Retail Sales'] = macro_df['Retail Sales'].pct_change(periods=1).bfill()\nmacro_df['MoM Unemployment'] = macro_df['Unemployment'].pct_change(periods=1).bfill()\nmacro_df['MoM Wage Increase'] = macro_df['Wage Increase'].pct_change(periods=1).bfill()\nmacro_df['MoM Home Sales'] = macro_df['Home Sales'].pct_change(periods=1).bfill()\nmacro_df['MoM Retail Trade'] = macro_df['Retail Trade'].pct_change(periods=1).bfill()\nmacro_df['MoM Real GDP'] = macro_df['Real GDP'].pct_change(periods=1).bfill()\n\nmacro_df['CPI_Unemployment'] = macro_df['CPI'] * macro_df['Unemployment']\nmacro_df['CPI_Industry Production'] = macro_df['CPI'] * macro_df['Industry Production']\n\nlags = [1, 3, 6, 12]\nfor lag in lags:\n    macro_df[f'Industry Production Lag_{lag}'] = macro_df['Industry Production'].shift(lag).bfill()\n    macro_df[f'PCE Lag_{lag}'] = macro_df['PCE'].shift(lag).bfill()\n    macro_df[f'Retail Sales Lag_{lag}'] = macro_df['Retail Sales'].shift(lag).bfill()\n    macro_df[f'Unemployment Lag_{lag}'] = macro_df['Unemployment'].shift(lag).bfill()\n    macro_df[f'Wage Increase Lag_{lag}'] = macro_df['Wage Increase'].shift(lag).bfill()\n    macro_df[f'Home Sales Lag_{lag}'] = macro_df['Home Sales'].shift(lag).bfill()\n    macro_df[f'Retail Trade Lag_{lag}'] = macro_df['Retail Trade'].shift(lag).bfill()\n    macro_df[f'Real GDP Lag_{lag}'] = macro_df['Real GDP'].shift(lag).bfill()\n\nmacro_df.sort_values(by=\"Date\", ascending=True)\nprint(f\"Shape: {macro_df.shape}\")\nmacro_df.tail(5)","metadata":{"execution":{"iopub.status.busy":"2024-07-29T12:28:41.351472Z","iopub.execute_input":"2024-07-29T12:28:41.351818Z","iopub.status.idle":"2024-07-29T12:28:41.491912Z","shell.execute_reply.started":"2024-07-29T12:28:41.351770Z","shell.execute_reply":"2024-07-29T12:28:41.490848Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Shape: (2492, 105)\n","output_type":"stream"},{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"      Unnamed: 0       Date         CPI  Industry Production         PCE  \\\n2487        2487 2024-03-01  312.332000              102.406  122.782000   \n2488        2488 2024-04-01  313.548000              102.433  123.109000   \n2489        2489 2024-05-01  314.069000              103.328  123.146000   \n2490        2490 2024-06-01  314.175000              103.994  123.243000   \n2491        2491 2024-07-01  314.489175              103.994  123.489486   \n\n      Retail Sales  Unemployment  Wage Increase  Fed Rate  Home Sales  ...  \\\n2487      225391.0           3.8          368.0      5.33       683.0  ...   \n2488      224350.0           3.9          368.0      5.33       730.0  ...   \n2489      224913.0           4.0          368.0      5.33       621.0  ...   \n2490      224988.0           4.1          368.0      5.33       617.0  ...   \n2491      224988.0           4.1          368.0      5.33       612.0  ...   \n\n      Retail Trade Lag_6  Real GDP Lag_6  Industry Production Lag_12  \\\n2487            608307.0       22679.255                     102.381   \n2488            606596.0       22679.255                     102.381   \n2489            606035.0       22758.752                     103.072   \n2490            608730.0       22758.752                     103.072   \n2491            601921.0       22758.752                     103.072   \n\n      PCE Lag_12  Retail Sales Lag_12  Unemployment Lag_12  \\\n2487     120.221             226521.0                  3.6   \n2488     120.221             226521.0                  3.6   \n2489     120.373             226640.0                  3.5   \n2490     120.373             226640.0                  3.5   \n2491     120.373             226640.0                  3.5   \n\n      Wage Increase Lag_12  Home Sales Lag_12 Retail Trade Lag_12  \\\n2487                 366.0              666.0            597826.0   \n2488                 366.0              666.0            597826.0   \n2489                 366.0              700.0            599037.0   \n2490                 366.0              700.0            599037.0   \n2491                 366.0              700.0            599037.0   \n\n      Real GDP Lag_12  \n2487        22490.692  \n2488        22490.692  \n2489        22490.692  \n2490        22490.692  \n2491        22490.692  \n\n[5 rows x 105 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>Date</th>\n      <th>CPI</th>\n      <th>Industry Production</th>\n      <th>PCE</th>\n      <th>Retail Sales</th>\n      <th>Unemployment</th>\n      <th>Wage Increase</th>\n      <th>Fed Rate</th>\n      <th>Home Sales</th>\n      <th>...</th>\n      <th>Retail Trade Lag_6</th>\n      <th>Real GDP Lag_6</th>\n      <th>Industry Production Lag_12</th>\n      <th>PCE Lag_12</th>\n      <th>Retail Sales Lag_12</th>\n      <th>Unemployment Lag_12</th>\n      <th>Wage Increase Lag_12</th>\n      <th>Home Sales Lag_12</th>\n      <th>Retail Trade Lag_12</th>\n      <th>Real GDP Lag_12</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2487</th>\n      <td>2487</td>\n      <td>2024-03-01</td>\n      <td>312.332000</td>\n      <td>102.406</td>\n      <td>122.782000</td>\n      <td>225391.0</td>\n      <td>3.8</td>\n      <td>368.0</td>\n      <td>5.33</td>\n      <td>683.0</td>\n      <td>...</td>\n      <td>608307.0</td>\n      <td>22679.255</td>\n      <td>102.381</td>\n      <td>120.221</td>\n      <td>226521.0</td>\n      <td>3.6</td>\n      <td>366.0</td>\n      <td>666.0</td>\n      <td>597826.0</td>\n      <td>22490.692</td>\n    </tr>\n    <tr>\n      <th>2488</th>\n      <td>2488</td>\n      <td>2024-04-01</td>\n      <td>313.548000</td>\n      <td>102.433</td>\n      <td>123.109000</td>\n      <td>224350.0</td>\n      <td>3.9</td>\n      <td>368.0</td>\n      <td>5.33</td>\n      <td>730.0</td>\n      <td>...</td>\n      <td>606596.0</td>\n      <td>22679.255</td>\n      <td>102.381</td>\n      <td>120.221</td>\n      <td>226521.0</td>\n      <td>3.6</td>\n      <td>366.0</td>\n      <td>666.0</td>\n      <td>597826.0</td>\n      <td>22490.692</td>\n    </tr>\n    <tr>\n      <th>2489</th>\n      <td>2489</td>\n      <td>2024-05-01</td>\n      <td>314.069000</td>\n      <td>103.328</td>\n      <td>123.146000</td>\n      <td>224913.0</td>\n      <td>4.0</td>\n      <td>368.0</td>\n      <td>5.33</td>\n      <td>621.0</td>\n      <td>...</td>\n      <td>606035.0</td>\n      <td>22758.752</td>\n      <td>103.072</td>\n      <td>120.373</td>\n      <td>226640.0</td>\n      <td>3.5</td>\n      <td>366.0</td>\n      <td>700.0</td>\n      <td>599037.0</td>\n      <td>22490.692</td>\n    </tr>\n    <tr>\n      <th>2490</th>\n      <td>2490</td>\n      <td>2024-06-01</td>\n      <td>314.175000</td>\n      <td>103.994</td>\n      <td>123.243000</td>\n      <td>224988.0</td>\n      <td>4.1</td>\n      <td>368.0</td>\n      <td>5.33</td>\n      <td>617.0</td>\n      <td>...</td>\n      <td>608730.0</td>\n      <td>22758.752</td>\n      <td>103.072</td>\n      <td>120.373</td>\n      <td>226640.0</td>\n      <td>3.5</td>\n      <td>366.0</td>\n      <td>700.0</td>\n      <td>599037.0</td>\n      <td>22490.692</td>\n    </tr>\n    <tr>\n      <th>2491</th>\n      <td>2491</td>\n      <td>2024-07-01</td>\n      <td>314.489175</td>\n      <td>103.994</td>\n      <td>123.489486</td>\n      <td>224988.0</td>\n      <td>4.1</td>\n      <td>368.0</td>\n      <td>5.33</td>\n      <td>612.0</td>\n      <td>...</td>\n      <td>601921.0</td>\n      <td>22758.752</td>\n      <td>103.072</td>\n      <td>120.373</td>\n      <td>226640.0</td>\n      <td>3.5</td>\n      <td>366.0</td>\n      <td>700.0</td>\n      <td>599037.0</td>\n      <td>22490.692</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 105 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"FEATURES = [\n    \"CPI\", \"Industry Production\", \"PCE\", \"Retail Sales\", \"Unemployment\",\n    \"Wage Increase\", \"Prior Fed Rate\", \"Home Sales\", \"Retail Trade\",\n    \"Real GDP\", \"Month\", \"Month_Sin\", \"Month_Cos\", \"Prior CPI\", \"Inflation\",\n    \"YoY Industry Production\", \"YoY PCE\", \"YoY Retail Sales\", \"YoY Unemployment\",\n    \"YoY Wage Increase\", \"YoY Home Sales\", \"YoY Retail Trade\", \"YoY Real GDP\",\n    \"MoM Industry Production\", \"MoM PCE\", \"MoM Retail Sales\", \"MoM Unemployment\",\n    \"MoM Wage Increase\", \"MoM Home Sales\", \"MoM Retail Trade\", \"MoM Real GDP\",\n    \"CPI_Unemployment\", \"CPI_Industry Production\",\n    \"Industry Production_Quarterly_Avg\", \"PCE_Quarterly_Avg\", \"Retail Sales_Quarterly_Avg\",\n    \"Unemployment_Quarterly_Avg\", \"Wage Increase_Quarterly_Avg\", \"Home Sales_Quarterly_Avg\",\n    \"Retail Trade_Quarterly_Avg\", \"Real GDP_Quarterly_Avg\"\n]","metadata":{"execution":{"iopub.status.busy":"2024-07-29T12:28:41.493891Z","iopub.execute_input":"2024-07-29T12:28:41.494235Z","iopub.status.idle":"2024-07-29T12:28:41.500656Z","shell.execute_reply.started":"2024-07-29T12:28:41.494207Z","shell.execute_reply":"2024-07-29T12:28:41.499457Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"# Machine Learning for Probabilities\n\nWe want to predict 2 things:\n- Class 0: Status Quo, this means the Fed keeps the rates or decreases them (what the market is pricing in).\n- Class 1: Rates Increase, inflation is persistant and the Fed will increase again the rates, which would lead to some market tormoil.\n\nFrom our macro data we will engineer this label by comparing the current Fed Rate with the next time step's Fed Rate:","metadata":{}},{"cell_type":"code","source":"macro_df['Fed Rate Increase'] = ((macro_df['Fed Rate'] - macro_df['Prior Fed Rate']) > 0).astype(int) # our label\n\ncounts = macro_df['Fed Rate Increase'].value_counts()\ncounts","metadata":{"execution":{"iopub.status.busy":"2024-07-29T12:28:41.502136Z","iopub.execute_input":"2024-07-29T12:28:41.502518Z","iopub.status.idle":"2024-07-29T12:28:41.516661Z","shell.execute_reply.started":"2024-07-29T12:28:41.502486Z","shell.execute_reply":"2024-07-29T12:28:41.515554Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"Fed Rate Increase\n0    1270\n1    1222\nName: count, dtype: int64"},"metadata":{}}]},{"cell_type":"markdown","source":"Note there is a slight imbalance between meetings that led to increases and keeping the status quo, for this we will use Synthetic Minority Oversampling Technique (SMOTE) to balance them out again.\n\nThe dataset has 2492 rows and 105, emsembles or tree classifiers perform well on such small datasets.\nWe will run some trails with a subset of such models, using out of the box parameters and stratified cross validation.\n\nThe models are:\n- Naive Bayes\n- State Vector Machines (SVM)\n- Logistic Regression\n- Decision Tree\n- Random Forest\n- Gradient Boosting\n\nBelow is the python code to scale the data, cross validate and test these:","metadata":{}},{"cell_type":"code","source":"import json\nimport os\nimport pandas as pd\nimport joblib\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\nfrom sklearn.metrics import accuracy_score, recall_score, f1_score, precision_score\nfrom sklearn.utils.class_weight import compute_sample_weight\nfrom sklearn.preprocessing import StandardScaler\nfrom imblearn.over_sampling import SMOTE\n\nX = macro_df[FEATURES]\ny = macro_df['Fed Rate Increase']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nsmote = SMOTE(random_state=42)\nX_train, y_train = smote.fit_resample(X_train, y_train)\nsample_weights = compute_sample_weight(class_weight='balanced', y=y_train)\n\nmodels = {\n    \"Naive Bayes\": GaussianNB(),\n    \"SVM\": SVC(probability=True),\n    \"Logistic Regression\": LogisticRegression(max_iter=10000),\n    \"Decision Tree\": DecisionTreeClassifier(),\n    \"Random Forest\": RandomForestClassifier(),\n    \"Gradient Boosting\": GradientBoostingClassifier()\n}\n\nbest_models = {}\nmetrics = []\n\nif os.path.exists(f\"{MODEL_PATH}/fomc_all_metrics.json\"):\n    metrics_df = pd.read_json(f\"{MODEL_PATH}/fomc_all_metrics.json\")\nelse:\n    scaler = StandardScaler()\n    X_train_scaled = scaler.fit_transform(X_train)\n    X_test_scaled = scaler.transform(X_test)\n\n    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\n    for model_name, model in tqdm(models.items(), desc=\"Training Models...\"):\n        model.fit(X_train_scaled, y_train, sample_weight=sample_weights)\n        best_models[model_name] = model\n        y_pred = model.predict(X_test_scaled)\n\n        accuracy = accuracy_score(y_test, y_pred)\n        precision = precision_score(y_test, y_pred, average='weighted')\n        recall = recall_score(y_test, y_pred, average='weighted')\n        f1 = f1_score(y_test, y_pred, average='weighted')\n\n        X_scaled = scaler.fit_transform(X)  # Scale the entire dataset for cross-validation\n        cv_scores = cross_val_score(model, X_scaled, y, cv=cv, scoring='precision_weighted')\n        cv_mean = cv_scores.mean()\n\n        metrics.append({\n            \"Model\": model_name,\n            \"Accuracy\": accuracy,\n            \"Precision\": precision,\n            \"Recall\": recall,\n            \"F1 Score\": f1,\n            \"CV Score\": cv_mean\n        })\n\n    metrics_df = pd.DataFrame(metrics)\n    metrics_df.to_json(f\"{MODEL_PATH}/fomc_all_metrics.json\")\n\nmetrics_df = metrics_df.sort_values(by=[\"CV Score\", \"Precision\"], ascending=False)\nmetrics_df","metadata":{"execution":{"iopub.status.busy":"2024-07-29T12:28:41.518867Z","iopub.execute_input":"2024-07-29T12:28:41.519295Z","iopub.status.idle":"2024-07-29T12:28:41.579858Z","shell.execute_reply.started":"2024-07-29T12:28:41.519257Z","shell.execute_reply":"2024-07-29T12:28:41.578771Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"                 Model  Accuracy  Precision    Recall  F1 Score  CV Score\n4        Random Forest  0.533066   0.532617  0.533066  0.532480  0.576705\n5    Gradient Boosting  0.513026   0.513525  0.513026  0.513026  0.575905\n3        Decision Tree  0.531062   0.531147  0.531062  0.531096  0.574991\n0          Naive Bayes  0.496994   0.508360  0.496994  0.433709  0.546882\n1                  SVM  0.494990   0.496351  0.494990  0.493625  0.543152\n2  Logistic Regression  0.496994   0.498425  0.496994  0.495525  0.534117","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Model</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1 Score</th>\n      <th>CV Score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>4</th>\n      <td>Random Forest</td>\n      <td>0.533066</td>\n      <td>0.532617</td>\n      <td>0.533066</td>\n      <td>0.532480</td>\n      <td>0.576705</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Gradient Boosting</td>\n      <td>0.513026</td>\n      <td>0.513525</td>\n      <td>0.513026</td>\n      <td>0.513026</td>\n      <td>0.575905</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Decision Tree</td>\n      <td>0.531062</td>\n      <td>0.531147</td>\n      <td>0.531062</td>\n      <td>0.531096</td>\n      <td>0.574991</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>Naive Bayes</td>\n      <td>0.496994</td>\n      <td>0.508360</td>\n      <td>0.496994</td>\n      <td>0.433709</td>\n      <td>0.546882</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>SVM</td>\n      <td>0.494990</td>\n      <td>0.496351</td>\n      <td>0.494990</td>\n      <td>0.493625</td>\n      <td>0.543152</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Logistic Regression</td>\n      <td>0.496994</td>\n      <td>0.498425</td>\n      <td>0.496994</td>\n      <td>0.495525</td>\n      <td>0.534117</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"The top performing was the GBC, no surprise there, these algos are excellent for tabular data.\n\n## Fine-Tune the Gradient Boosting Classifier\n\nNow we will sampling the models hyperparameters using the bayes optimization library to find the right parameters.\n\nThe library will create gaussian distribution to maximize the function we will provide it, and using exploration and exploitation will find the upper limits of the parameters. The problem of this library is that it only works with continuous parameters, and discrete or categorical parameters will need the more traditional gridsearch.\n\nIt's also fast!","metadata":{}},{"cell_type":"code","source":"from bayes_opt import BayesianOptimization\nfrom bayes_opt.logger import JSONLogger\nfrom bayes_opt.event import Events\nfrom bayes_opt.util import load_logs\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score\nfrom sklearn.metrics import accuracy_score, recall_score, f1_score, roc_auc_score, precision_score, roc_curve\n\ndef max_function(n_estimators, max_depth, learning_rate, min_samples_split, min_samples_leaf, sample_weights=None):\n    model = GradientBoostingClassifier(\n        n_estimators=int(n_estimators),\n        max_depth=int(max_depth),\n        learning_rate=learning_rate,\n        min_samples_split=int(min_samples_split),\n        min_samples_leaf=int(min_samples_leaf),\n        random_state=42\n    )\n    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n    scores = cross_val_score(model, X_train, y_train, cv=cv, scoring='precision')\n    return scores.mean()\n\nparam_bounds = {\n    'n_estimators': (20, 1000),\n    'max_depth': (2, 10),\n    'learning_rate': (0.001, 0.5),\n    'min_samples_split': (2, 30),\n    'min_samples_leaf': (1, 20),\n}\n\nif os.path.exists(f\"{MODEL_PATH}/fomc_model.joblib\"):\n    best_model = joblib.load(f\"{MODEL_PATH}/fomc_model.joblib\")\n    scaler = joblib.load(f\"{MODEL_PATH}/fomc_model_scaler.joblib\")\n    with open(f\"{MODEL_PATH}/fomc_model_params.json\", 'r') as file:\n        best_params = json.load(file)\n    metrics_df = pd.read_json(f\"{MODEL_PATH}/fomc_model_metrics.json\")\n\n    y_pred = best_model.predict(X_test)\n    y_pred_proba = best_model.predict_proba(X_test)[:, 1]\nelse:\n    scaler = StandardScaler()\n    X_train_scaled  = scaler.fit_transform(X_train)\n    X_test_scaled = scaler.transform(X_test)\n\n    optimizer = BayesianOptimization(\n        f=max_function,\n        pbounds=param_bounds,\n        random_state=42,\n        verbose=2\n    )\n    if os.path.exists(f\"{MODEL_PATH}/cv_logs.log\"):\n        load_logs(optimizer, logs=[f\"{MODEL_PATH}/cv_logs.log\"])\n    optimizer.maximize(\n        init_points=10,\n        n_iter=20,\n    )\n    logger = JSONLogger(path=f\"{MODEL_PATH}/cv_logs.log\")\n    optimizer.subscribe(Events.OPTIMIZATION_STEP, logger)\n    best_params = optimizer.max['params']\n    best_params['n_estimators'] = int(best_params['n_estimators'])\n    best_params['max_depth'] = int(best_params['max_depth'])\n    best_params['min_samples_split'] = int(best_params['min_samples_split'])\n    best_params['min_samples_leaf'] = int(best_params['min_samples_leaf'])\n\n    best_model = GradientBoostingClassifier(\n        **best_params,\n        random_state=42\n    )\n    best_model.fit(X_train_scaled, y_train, sample_weight=sample_weights)\n\n    y_pred = best_model.predict(X_test_scaled)\n    y_pred_proba = best_model.predict_proba(X_test_scaled)[:, 1]\n\n    accuracy = accuracy_score(y_test, y_pred)\n    recall = recall_score(y_test, y_pred)\n    f1 = f1_score(y_test, y_pred)\n    precision = precision_score(y_test, y_pred)\n    auc = roc_auc_score(y_test, y_pred_proba)\n\n    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n    cv_scores = cross_val_score(best_model, X_train_scaled, y_train, cv=cv, scoring='precision')\n    cv_mean = cv_scores.mean()\n\n    metrics = {\n        \"Model\": \"Gradient Boosting Classifier\",\n        \"Accuracy\": accuracy,\n        \"Recall\": recall,\n        \"Precision\": precision,\n        \"F1 Score\": f1,\n        \"AUC\": auc,\n        \"CV Score\": cv_mean\n    }\n\n    metrics_df = pd.DataFrame([metrics])\n    with open(f\"{MODEL_PATH}/fomc_model_params.json\", 'w') as file:\n        json.dump(best_params, file)\n    metrics_df.to_json(f\"{MODEL_PATH}/fomc_model_metrics.json\")\n    joblib.dump(best_model, f\"{MODEL_PATH}/fomc_model.joblib\")\n    joblib.dump(scaler, f\"{MODEL_PATH}/fomc_model_scaler.joblib\")\nprint(\"Best model parameters:\", best_params)\nmetrics_df","metadata":{"execution":{"iopub.status.busy":"2024-07-29T12:28:41.581265Z","iopub.execute_input":"2024-07-29T12:28:41.581612Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"|   iter    |  target   | learni... | max_depth | min_sa... | min_sa... | n_esti... |\n-------------------------------------------------------------------------------------\n| \u001b[30m1         | \u001b[30m0.6057    | \u001b[30m0.1879    | \u001b[30m9.606     | \u001b[30m14.91     | \u001b[30m18.76     | \u001b[30m172.9     |\n| \u001b[30m2         | \u001b[30m0.602     | \u001b[30m0.07884   | \u001b[30m2.465     | \u001b[30m17.46     | \u001b[30m18.83     | \u001b[30m713.9     |\n| \u001b[30m3         | \u001b[30m0.5981    | \u001b[30m0.01127   | \u001b[30m9.759     | \u001b[30m16.82     | \u001b[30m7.945     | \u001b[30m198.2     |\n| \u001b[30m4         | \u001b[30m0.6053    | \u001b[30m0.09252   | \u001b[30m4.434     | \u001b[30m10.97     | \u001b[30m14.09     | \u001b[30m305.4     |\n| \u001b[35m5         | \u001b[35m0.607     | \u001b[35m0.3063    | \u001b[35m3.116     | \u001b[35m6.551     | \u001b[35m12.26     | \u001b[35m466.9     |\n| \u001b[30m6         | \u001b[30m0.6026    | \u001b[30m0.3928    | \u001b[30m3.597     | \u001b[30m10.77     | \u001b[30m18.59     | \u001b[30m65.52     |\n| \u001b[30m7         | \u001b[30m0.596     | \u001b[30m0.3042    | \u001b[30m3.364     | \u001b[30m2.236     | \u001b[30m28.57     | \u001b[30m966.3     |\n| \u001b[30m8         | \u001b[30m0.5928    | \u001b[30m0.4044    | \u001b[30m4.437     | \u001b[30m2.856     | \u001b[30m21.16     | \u001b[30m451.3     |\n| \u001b[30m9         | \u001b[30m0.6034    | \u001b[30m0.0619    | \u001b[30m5.961     | \u001b[30m1.653     | \u001b[30m27.46     | \u001b[30m273.6     |\n| \u001b[30m10        | \u001b[30m0.6062    | \u001b[30m0.3316    | \u001b[30m4.494     | \u001b[30m10.88     | \u001b[30m17.31     | \u001b[30m201.2     |\n| \u001b[35m11        | \u001b[35m0.6118    | \u001b[35m0.3368    | \u001b[35m3.147     | \u001b[35m6.581     | \u001b[35m12.29     | \u001b[35m467.0     |\n| \u001b[30m12        | \u001b[30m0.6029    | \u001b[30m0.374     | \u001b[30m3.184     | \u001b[30m6.619     | \u001b[30m12.33     | \u001b[30m467.0     |\n| \u001b[30m13        | \u001b[30m0.611     | \u001b[30m0.08105   | \u001b[30m9.25      | \u001b[30m19.16     | \u001b[30m9.414     | \u001b[30m624.4     |\n| \u001b[30m14        | \u001b[30m0.5911    | \u001b[30m0.07982   | \u001b[30m2.998     | \u001b[30m16.19     | \u001b[30m4.691     | \u001b[30m515.6     |\n| \u001b[35m15        | \u001b[35m0.6133    | \u001b[35m0.3182    | \u001b[35m3.163     | \u001b[35m6.53      | \u001b[35m12.3      | \u001b[35m467.0     |\n| \u001b[30m16        | \u001b[30m0.6043    | \u001b[30m0.2674    | \u001b[30m3.159     | \u001b[30m6.574     | \u001b[30m12.28     | \u001b[30m467.0     |\n| \u001b[35m17        | \u001b[35m0.614     | \u001b[35m0.368     | \u001b[35m3.157     | \u001b[35m6.515     | \u001b[35m12.31     | \u001b[35m467.0     |\n| \u001b[30m18        | \u001b[30m0.613     | \u001b[30m0.3113    | \u001b[30m3.181     | \u001b[30m6.518     | \u001b[30m12.36     | \u001b[30m466.9     |\n| \u001b[30m19        | \u001b[30m0.6064    | \u001b[30m0.3714    | \u001b[30m3.16      | \u001b[30m6.518     | \u001b[30m12.31     | \u001b[30m467.0     |\n| \u001b[30m20        | \u001b[30m0.6078    | \u001b[30m0.03835   | \u001b[30m8.451     | \u001b[30m14.77     | \u001b[30m20.05     | \u001b[30m368.1     |\n| \u001b[30m21        | \u001b[30m0.6008    | \u001b[30m0.4711    | \u001b[30m8.554     | \u001b[30m3.527     | \u001b[30m17.79     | \u001b[30m151.7     |\n","output_type":"stream"}]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, roc_curve\n\ndef classification_plots(X_test, y_test, best_model, scaler, metrics_df):\n    X_test_scaled = scaler.transform(X_test)\n    y_pred = best_model.predict(X_test_scaled)\n    y_pred_proba = best_model.predict_proba(X_test_scaled)[:, 1]\n\n    fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n\n    # ROC Curve\n    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n    auc = metrics_df['AUC'].iloc[-1]\n    ax[0].plot(fpr, tpr, label=f\"Classifier (AUC={auc:.2f})\")\n    ax[0].plot([0, 1], [0, 1], 'k--')\n    ax[0].set_xlabel('False Positive Rate')\n    ax[0].set_ylabel('True Positive Rate')\n    ax[0].set_title('ROC Curve')\n    ax[0].legend(loc='best')\n\n    # Confusion Matrix\n    cm = confusion_matrix(y_test, y_pred)\n    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Status Quo', 'Rates Increase'])\n    disp.plot(cmap='Blues', ax=ax[1])\n    ax[1].set_title('Confusion Matrix')\n\n    plt.tight_layout()\n    plt.show()\n\nclassification_plots(X_test, y_test, best_model, scaler, metrics_df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The confusion matrix and ROC curves show a worrisome story, there are quite a number of false positives and negatives. Our GBC has a 60% precisioon rate though, this is better than random chance but far from perfect.\n\nFOMC outcomes involve large ammount of data taken from many economic domains, so we are doing a best effort to estimate the outcome here.\n\nBelow we test the rubustness of the classifier by purturbing it with gaussian noise:","metadata":{}},{"cell_type":"code","source":"def perturb_gaussiannoise(X, noise_level=0.01):\n    sigma = noise_level * np.std(X)\n    noise = np.random.normal(0, sigma, X.shape)\n    return X + noise\n\ndef test_robustness():\n    X_train_scaled  = scaler.transform(X_train)\n    X_test_scaled = scaler.transform(X_test)\n\n    X_train_pert = perturb_gaussiannoise(X_train_scaled)\n    X_test_pert = perturb_gaussiannoise(X_test_scaled)\n    y_pred = best_model.predict(X_test_pert)\n\n    accuracy = accuracy_score(y_test, y_pred)\n    recall = recall_score(y_test, y_pred)\n    f1 = f1_score(y_test, y_pred)\n    precision = precision_score(y_test, y_pred)\n    auc = roc_auc_score(y_test, y_pred_proba)\n\n    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n    cv_scores = cross_val_score(best_model, X_train_pert, y_train, cv=cv, scoring='precision')\n    cv_mean = cv_scores.mean()\n    perturb_metrics = {\n        \"Model\": \"Gradient Boosting Classifier (Perturbed)\",\n        \"Accuracy\": accuracy,\n        \"Recall\": recall,\n        \"Precision\": precision,\n        \"F1 Score\": f1,\n        \"AUC\": auc,\n        \"CV Score\": cv_mean\n    }\n\n    new_metrics_df = pd.concat([metrics_df, pd.DataFrame([perturb_metrics])], ignore_index=True)\n    return new_metrics_df\n\ntest_robustness()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The model is robust, with minor gaussian noise it had small degredation in the classification scores, but still better than random chance.\n\nFinally, we have a look at what the model deems important: PCE, inflation and unemployment are economically sound, as this is what the Fed has been focusing during these times of inflation. Retail and GDP not so much, which might be relevant for the next FOMC (at the time of writing 31st July 2024) as there was a GDP surprise of 2.8 when the expected was 2.1.","metadata":{}},{"cell_type":"code","source":"importances = best_model.feature_importances_\nindices = np.argsort(importances)[::-1]\nfeature_ranking = pd.DataFrame({\n    'Feature': [FEATURES[i] for i in indices],\n    'Importance': importances[indices]\n})\n\n\nsorted_feature_ranking = feature_ranking.sort_values(by=\"Importance\", ascending=True)\ntop_features = sorted_feature_ranking.tail(5)\nbottom_features = sorted_feature_ranking.head(5)\n\ncombined_df = pd.concat([bottom_features.reset_index(drop=True), top_features.reset_index(drop=True)], axis=1)\ncombined_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Market Data\n\nNow we get the market data, we will use the ETF SPY as proxy for the market, we won't use use the SnP index itself due to its large numbers which might skew the simulations.","metadata":{}},{"cell_type":"code","source":"import yfinance as yf\n\nSPY_TICKER = \"SPY\" # SnP 500 Index\n\nmacro_df['FOMC Meeting'] = pd.to_datetime(macro_df['FOMC Meeting'], errors='coerce')\nmacro_df['FOMC Meeting'] = macro_df['FOMC Meeting'].dt.tz_localize(None)\nfiltered_meetings = macro_df[(macro_df['FOMC Meeting'] >= '2023-06-01') & (macro_df['FOMC Meeting'] <= '2024-08-01')]\nfiltered_meetings = filtered_meetings.dropna().drop_duplicates()\n\nmarket_df = yf.download(SPY_TICKER, start='2020-01-01', end='2024-07-01', interval='1d')\nmarket_df.index = market_df.index.tz_localize(None)\nmarket_df.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Going back to the Heston, most of its parameters can sensibly be estimated from our data. With the exception of Kappa, Kappa is the speed of mean reversion and that needs its own model to estimate. For this we will use the Ornstein-Uhlenbeck.","metadata":{}},{"cell_type":"markdown","source":"## Ornstein-Uhlenbeck (OU) Process\n\nAn Ornstein–Uhlenbeck process is defined by this SDE:\n\n$$\ndx_t = \\gamma(\\mu - x_t)dt + \\sigma dW_t\n$$\nWhere:\n- $\\gamma$ is the rate of mean reversion.\n- $\\mu$ is the long-term mean.\n- $\\sigma$ is the volatility.\n- $dW_t$ is the increment of a Wiener process (Brownian motion).\n\nWith the help of the Itô lemma, we can find a continuous-time solution:\n\n$$\nx_t = x_0 e^{-\\gamma t} + \\mu(1 - e^{-\\gamma t}) + \\sigma \\int_0^t e^{-\\gamma(t-s)} dW_s\n$$\n\nIts first two moments of its equilibrium distribution become:\n\n$$\nE[x_t] = x_0 e^{-\\gamma t} + \\mu(1 - e^{-\\gamma t}) \\rightarrow \\mu\n$$\nAs $t \\rightarrow \\infty$, $E[x_t]$ approaches $\\mu$, showing that the process reverts to the mean $\\mu$ over time.\n\n$$\nVar[x_t] = \\frac{\\sigma^2}{2\\gamma} (1 - e^{-2\\gamma t}) \\rightarrow \\frac{\\sigma^2}{2\\gamma}\n$$\nAs $t \\rightarrow \\infty, Var[x_t]$ approaches $\\frac{\\sigma^2}{2\\gamma}$, indicating the long-term variance of the process.\n\nIts half-life is defined as the (expected) time to return half way to its mean, or:\n\n$$\nE[x_{\\kappa}] - \\mu = \\frac{x_0 - \\mu}{2}\n$$\n\nSubstituting the expectation at time $\\kappa$ with the discretized form, its left-hand-side becomes:\n$$\nE[x_{\\kappa}] - \\mu = x_0 e^{-\\gamma \\kappa} + \\mu(1 - e^{-\\gamma \\kappa}) - \\mu \n= e^{-\\gamma \\kappa} (x_0 - \\mu)\n$$\n\nSolving it with right-hand-side gives the half-life $\\kappa$:\n\n$$\ne^{-\\gamma \\kappa} (x_0 - \\mu) = \\frac{x_0 - \\mu}{2} \\implies \\kappa = \\frac{−\\log(2)}{\\gamma}\n$$\n\nA large $\\gamma$ is a short half life, having strong mean-reversion effect.\n","metadata":{}},{"cell_type":"markdown","source":"We still have to find $\\gamma$. Using the Euler-Maruyama method, for a small discrete time step $\\Delta t$:\n\n$$\nx_{t + \\Delta t} - x_t \\approx \\gamma(\\mu - x_t)\\Delta t + \\sigma \\sqrt{\\Delta t} \\epsilon_t\n$$\n\nRewriting for $x_{t+1}$:\n\n$$\nx_{t+1} = x_t + \\gamma(\\mu - x_t)\\Delta t + \\sigma \\sqrt{\\Delta t} \\epsilon_t \n$$\n\nSimplifying the expression:\n\n$$\nx_{t+1} = \\gamma \\mu \\Delta t + (1 - \\gamma \\Delta t)x_t + \\sigma \\sqrt{\\Delta t} \\epsilon_t \n$$\n\nWe can rewrite this equation in a linear regression form for 1 time step $\\Delta t=1$:\n\n$$\nx_{t+1} = \\alpha + \\beta x_t + \\epsilon_t\n$$\n\nwhere:\n\n$$\n\\alpha = \\gamma \\mu \n$$\n\n$$\n\\beta = 1 - \\gamma  \n$$\n\nUsing ordinary least squares (OLS) or any linear regression model, we can estimate the parameters $\\alpha$ and $\\beta$ from the data. We ignore the error term $\\epsilon$. Once we have these estimates, we can solve for $\\gamma$ as follows:\n\n1. Estimate $\\alpha$ and $\\beta$:\n\n$$\nx_{t+1} = \\alpha + \\beta x_t \n$$\n\n2. Solve for $\\gamma$ from the estimated slope $\\beta$:\n\n$$\n\\gamma = 1 - \\beta\n$$\n\n3. Solve for $\\mu$:\n\n$$\n\\mu = \\frac{\\alpha}{\\gamma}\n$$\n\npython code below does that:","metadata":{}},{"cell_type":"code","source":"import scipy.stats as ss\nfrom statsmodels.tsa.stattools import adfuller\n\ndef estimate_ou(ts_df):\n    # Check for stationarity\n    adf_test = adfuller(ts_df)\n    if adf_test[1] > 0.05:\n        print(f\"Warning: Time series is not stationary. ADF p-value: {adf_test[1]}\")\n\n    X = ts_df.values\n    XX = X[:-1]\n    YY = X[1:]\n\n    res = ss.linregress(XX, YY)\n    alpha = res.intercept\n    beta = res.slope\n    gamma = 1 - beta\n    mu = alpha / gamma\n    kappa = -np.log(2) / gamma\n\n    return kappa, gamma, mu\n\nkappa, gamma, mu = estimate_ou(market_df['Close'])\n\nprint(f\"Estimated kappa: {kappa}\")\nprint(f\"Estimated gamma: {gamma}\")\nprint(f\"Estimated mu: ${mu}\")\nprint(f\"Origin range ${market_df['Close'].iloc[0]} to ${market_df['Close'].iloc[-1]}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The results don't make sense, because the market is not stationary and does not mean revert in the short term, and as we saw these past 4 years, can explode and trend up or down. Still wa have our kappa of -400.\n\n## Mining Trading Days Around FOMC\n\nTo run simulations, we need the market data around the FOMC meetings, a week before and a week after when the shock down will wear out:","metadata":{}},{"cell_type":"code","source":"results = []\nfor i, row in filtered_meetings.iterrows():\n    meeting_date = row['FOMC Meeting']\n    start_date = meeting_date - pd.tseries.offsets.BDay(5)\n    end_date = meeting_date + pd.tseries.offsets.BDay(6)\n    spx_after_meeting = market_df.loc[start_date:end_date]\n\n    rets = spx_after_meeting['Close'].pct_change().bfill()\n    mean = rets.mean()\n    var = rets.rolling(window=2).var()\n    theta = var.mean()\n    sigma_variance = var.std()\n    rho = rets.corr(var)\n\n    result = {\n        'FOMOC Meeting Date': meeting_date,\n        'mean rets': mean,\n        'prices': spx_after_meeting['Close'].values,\n        'variance': var.mean(),\n        'theta': theta,\n        'sigma_variance': sigma_variance,\n        'rho': rho,\n        'shock': row['Fed Rate Increase']\n    }\n\n    # Add all features dynamically\n    for feature in FEATURES:\n        result[feature] = row[feature]\n\n    results.append(result)\n\nall_fomc_data_df = pd.DataFrame(results)\nall_fomc_data_df[[\"FOMOC Meeting Date\", \"mean rets\", \"variance\", \"theta\", \"sigma_variance\", \"rho\", \"shock\"]].head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Testing with Market Shock\n\nlet's select a FOMC where they last increased the rates, the one in 2023-06-14:","metadata":{}},{"cell_type":"code","source":"train_fomc = all_fomc_data_df.iloc[1:2]\ntrain_fomc[[\"FOMOC Meeting Date\", \"mean rets\", \"variance\", \"theta\", \"sigma_variance\", \"rho\", \"shock\"]]\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ntest_fomc = all_fomc_data_df.iloc[2:3]\ntest_fomc[[\"FOMOC Meeting Date\", \"mean rets\", \"variance\", \"theta\", \"sigma_variance\", \"rho\", \"shock\"]]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We dryrun the Heston, to see if the numbers make sense:","metadata":{}},{"cell_type":"code","source":"def get_params(market_df, test_fomc):\n    date_end = pd.to_datetime(test_fomc['FOMOC Meeting Date'].iloc[-1])\n    historic_df = market_df.loc[:date_end]\n\n    kappa, _, _ = estimate_ou(historic_df['Close'])\n    rets = historic_df['Close'].pct_change().bfill()\n    mean = rets.mean()\n    var = rets.rolling(window=20).var()\n    X0 = test_fomc['prices'].iloc[-1][0] if len(test_fomc['prices']) > 0 else None\n    v0 = var.iloc[-1]\n    mu = rets.iloc[-1]\n    gamma = var.mean()\n    sigma_variance = var.std()\n    rho = rets.corr(var)\n    T = 1 / 252\n    N = len(test_fomc['prices'].iloc[-1]) if len(test_fomc['prices']) > 0 else None\n\n    return X0, v0, mu, kappa, gamma, sigma_variance, rho, T, N\n\nX0, v0, mu, kappa, gamma, sigma_variance, rho, T, N= get_params(market_df, test_fomc)\nprint(X0, v0, mu, kappa, theta, sigma_variance, rho, T)\n\nX, variance = heston_model(X0, v0, mu, kappa, gamma, sigma_variance, rho, T, N)\ncomparison_df = pd.DataFrame([X, test_fomc['prices'].iloc[-1]], index=['X', 'Prices'])\ncomparison_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Without hinting the Heston that there was a shock, it will random walk sideways.\n\nTo give the correct parameters for the shocks, we need to collect it from the historical data:","metadata":{}},{"cell_type":"code","source":"def calculate_fomcstats(df, target_date):\n    filtered_df = df[df['FOMOC Meeting Date'] < target_date]\n\n    mean_prices = []\n    std_prices = []\n\n    for prices in filtered_df['prices']:\n        returns = np.diff(prices) / prices[:-1]\n        mean_prices.append(np.mean(returns))\n        std_prices.append(np.std(returns))\n\n    mean = np.mean(mean_prices)\n    sigma = np.mean(std_prices)\n\n    return mean, sigma\n\ncalculate_fomcstats(all_fomc_data_df, pd.to_datetime(test_fomc['FOMOC Meeting Date'].iloc[-1]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Monte Carlo Simulations\n\nHere is where it will get interesting.\n\nWe have the Heston, we have the jumps mean and standard devation throughout history and we have a GBC that can supply probabilities for the status quo (no or slight positive shocks) and a rate increase (negative shocks). Now we get a normal distribution of what might happen to SPY's price, and therefore the market by running a million different paths with a Monte Carlo.\n\nThe python code below simulates that:","metadata":{}},{"cell_type":"code","source":"from scipy.stats import norm, skew, kurtosis, shapiro\nimport seaborn as sns\n\ndef monte_carlo(X0, v0, mu, kappa, gamma, sigma_variance, rho, T, N , df = None, historic_df=None, model = None, scaler=None, features = FEATURES, num_simulations=100000):\n    final_prices = np.zeros(num_simulations)\n    final_variances = np.zeros(num_simulations)\n    jump_probas = None\n    jummp_mean = None\n    jump_std = None\n\n    if df is not None and model is not None and scaler is not None and historic_df is not None:\n        scaled_features = scaler.transform(df[features].head(1))\n        jump_probas = best_model.predict_proba(scaled_features)[0]\n        jummp_mean, jump_std = calculate_fomcstats(historic_df, pd.to_datetime(df['FOMOC Meeting Date'].iloc[-1]))\n        print(jump_probas) # 0: No change or less, 1: increase\n\n    for i in range(num_simulations):\n        if jump_probas is not None:\n            X, variance = heston_model(X0, v0, mu, kappa, gamma, sigma_variance, rho, T, N, pos_jump_proba=jump_probas[0], neg_jump_proba=jump_probas[1], jump_mean=jummp_mean, jump_std=jump_std)\n        else:\n            X, variance = heston_model(X0, v0, mu, kappa, gamma, sigma_variance, rho, T, N )\n        final_prices[i] = X[-1]\n        final_variances[i] = variance[-1]\n\n    results = pd.DataFrame({\n        'final_prices': final_prices,\n        'final_variances': final_variances\n    })\n\n    return results.describe(), results\n\ndef plot_results(results, start_price=None, end_price=None, fomc_date=None, high=None, low=None):\n    skew_credit = skew(results['final_prices'])\n    kurtosis_credit = kurtosis(results['final_prices'])\n    stat, p_value = shapiro(results['final_prices'])\n    normality_test_result = f'Stat={stat:.3f}, p-value={p_value:.3f}, skew={skew_credit:.2f}, kurt={kurtosis_credit:.2f}'\n\n    plt.figure(figsize=(12, 4))\n    sns.histplot(results['final_prices'], bins=50, kde=True, color='blue', edgecolor='black', alpha=0.2, stat='density', label='KDE')\n\n    if start_price is not None:\n        plt.axvline(start_price, color='green', linestyle='dashed', linewidth=2, label='Start Price', alpha=0.5)\n    if end_price is not None:\n        plt.axvline(end_price, color='red', linestyle='dashed', linewidth=2, label='End Price', alpha=0.5)\n    if high is not None:\n        plt.axvline(high, color='orange', linewidth=2, label='High Price', alpha=0.7)\n    if low is not None:\n        plt.axvline(low, color='cyan', linewidth=2, label='Low Price', alpha=0.7)\n\n    plt.axvline(results['final_prices'].mean(), color='yellow', linestyle='dashed', linewidth=2, label='Mean Price', alpha=0.5)\n\n    n_mu, n_std = norm.fit(results['final_prices'])\n    xmin, xmax = plt.xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = norm.pdf(x, n_mu, n_std)\n    plt.plot(x, p, 'k', linewidth=2, label='Normal PDF', alpha=0.5)\n\n\n    plt.title(f'{fomc_date} Distribution ({normality_test_result})')\n    plt.xlabel('Price')\n    plt.ylabel('Density')\n\n    plt.legend()\n    plt.show()\n\ntrain_fomc = all_fomc_data_df.iloc[1:2]\ntest_fomc = all_fomc_data_df.iloc[2:3]\n\nX0, v0, mu, kappa, gamma, sigma_variance, rho, T, N= get_params(market_df, test_fomc)\nsummary_statistics, results = monte_carlo(X0, variance0, mu, kappa, gamma, sigma_variance, rho, T, N, df=test_fomc, historic_df=all_fomc_data_df, model=best_model, scaler=scaler)\n\nplot_results(results,\n             start_price=test_fomc['prices'].iloc[-1][-1],\n             end_price=test_fomc['prices'].iloc[-1][-1],\n             high=test_fomc['prices'].iloc[-1].max(),\n             low=test_fomc['prices'].iloc[-1].min(),\n             fomc_date= pd.to_datetime(test_fomc['FOMOC Meeting Date'].iloc[-1]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"First simulation for the meeting on 2023-07-26 was a failure, mainly because the GBC signalled weakly the status quo (probablilites: [0.77837802 0.22162198]).","metadata":{}},{"cell_type":"markdown","source":"# No Shock FOMC\n\nThe next FOMC meeting to be simulated is a recent one on the 2024-06-12.","metadata":{}},{"cell_type":"code","source":"train_fomc = all_fomc_data_df.iloc[-3:-2]\ntest_fomc = all_fomc_data_df.iloc[-2:-1]\ntest_fomc","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X0, v0, mu, kappa, gamma, sigma_variance, rho, T, N= get_params(market_df, test_fomc)\nsummary_statistics, results = monte_carlo(X0, variance0, mu, kappa, gamma, sigma_variance, rho, T, N, df=test_fomc, historic_df=all_fomc_data_df, model=best_model, scaler=scaler)\nplot_results(results,\n             start_price=test_fomc['prices'].iloc[-1][-1],\n             end_price=test_fomc['prices'].iloc[-1][-1],\n             high=test_fomc['prices'].iloc[-1].max(),\n             low=test_fomc['prices'].iloc[-1].min(),\n             fomc_date= pd.to_datetime(test_fomc['FOMOC Meeting Date'].iloc[-1]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This simulation was perfect, with all price ranges being central to the historic.","metadata":{}},{"cell_type":"markdown","source":"# FOMC 31st July 2024 Predictions","metadata":{}},{"cell_type":"code","source":"next_fomc = all_fomc_data_df.iloc[-1:]\n\n\nX0, v0, mu, kappa, gamma, sigma_variance, rho, T, N= get_params(market_df, test_fomc)\nX0 = 544.44 # SPY as 29th July 20024\nN = 11\n\nsummary_statistics, results = monte_carlo(X0, variance0, mu, kappa, gamma, sigma_variance, rho, T, N, df=next_fomc, historic_df=all_fomc_data_df, model=best_model, scaler=scaler)\nplot_results(results,\n             start_price=X0,\n             fomc_date= pd.to_datetime(next_fomc['FOMOC Meeting Date'].iloc[-1]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"summary_statistics","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This is the distribution for this FOMC on the 31st July 2024. The GBC predictions lean strongly to maintaining the status quo, with probabilities: [0.98792802 0.01207198] and the distribution has a slightly stronger positive skew, meaning the market should trend green on average around these 2 weeks, with SPY clustering around $550.92, and might range from $540.21 (25%) to $561.38 (75%).","metadata":{}},{"cell_type":"markdown","source":"# Conclusion\n\nIn this article we used a Heston stochastic process and a gradient boosting classifier to create distributions for what the next FOMC impact on the market, using the SPY ETF as a proxy.\n\nWe attempt to guess what will happen for the next meeting, on the 31st July 2024. Future readers are welcome to comment if we where close or far!","metadata":{}},{"cell_type":"markdown","source":"# References\n\n- https://en.wikipedia.org/wiki/History_of_Federal_Open_Market_Committee_actions\n- https://www.federalreserve.gov/monetarypolicy/fomccalendars.htm\n- https://tradingeconomics.com/united-states/gdp-growth-annual\n- https://www.usinflationcalculator.com/inflation/historical-inflation-rates/ \n- https://www.investopedia.com/terms/c/consumerpriceindex.asp\n- https://www.investing.com/economic-calendar/retail-sales-256\n- https://fred.stlouisfed.org/series/MRTSSM44000USS\n- https://en.wikipedia.org/wiki/Ornstein%E2%80%93Uhlenbeck_process\n- https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html\n- https://bayesian-optimization.github.io/BayesianOptimization/code_docs.html\n- https://www.probabilitycourse.com/chapter11/11_0_0_intro.php\n- https://numpy.org/doc/stable/reference/random/generated/numpy.random.poisson.html","metadata":{}},{"cell_type":"markdown","source":"## Github\n\nArticle and code available on [Github](https://github.com/adamd1985/quant_research/blob/main/bond_valuations.ipynb)\n\nKaggle notebook available [here](https://www.kaggle.com/code/addarm/bond-valuations)\n\nGoogle Collab available [here](https://colab.research.google.com/github/adamd1985/quant_research/blob/main/bond_valuations.ipynb)\n\n## Media\n\nAll media used (in the form of code or images) are either solely owned by me, acquired through licensing, or part of the Public Domain and granted use through Creative Commons License.\n\n## CC Licensing and Use\n\n<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc/4.0/\"><img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by-nc/4.0/88x31.png\" /></a><br />This work is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc/4.0/\">Creative Commons Attribution-NonCommercial 4.0 International License</a>.","metadata":{}}]}