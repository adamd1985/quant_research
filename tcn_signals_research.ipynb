{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4f2af90",
   "metadata": {
    "papermill": {
     "duration": 0.00682,
     "end_time": "2023-11-05T17:59:38.297125",
     "exception": false,
     "start_time": "2023-11-05T17:59:38.290305",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Machine Learning with Broad Stock Market Timeseries - a SARIMA Rollercoaster\n",
    "\n",
    "\n",
    "<a href=\"\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a827e09",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-11-05T17:59:38.335093Z",
     "iopub.status.busy": "2023-11-05T17:59:38.334737Z",
     "iopub.status.idle": "2023-11-05T18:01:02.444324Z",
     "shell.execute_reply": "2023-11-05T18:01:02.443308Z"
    },
    "papermill": {
     "duration": 84.119909,
     "end_time": "2023-11-05T18:01:02.447010",
     "exception": false,
     "start_time": "2023-11-05T17:59:38.327101",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adamd\\AppData\\Local\\Temp\\ipykernel_1872\\3477214583.py:2: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Local...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\adamd\\\\workspace\\\\quant_research'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "import dotenv\n",
    "%load_ext dotenv\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "IS_KAGGLE = os.getenv('IS_KAGGLE', 'True') == 'True'\n",
    "\n",
    "if IS_KAGGLE:\n",
    "    # Kaggle confgs\n",
    "    print('Running in Kaggle...')\n",
    "    %pip install yfinance\n",
    "    %pip install statsmodels\n",
    "    %pip install seaborn\n",
    "    %pip install itertools\n",
    "    %pip install scikit-learn\n",
    "\n",
    "    for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "        for filename in filenames:\n",
    "            print(os.path.join(dirname, filename))\n",
    "else:\n",
    "    print('Running Local...')\n",
    "\n",
    "import yfinance as yf\n",
    "from analysis_utils import load_ticker_prices_ts_df, load_ticker_ts_df\n",
    "\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f226f2c",
   "metadata": {},
   "source": [
    "# Financial Data\n",
    "\n",
    "Collect market time series data (like stock prices, trading volumes, etc.).\n",
    "Clean the data to handle missing values, outliers, or anomalies.\n",
    "Ensure the data is in a time series format, typically with a timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38727fc7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-05T18:01:02.470424Z",
     "iopub.status.busy": "2023-11-05T18:01:02.469635Z",
     "iopub.status.idle": "2023-11-05T18:01:03.256388Z",
     "shell.execute_reply": "2023-11-05T18:01:03.255137Z"
    },
    "papermill": {
     "duration": 0.801372,
     "end_time": "2023-11-05T18:01:03.258937",
     "exception": false,
     "start_time": "2023-11-05T18:01:02.457565",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PRICE_FEATURES' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 52\u001b[0m\n\u001b[0;32m     49\u001b[0m qqq_df \u001b[38;5;241m=\u001b[39m tickers\u001b[38;5;241m.\u001b[39mget(TECH_INDEX)\n\u001b[0;32m     50\u001b[0m rus_df \u001b[38;5;241m=\u001b[39m tickers\u001b[38;5;241m.\u001b[39mget(SMALLCAP_INDEX)\n\u001b[1;32m---> 52\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m sp500_df[\u001b[43mPRICE_FEATURES\u001b[49m]\u001b[38;5;241m.\u001b[39misna()\u001b[38;5;241m.\u001b[39many()\u001b[38;5;241m.\u001b[39many()\n\u001b[0;32m     54\u001b[0m rus_df\u001b[38;5;241m.\u001b[39mtail(\u001b[38;5;241m5\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'PRICE_FEATURES' is not defined"
     ]
    }
   ],
   "source": [
    "START_DATE = \"2009-01-01\"\n",
    "END_DATE = \"2023-12-31\"\n",
    "DATA_DIR = \"data\"\n",
    "INDEX = \"Date\"\n",
    "\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "TARGET_ETF = \"SPY\"  # S&P 500\n",
    "RATES_INDEX = \"^TNX\"  # 10 Year Treasury Note Yield\n",
    "VOLATILITY_INDEX = \"^VIX\"  # CBOE Volatility Index\n",
    "MACRO_INDEX = \"VEU\"  # Vanguard FTSE All-World ex-US ETF\n",
    "TECH_INDEX = \"QQQ\"\n",
    "SMALLCAP_INDEX = \"R2US.MI\"\n",
    "\n",
    "tickers_symbols = [\n",
    "    TARGET_ETF,\n",
    "    VOLATILITY_INDEX,\n",
    "    RATES_INDEX,\n",
    "    MACRO_INDEX,\n",
    "    TECH_INDEX,\n",
    "    SMALLCAP_INDEX,\n",
    "]\n",
    "\n",
    "tickers = {}\n",
    "for symbol in tickers_symbols:\n",
    "    cached_file_path = f\"{DATA_DIR}/{symbol}-{START_DATE}-{END_DATE}.csv\"\n",
    "\n",
    "    try:\n",
    "        if os.path.exists(cached_file_path):\n",
    "            df = pd.read_csv(cached_file_path, index_col=INDEX)\n",
    "            df.index = pd.to_datetime(df.index)\n",
    "            assert len(df) > 0\n",
    "        else:\n",
    "            df = yf.download(\n",
    "                symbol, start=START_DATE, end=END_DATE, progress=False, interval=\"1d\"\n",
    "            )\n",
    "            assert len(df) > 0\n",
    "            df.to_csv(cached_file_path)\n",
    "        df.fillna(0, inplace=True)\n",
    "        tickers[symbol] = df\n",
    "    except Exception as e:\n",
    "        print(f\"Error with {symbol}: {e}\")\n",
    "\n",
    "\n",
    "sp500_df = tickers.get(TARGET_ETF)\n",
    "vix_df = tickers.get(VOLATILITY_INDEX)\n",
    "t10ytt_df = tickers.get(RATES_INDEX)\n",
    "veu_df = tickers.get(MACRO_INDEX)\n",
    "qqq_df = tickers.get(TECH_INDEX)\n",
    "rus_df = tickers.get(SMALLCAP_INDEX)\n",
    "\n",
    "assert not sp500_df[PRICE_FEATURES].isna().any().any()\n",
    "\n",
    "rus_df.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122af07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp500_pr = ((1 + sp500_df[\"Close\"].pct_change()).cumprod() - 1) * 100\n",
    "veu_pr = ((1 + veu_df[\"Close\"].pct_change()).cumprod() - 1) * 100\n",
    "vix_pr = ((1 + vix_df[\"Close\"].pct_change()).cumprod() - 1) * 100\n",
    "tnx_pr = ((1 + t10ytt_df[\"Close\"].pct_change()).cumprod() - 1) * 100\n",
    "qqq_pr = ((1 + qqq_df[\"Close\"].pct_change()).cumprod() - 1) * 100\n",
    "rus_pr = ((1 + rus_df[\"Close\"].pct_change()).cumprod() - 1) * 100\n",
    "\n",
    "plt.figure(figsize=(16, 6))\n",
    "plt.plot(sp500_pr, label=TARGET_ETF)\n",
    "plt.plot(veu_pr, label=MACRO_INDEX, alpha=0.5)\n",
    "plt.plot(vix_pr, label=VOLATILITY_INDEX)\n",
    "plt.plot(tnx_pr, label=RATES_INDEX)\n",
    "plt.plot(qqq_pr, label=TECH_INDEX)\n",
    "plt.plot(rus_pr, label=SMALLCAP_INDEX)\n",
    "\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c42ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "days_rising = sp500_df[sp500_df[\"Close\"] > sp500_df[\"Open\"]]\n",
    "days_not_rising = sp500_df[sp500_df[\"Close\"] <= sp500_df[\"Open\"]]\n",
    "days_rising_count = days_rising.groupby(days_rising.index.year).size()\n",
    "days_not_rising_count = days_not_rising.groupby(days_not_rising.index.year).size()\n",
    "\n",
    "total_days = sp500_df.groupby(sp500_df.index.year).size()\n",
    "percentage_rising = (days_rising_count / total_days) * 100\n",
    "\n",
    "yearly_counts = pd.DataFrame(\n",
    "    {\"Rising\": days_rising_count, \"Not Rising\": days_not_rising_count}\n",
    ")\n",
    "yearly_counts.plot(kind=\"bar\", color=[\"green\", \"red\"], figsize=(16, 6))\n",
    "plt.title(\"Days Rising vs Not Rising per Year\")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.tick_params(axis=\"x\", rotation=45)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Baseline Accuracy: {percentage_rising.mean():0.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfdd0638",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5751b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from tensorflow.keras.layers import Normalization\n",
    "\n",
    "MONTH_SINE = \"month_sin\"\n",
    "MONTH_COS = \"month_cos\"\n",
    "DAY_SINE = \"day_sin\"\n",
    "DAY_COS = \"day_cos\"\n",
    "Q_SINE = \"quart_sin\"\n",
    "Q_COS = \"quart_cos\"\n",
    "BIZ_SINE = \"biz_sin\"\n",
    "BIZ_COS = \"biz_cos\"\n",
    "FACTOR_COSTS = 0.0025  # Assume 2.5% commissions.\n",
    "FACTOR_SPREAD = 0.0003  # assume 0.3% spread & slippage.\n",
    "FACTOR_AVG_MOVE = (\n",
    "    0  # (tickers.get(TARGET_ETF)[\"Close\"].tail(365*2).pct_change().mean() / 4)\n",
    ")\n",
    "TARGET_FACTOR = FACTOR_COSTS + FACTOR_SPREAD + FACTOR_AVG_MOVE\n",
    "TARGET = \"Close_target\"\n",
    "PRICE_FEATURES = [\"Open\", \"High\", \"Low\", \"Close\"]\n",
    "TIME_FEATURES = [\n",
    "    DAY_SINE,\n",
    "    DAY_COS,\n",
    "    MONTH_SINE,\n",
    "    MONTH_COS,\n",
    "    BIZ_SINE,\n",
    "    BIZ_COS,\n",
    "    Q_SINE,\n",
    "    Q_COS,\n",
    "]\n",
    "FEATURES = [\n",
    "    \"Volume\",\n",
    "    RATES_INDEX,\n",
    "    MACRO_INDEX,\n",
    "    VOLATILITY_INDEX,\n",
    "    TECH_INDEX,\n",
    "]\n",
    "EXT_FEATURES = FEATURES + PRICE_FEATURES + TIME_FEATURES\n",
    "WINDOW_SIZE = int(252 / 4)  # 1 years trading, sampled across days\n",
    "PREDICTION_HORIZON = 1  # next 1 trading day\n",
    "\n",
    "print(f\"target factor used: {TARGET_FACTOR}\")\n",
    "\n",
    "\n",
    "def create_time_features(data_df):\n",
    "    \"\"\"\n",
    "    Encodes time cyclic features for a dataset with monthly sampling.\n",
    "    Including cyclic encoding for day and year.\n",
    "    :param data_df: The timeseries with a date in the format YYYY-MM-DD as index.\n",
    "    :return: data_df with added wave features for month, day, and year.\n",
    "    \"\"\"\n",
    "    if not isinstance(data_df.index, pd.DatetimeIndex):\n",
    "        raise ValueError(\"The DataFrame index must be a DateTimeIndex.\")\n",
    "\n",
    "    def _sin_transformer(period):\n",
    "        return FunctionTransformer(lambda x: np.sin(x / period * 2 * np.pi))\n",
    "\n",
    "    def _cos_transformer(period):\n",
    "        return FunctionTransformer(lambda x: np.cos(x / period * 2 * np.pi))\n",
    "\n",
    "    months = data_df.index.month\n",
    "    data_df[MONTH_SINE] = _sin_transformer(12).fit_transform(months)\n",
    "    data_df[MONTH_COS] = _cos_transformer(12).fit_transform(months)\n",
    "    data_df[Q_SINE] = _sin_transformer(12 / 4).fit_transform(months)\n",
    "    data_df[Q_COS] = _cos_transformer(12 / 4).fit_transform(months)\n",
    "    data_df[BIZ_SINE] = _sin_transformer(12 * 5).fit_transform(months)\n",
    "    data_df[BIZ_COS] = _cos_transformer(12 * 5).fit_transform(months)\n",
    "    days = data_df.index.day\n",
    "    data_df[DAY_SINE] = _sin_transformer(365).fit_transform(days)\n",
    "    data_df[DAY_COS] = _cos_transformer(365).fit_transform(days)\n",
    "    return data_df\n",
    "\n",
    "\n",
    "def create_features_df(tickers):\n",
    "    \"\"\"\n",
    "    Create all exogenous features that lead to our target etf.\n",
    "        - if the trading day close is higher than the open.\n",
    "        - price log returns\n",
    "    \"\"\"\n",
    "\n",
    "    def _get_log_returns(data_df):\n",
    "        return np.log(data_df / data_df.shift(1)).fillna(0)\n",
    "\n",
    "    IDX_COL = \"Open\"\n",
    "\n",
    "    data_df = tickers.get(TARGET_ETF).copy()\n",
    "    data_df[TARGET] = (data_df[\"Close\"] * (1 - TARGET_FACTOR)).astype(float)\n",
    "\n",
    "    data_df[PRICE_FEATURES] = _get_log_returns(data_df[PRICE_FEATURES])\n",
    "\n",
    "    rates_df = tickers.get(RATES_INDEX)\n",
    "    data_df[RATES_INDEX] = _get_log_returns(rates_df[IDX_COL])\n",
    "    macro_df = tickers.get(MACRO_INDEX)\n",
    "    data_df[MACRO_INDEX] = _get_log_returns(macro_df[IDX_COL])\n",
    "    vix_df = tickers.get(VOLATILITY_INDEX)\n",
    "    data_df[VOLATILITY_INDEX] = _get_log_returns(vix_df[IDX_COL])\n",
    "    data_df = data_df.fillna(0)\n",
    "\n",
    "    tech_df = tickers.get(TECH_INDEX)\n",
    "    data_df[TECH_INDEX] = _get_log_returns(tech_df[IDX_COL])\n",
    "    data_df = data_df.fillna(0)\n",
    "    small_df = tickers.get(SMALLCAP_INDEX)\n",
    "    data_df[SMALLCAP_INDEX] = _get_log_returns(small_df[IDX_COL])\n",
    "    data_df = data_df.fillna(0)\n",
    "\n",
    "    return data_df\n",
    "\n",
    "\n",
    "def prepare_data(tickers):\n",
    "    \"\"\"\n",
    "    Utility function to prepare the data.\n",
    "    :data_df dataframe: dataframe with `window_size` months of data to predict the `window_size`+`horizon`.\n",
    "    :param window_size: int, length of the input sequence\n",
    "    :param horizon: int, forecasting horizon, defaults to 1\n",
    "    :return: Array in the shape of (n_samples, n_steps, n_features)\n",
    "    \"\"\"\n",
    "    data_df = create_features_df(tickers)\n",
    "    data_df = create_time_features(data_df)\n",
    "\n",
    "    normalizer = Normalization(axis=-1)\n",
    "    normalizer.adapt(data_df[EXT_FEATURES])\n",
    "    data_df_normalized = normalizer(data_df[EXT_FEATURES])\n",
    "    data_df_normalized = pd.DataFrame(data_df_normalized.numpy(), columns=EXT_FEATURES)\n",
    "\n",
    "    return data_df_normalized, normalizer\n",
    "\n",
    "\n",
    "def prepare_windows(\n",
    "    data_df,\n",
    "    features=EXT_FEATURES,\n",
    "    target=TARGET,\n",
    "    window_size=WINDOW_SIZE,\n",
    "    horizon=PREDICTION_HORIZON,\n",
    "):\n",
    "    \"\"\"\n",
    "    Create input and target windows suitable for TCN model.\n",
    "    :param data: DataFrame with shape (n_samples, n_features)\n",
    "    :param features: List of strings, names of the feature columns\n",
    "    :param target: String, name of the target column\n",
    "    :param window_size: int, length of the input sequence.\n",
    "    :param horizon: int, forecasting horizon.\n",
    "    :return: Array in the shape of (n_samples, n_steps, n_features)\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    for i in tqdm(\n",
    "        range(len(data_df) - window_size - horizon + 1), desc=\"Encoding Widows\"\n",
    "    ):\n",
    "        input_window = data_df[features].iloc[i : i + window_size].values\n",
    "        X.append(input_window)\n",
    "        if horizon == 1:\n",
    "            target_value = data_df[target].iloc[i + window_size]\n",
    "        else:\n",
    "            target_value = (\n",
    "                data_df[target].iloc[i + window_size : i + window_size + horizon].values\n",
    "            )\n",
    "        y.append(target_value)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "\n",
    "# Yes, we are using the whole dataset not the training dataset.\n",
    "# See: https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit\n",
    "# we will tell keras to do a validation split, it will not fit on the validation data.\n",
    "data_df, normalizer = prepare_data(tickers)\n",
    "data_df.tail(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c200b8",
   "metadata": {},
   "source": [
    "# DIM Reduction & Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0262feac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "MAX_VARIANCE = 0.95\n",
    "\n",
    "pca = PCA()\n",
    "xdata = pca.fit_transform(data_df)\n",
    "\n",
    "cum_var_exp = np.cumsum(pca.explained_variance_ratio_)\n",
    "num_components = np.argmax(cum_var_exp >= MAX_VARIANCE) + 1\n",
    "print(\n",
    "    f\"Max components for {MAX_VARIANCE*100}% variance: {num_components} out of {data_df.shape[1]}\"\n",
    ")\n",
    "\n",
    "\n",
    "feature_importance = np.abs(pca.components_[:num_components])\n",
    "most_important = np.argsort(-feature_importance, axis=1)\n",
    "most_important_names = [\n",
    "    [data_df.columns[idx] for idx in most_important[i]] for i in range(num_components)\n",
    "]\n",
    "\n",
    "\n",
    "dic = {f\"PC{i+1}\": most_important_names[i] for i in range(num_components)}\n",
    "pca_df = pd.DataFrame(dic)\n",
    "\n",
    "pca_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4a97a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = prepare_windows(data_df)\n",
    "\n",
    "print(f\"Label shape encoded: {y.shape}\")\n",
    "print(f\"Data shape: {X.shape}\")\n",
    "print(f\"First window exog normalized: {X[0,  :]}\")\n",
    "print(f\"First window targets: {y[:WINDOW_SIZE]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e574d6",
   "metadata": {},
   "source": [
    "# TCN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421953b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import (\n",
    "    SpatialDropout1D,\n",
    "    Dense,\n",
    "    Conv1D,\n",
    "    Layer,\n",
    "    Add,\n",
    "    Input,\n",
    "    Lambda,\n",
    ")\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "\n",
    "class TCNBlock(Layer):\n",
    "    \"\"\"\n",
    "    TCN Residual Block that uses zero-padding to maintain `steps` value of the ouput equal to the one in the input.\n",
    "    Residual Block is obtained by stacking togeather (2x) the following:\n",
    "        - 1D Dilated Convolution\n",
    "        - ReLu\n",
    "        - Spatial Dropout\n",
    "    And adding the input after trasnforming it with a 1x1 Conv\n",
    "    forked and extended from: https://github.com/albertogaspar/dts/blob/master/dts/models/TCN.py\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        filters=1,\n",
    "        kernel_size=2,\n",
    "        dilation_rate=None,\n",
    "        kernel_initializer=\"glorot_normal\",\n",
    "        bias_initializer=\"glorot_normal\",\n",
    "        kernel_regularizer=None,\n",
    "        bias_regularizer=None,\n",
    "        use_bias=False,\n",
    "        dropout_rate=0.0,\n",
    "        id=None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\" \"\n",
    "        Arguments\n",
    "            filters: Integer, the dimensionality of the output space\n",
    "                (i.e. the number of output filters in the convolution).\n",
    "            kernel_size: An integer or tuple/list of a single integer,\n",
    "                specifying the length of the 1D convolution window.\n",
    "            dilation_rate: an integer or tuple/list of a single integer, specifying\n",
    "                the dilation rate to use for dilated convolution.\n",
    "                Usually dilation rate increases exponentially with the depth of the network.\n",
    "            activation: Activation function to use\n",
    "                If you don't specify anything, no activation is applied\n",
    "                (ie. \"linear\" activation: `a(x) = x`).\n",
    "            use_bias: Boolean, whether the layer uses a bias vector.\n",
    "            kernel_initializer: Initializer for the `kernel` weights matrix\n",
    "            bias_initializer: Initializer for the bias vector\n",
    "            kernel_regularizer: Regularizer function applied to the `kernel` weights matrix\n",
    "            bias_regularizer: Regularizer function applied to the bias vector\n",
    "                (see [regularizer](../regularizers.md)).\n",
    "        # Input shape\n",
    "            3D tensor with shape: `(batch, steps, n_features)`\n",
    "        # Output shape\n",
    "            3D tensor with shape: `(batch, steps, filters)`\n",
    "        \"\"\"\n",
    "        super(TCNBlock, self).__init__(**kwargs)\n",
    "        self.filters = filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.dilation_rate = dilation_rate\n",
    "\n",
    "        # Capture feature set from the input\n",
    "        self.conv1 = Conv1D(\n",
    "            filters=filters,\n",
    "            kernel_size=kernel_size,\n",
    "            use_bias=use_bias,\n",
    "            bias_initializer=bias_initializer,\n",
    "            bias_regularizer=bias_regularizer,\n",
    "            kernel_initializer=kernel_initializer,\n",
    "            kernel_regularizer=kernel_regularizer,\n",
    "            padding=\"causal\",\n",
    "            dilation_rate=dilation_rate,\n",
    "            activation=\"relu\",\n",
    "            name=f\"Conv1D_1_{id}\",\n",
    "        )\n",
    "\n",
    "        # Spatial dropout is specific to convolutions by dropping an entire timewindow,\n",
    "        # not to rely too heavily on specific features detected by the kernels.\n",
    "        self.dropout1 = SpatialDropout1D(\n",
    "            dropout_rate, trainable=True, name=f\"SpatialDropout1D_1_{id}\"\n",
    "        )\n",
    "        # Capture a higher order feature set from the previous convolution\n",
    "        self.conv2 = Conv1D(\n",
    "            filters=filters,\n",
    "            kernel_size=kernel_size,\n",
    "            use_bias=use_bias,\n",
    "            bias_initializer=bias_initializer,\n",
    "            bias_regularizer=bias_regularizer,\n",
    "            kernel_initializer=kernel_initializer,\n",
    "            kernel_regularizer=kernel_regularizer,\n",
    "            padding=\"causal\",\n",
    "            dilation_rate=dilation_rate,\n",
    "            activation=\"relu\",\n",
    "            name=f\"Conv1D_2_{id}\",\n",
    "        )\n",
    "        self.dropout2 = SpatialDropout1D(\n",
    "            dropout_rate, trainable=True, name=f\"SpatialDropout1D_2_{id}\"\n",
    "        )\n",
    "\n",
    "        # The skip connection is an addition of the input to the block with the output of the second dropout layer.\n",
    "        # Solves vanishing gradient, carries info from earlier layers to later layers, allowing gradients to flow across this alternative path.\n",
    "        # Does not learn direct mappings, but differences (residuals) while keeping temporal context.\n",
    "        # Note how it keeps dims intact with kernel 1.\n",
    "        self.skip_out = Conv1D(\n",
    "            filters=filters,\n",
    "            kernel_size=1,\n",
    "            activation=\"linear\",\n",
    "            name=f\"Conv1D_skipconnection_{id}\",\n",
    "        )\n",
    "        # This is the elementwise add for the residual connection and Conv1d 2's output\n",
    "        self.residual_out = Add(name=f\"residual_Add_{id}\")\n",
    "\n",
    "    def apply_block(self, inputs):\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        # Residual output by adding the inputs back:\n",
    "        skip_out_x = self.skip_out(inputs)\n",
    "        x = self.residual_out([x, skip_out_x])\n",
    "        return x\n",
    "\n",
    "\n",
    "def TCN(\n",
    "    input_shape,\n",
    "    output_horizon=1,\n",
    "    num_filters=32,\n",
    "    num_layers=1,\n",
    "    kernel_size=2,\n",
    "    dilation_rate=2,\n",
    "    kernel_initializer=\"glorot_normal\",\n",
    "    bias_initializer=\"glorot_normal\",\n",
    "    kernel_regularizer=None,\n",
    "    bias_regularizer=None,\n",
    "    use_bias=False,\n",
    "    dropout_rate=0.0,\n",
    "):\n",
    "    \"\"\"\n",
    "    Tensorflow TCN Model builder.\n",
    "    forked and extended from: https://github.com/albertogaspar/dts/blob/master/dts/models/TCN.py\n",
    "    see: https://www.tensorflow.org/api_docs/python/tf/keras/Model\n",
    "    see: https://www.tensorflow.org/guide/keras/making_new_layers_and_models_via_subclassing#the_model_class\n",
    "    see: https://www.tensorflow.org/api_docs/python/tf/keras/regularizers/L2\n",
    "\n",
    "    :param layers: int\n",
    "        Number of layers for the network. Defaults to 1 layer.\n",
    "    :param filters: int\n",
    "        the number of output filters in the convolution. Defaults to 32.\n",
    "    :param kernel_size: int or tuple\n",
    "        the length of the 1D convolution window\n",
    "    :param dilation_rate: int\n",
    "        the dilation rate to use for dilated convolution. Defaults to 1.\n",
    "    :param output_horizon: int\n",
    "        the output horizon.\n",
    "    \"\"\"\n",
    "    x = inputs = Input(shape=input_shape)\n",
    "    for i in range(num_layers):\n",
    "        block = TCNBlock(\n",
    "            filters=num_filters,\n",
    "            kernel_size=kernel_size,\n",
    "            dilation_rate=dilation_rate**i,\n",
    "            kernel_initializer=kernel_initializer,\n",
    "            bias_initializer=bias_initializer,\n",
    "            kernel_regularizer=kernel_regularizer,\n",
    "            bias_regularizer=bias_regularizer,\n",
    "            use_bias=use_bias,\n",
    "            dropout_rate=dropout_rate,\n",
    "            id=i,\n",
    "        )\n",
    "        x = block.apply_block(x)\n",
    "    # Selects the last timestep and predict in the 1 DIM layer.\n",
    "    x = Lambda(lambda x: x[:, -output_horizon:, 0], name=\"lambda_last_timestep\")(x)\n",
    "    outputs = Dense(output_horizon, name=\"Dense_singleoutput\", activation=\"linear\")(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs, name=\"TCN\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e8329e",
   "metadata": {},
   "source": [
    "# Train Model\n",
    "Using tensorbards: `tensorboard --logdir logs/hparam_tuning`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba82142",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "LOG_BASEPATH = \"./logs\"\n",
    "if os.path.exists(LOG_BASEPATH):\n",
    "    assert os.path.isdir(LOG_BASEPATH)\n",
    "    shutil.rmtree(LOG_BASEPATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f481f001",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.regularizers import L2\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.summary import create_file_writer\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import ParameterGrid, TimeSeriesSplit\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "VAL_SPLIT = 0.15\n",
    "EPOCHS = 1\n",
    "PATIENCE_EPOCHS = 25\n",
    "BATCH_SIZE = 32\n",
    "FILTER = 128\n",
    "DROPRATE = 0.5\n",
    "POOL_SIZE = 5\n",
    "KERNEL_SIZE = 5\n",
    "DILATION_RATE = 2\n",
    "MAX_LAYERS = 5\n",
    "L2_REG = 0.005\n",
    "LEARN_RATE = 0.0001\n",
    "MODEL_LOG_DIR = f'{LOG_BASEPATH}/{datetime.now().strftime(\"%d%H%M%S\")}'\n",
    "CV_SPLITS = 5\n",
    "TARGET_METRIC = \"mse\"\n",
    "METRICS = [TARGET_METRIC]\n",
    "\n",
    "# See: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.ParameterGrid.html\n",
    "# See paper: https://www.mdpi.com/2076-3417/10/7/2322\n",
    "HP_NUM_FILTERS = hp.HParam(\"num_filters\", hp.Discrete([FILTER * 2, FILTER]))\n",
    "HP_KERNEL_SIZE = hp.HParam(\n",
    "    \"kernel_size\", hp.Discrete([KERNEL_SIZE * 4, KERNEL_SIZE * 2, KERNEL_SIZE])\n",
    ")\n",
    "HP_BATCH_SIZE = hp.HParam(\"batch_size\", hp.Discrete([BATCH_SIZE]))\n",
    "HP_EPOCHS = hp.HParam(\"epochs\", hp.Discrete([EPOCHS]))\n",
    "HP_DILATION_RATE = hp.HParam(\n",
    "    \"dilation_rate\", hp.Discrete([DILATION_RATE * 4, DILATION_RATE * 2, DILATION_RATE])\n",
    ")\n",
    "HP_DROPOUT_RATE = hp.HParam(\"dropout_rate\", hp.Discrete([DROPRATE]))\n",
    "HP_NUM_LAYERS = hp.HParam(\n",
    "    \"num_layers\", hp.Discrete([MAX_LAYERS * 4, MAX_LAYERS * 2, MAX_LAYERS])\n",
    ")\n",
    "HP_L2_REG = hp.HParam(\"l2_reg\", hp.Discrete([L2_REG]))\n",
    "HP_LEARNING_RATE = hp.HParam(\"learning_rate\", hp.Discrete([LEARN_RATE]))\n",
    "HP_PATIENCE = hp.HParam(\"patience\", hp.Discrete([PATIENCE_EPOCHS]))\n",
    "\n",
    "HPARAMS = [\n",
    "    HP_NUM_FILTERS,\n",
    "    HP_KERNEL_SIZE,\n",
    "    HP_BATCH_SIZE,\n",
    "    HP_EPOCHS,\n",
    "    HP_DILATION_RATE,\n",
    "    HP_DROPOUT_RATE,\n",
    "    HP_NUM_LAYERS,\n",
    "    HP_L2_REG,\n",
    "    HP_LEARNING_RATE,\n",
    "    HP_PATIENCE,\n",
    "]\n",
    "\n",
    "print(f\"Model logs for Tensorboard available here: {MODEL_LOG_DIR}\")\n",
    "\n",
    "\n",
    "def grid_search_build_tcn(\n",
    "    input_shape, X, y, hparams=HPARAMS, file_name=\"best_params.json\"\n",
    "):\n",
    "    def _create_model(input_shape, hyperparams):\n",
    "        model = TCN(\n",
    "            input_shape=input_shape,\n",
    "            output_horizon=PREDICTION_HORIZON,\n",
    "            num_filters=hyperparams[\"num_filters\"],\n",
    "            kernel_size=hyperparams[\"kernel_size\"],\n",
    "            num_layers=hyperparams[\"num_layers\"],\n",
    "            dilation_rate=hyperparams[\"dilation_rate\"],\n",
    "            kernel_regularizer=L2(l2=hyperparams[\"l2_reg\"]),\n",
    "            bias_regularizer=L2(l2=hyperparams[\"l2_reg\"]),\n",
    "            dropout_rate=hyperparams[\"dropout_rate\"],\n",
    "        )\n",
    "        optimizer = Adam(learning_rate=hyperparams[\"learning_rate\"])\n",
    "        model.compile(loss=\"mse\", optimizer=optimizer, metrics=METRICS)\n",
    "        return model\n",
    "\n",
    "    def _save_best_params(best_params, best_loss, file_name=\"best_params.json\"):\n",
    "        with open(file_name, \"w\") as file:\n",
    "            json.dump({\"best_params\": best_params, \"best_loss\": best_loss}, file)\n",
    "\n",
    "    with create_file_writer(f\"{MODEL_LOG_DIR}/hparam_tuning\").as_default():\n",
    "        hp.hparams_config(\n",
    "            hparams=hparams,\n",
    "            metrics=[hp.Metric(TARGET_METRIC, display_name=TARGET_METRIC)],\n",
    "        )\n",
    "    grid = list(ParameterGrid({h.name: h.domain.values for h in hparams}))\n",
    "    best_model = None\n",
    "    best_loss = np.inf\n",
    "    best_params = None\n",
    "    best_history = None\n",
    "\n",
    "    for hp_values in tqdm(grid, desc=\"Grid Search..\"):\n",
    "        try:\n",
    "            model = _create_model(input_shape, hp_values)\n",
    "            callbacks = [\n",
    "                EarlyStopping(\n",
    "                    patience=hp_values[\"patience\"], monitor=f\"val_{TARGET_METRIC}\"\n",
    "                ),\n",
    "                TensorBoard(log_dir=MODEL_LOG_DIR),\n",
    "                hp.KerasCallback(MODEL_LOG_DIR, hp_values),\n",
    "            ]\n",
    "            history = model.fit(\n",
    "                X,\n",
    "                y,\n",
    "                epochs=hp_values[\"epochs\"],\n",
    "                batch_size=hp_values[\"batch_size\"],\n",
    "                validation_split=VAL_SPLIT,\n",
    "                verbose=1,\n",
    "                callbacks=callbacks,\n",
    "            )\n",
    "            metric = np.min(history.history[f\"val_{TARGET_METRIC}\"])\n",
    "\n",
    "            if metric < best_loss:\n",
    "                best_history = history\n",
    "                best_loss = metric\n",
    "                best_model = model\n",
    "                best_params = hp_values\n",
    "                _save_best_params(best_params, best_loss, file_name)\n",
    "        except Exception as e:\n",
    "            print(f\"Grid Search ERROR on params: {hp_values}\\n\", e)\n",
    "\n",
    "    return best_model, best_history\n",
    "\n",
    "\n",
    "def build_tcn(input_shape, X, y, Xt=None, yt=None, val_split=VAL_SPLIT):\n",
    "    model = TCN(\n",
    "        input_shape=input_shape,\n",
    "        output_horizon=PREDICTION_HORIZON,\n",
    "        num_filters=FILTER,\n",
    "        kernel_size=KERNEL_SIZE,\n",
    "        num_layers=MAX_LAYERS,\n",
    "        dilation_rate=DILATION_RATE,\n",
    "        kernel_regularizer=L2(l2=L2_REG),\n",
    "        bias_regularizer=L2(l2=L2_REG),\n",
    "    )\n",
    "    optimizer = Adam(LEARN_RATE)\n",
    "    model.compile(loss=\"mse\", optimizer=optimizer, metrics=METRICS)\n",
    "    callbacks = [\n",
    "        EarlyStopping(\n",
    "            patience=PATIENCE_EPOCHS,\n",
    "            monitor=f\"val_{TARGET_METRIC}\",\n",
    "            restore_best_weights=True,\n",
    "        ),\n",
    "        TensorBoard(log_dir=MODEL_LOG_DIR),\n",
    "    ]\n",
    "    history = model.fit(\n",
    "        X,\n",
    "        y,\n",
    "        validation_data=(Xt, yt) if Xt is not None else None,\n",
    "        validation_split=val_split if Xt is None else None,\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1,\n",
    "    )\n",
    "    return model, history\n",
    "\n",
    "\n",
    "def train_cv_model(X, y, input_shape, n_splits=CV_SPLITS):\n",
    "    results = []\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "    for train_index, test_index in tscv.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        try:\n",
    "            model = build_tcn(input_shape, X_train, y_train, X_test, y_test)\n",
    "            result = model.evaluate(X_test, y_test)\n",
    "            results.append(result)\n",
    "        except Exception as e:\n",
    "            print(f\"CV error!\", e)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "MODEL_DIR = f\"./models/{datetime.now().strftime('%Y%m%d-%H')}\"\n",
    "\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "assert os.path.exists(MODEL_DIR)\n",
    "\n",
    "# model = load_model(f\"{MODEL_DIR}/tcn.h5\")\n",
    "# with open(f\"{MODEL_DIR}/history.pkl\", 'rb') as file:\n",
    "#    loaded_history = pickle.load(file)\n",
    "# model, history = build_tcn(input_shape, X, y)\n",
    "model, history = grid_search_build_tcn(input_shape, X, y)\n",
    "model.save(f\"{MODEL_DIR}/tcn.h5\")\n",
    "with open(f\"{MODEL_DIR}/history.pkl\", \"wb\") as file:\n",
    "    pickle.dump(history.history, file)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0601dd19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir=LOG_BASEPATH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34348cd",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b862085f",
   "metadata": {},
   "outputs": [],
   "source": [
    "VAL_SIZE = round(len(X) * VAL_SPLIT)\n",
    "\n",
    "train_data = X[:-VAL_SIZE]\n",
    "test_data = X[-VAL_SIZE:]\n",
    "ytrain_data = y[:-VAL_SIZE]\n",
    "ytest_data = y[-VAL_SIZE:]\n",
    "print(f\"Test data shape: {ytest_data.shape}\")\n",
    "print(f\"Test data 1 horizon sample: {ytest_data[0]}\")\n",
    "\n",
    "y_pred = model.predict(train_data)\n",
    "yt_pred = model.predict(test_data)\n",
    "\n",
    "print(f\"Prediction shape: {yt_pred.shape}\")\n",
    "print(f\"Predition 1 horizon sample: {yt_pred[0].flatten()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf9703b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    mean_squared_error,\n",
    "    r2_score,\n",
    "    mean_absolute_percentage_error,\n",
    "    mean_absolute_error,\n",
    ")\n",
    "\n",
    "mae_train = mean_absolute_error(ytrain_data, y_pred)\n",
    "mae_test = mean_absolute_error(ytest_data, yt_pred)\n",
    "mse_train = mean_squared_error(ytrain_data, y_pred)\n",
    "mse_test = mean_squared_error(ytest_data, yt_pred)\n",
    "rmse_train = mean_squared_error(ytrain_data, y_pred, squared=False)\n",
    "rmse_test = mean_squared_error(ytest_data, yt_pred, squared=False)\n",
    "mape_train = mean_absolute_percentage_error(ytrain_data, y_pred) * 100\n",
    "mape_test = mean_absolute_percentage_error(ytest_data, yt_pred) * 100\n",
    "\n",
    "\n",
    "print(f\"shapes y_pred: {y_pred.shape} and yt_pred: {yt_pred.shape}\")\n",
    "\n",
    "r2 = r2_score(\n",
    "    ytest_data,\n",
    "    yt_pred,\n",
    ")\n",
    "metrics_df = pd.DataFrame(\n",
    "    {\n",
    "        \"MAE\": [mae_test],\n",
    "        \"MSE\": [mse_test],\n",
    "        \"RMSE\": [rmse_test],\n",
    "        \"MAPE\": [mape_test],\n",
    "        \"R2\": [r2],\n",
    "    }\n",
    ")\n",
    "\n",
    "fig, axs = plt.subplots(2, 1, figsize=(12, 10))\n",
    "\n",
    "axs[0].plot(history.history[\"loss\"], label=\"Train loss\")\n",
    "axs[0].plot(history.history[\"val_loss\"], label=\"Validation loss\")\n",
    "axs[0].set_xlabel(\"Epochs\")\n",
    "axs[0].set_ylabel(\"Loss\")\n",
    "axs[0].legend()\n",
    "axs[1].plot(\n",
    "    history.history[TARGET_METRIC],\n",
    "    label=f\"Train {TARGET_METRIC}\",\n",
    ")\n",
    "axs[1].plot(\n",
    "    history.history[f\"val_{TARGET_METRIC}\"],\n",
    "    label=f\"Test {TARGET_METRIC}\",\n",
    ")\n",
    "axs[1].axhline(mae_test, color=\"b\", linestyle=\"--\", label=\"Train Sample MAE\")\n",
    "axs[1].axhline(mae_train, color=\"b\", linestyle=\"-\", label=\"Test Sample MAE\")\n",
    "axs[1].axhline(rmse_train, color=\"r\", linestyle=\"--\", label=\"Train Sample RMSE\")\n",
    "axs[1].axhline(rmse_test, color=\"r\", linestyle=\"-\", label=\"Test Sample RMSE\")\n",
    "axs[1].set_xlabel(\"Epochs\")\n",
    "axs[1].set_ylabel(\"Error\")\n",
    "axs[1].legend()\n",
    "plt.show()\n",
    "\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e9a9c5",
   "metadata": {
    "papermill": {
     "duration": 0.037345,
     "end_time": "2023-11-05T18:01:12.947046",
     "exception": false,
     "start_time": "2023-11-05T18:01:12.909701",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "![]()\n",
    "\n",
    "## References\n",
    "\n",
    "- [YFinance Github](https://github.com/ranaroussi/yfinance)\n",
    "- [Vanguard All World excluding US](https://investor.vanguard.com/investment-products/etfs/profile/veu)\n",
    "\n",
    "\n",
    "## Github\n",
    "\n",
    "Article here is also available on [Github]()\n",
    "\n",
    "Kaggle notebook available [here]()\n",
    "\n",
    "\n",
    "## Media\n",
    "\n",
    "All media used (in the form of code or images) are either solely owned by me, acquired through licensing, or part of the Public Domain and granted use through Creative Commons License.\n",
    "\n",
    "## CC Licensing and Use\n",
    "\n",
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc/4.0/\"><img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by-nc/4.0/88x31.png\" /></a><br />This work is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc/4.0/\">Creative Commons Attribution-NonCommercial 4.0 International License</a>."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 98.905855,
   "end_time": "2023-11-05T18:01:13.607303",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-11-05T17:59:34.701448",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
