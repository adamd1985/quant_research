{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4f2af90",
   "metadata": {
    "papermill": {
     "duration": 0.00682,
     "end_time": "2023-11-05T17:59:38.297125",
     "exception": false,
     "start_time": "2023-11-05T17:59:38.290305",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Temporal Convolution Neural Network with Conditioning for Broad Market Signals\n",
    "\n",
    "\n",
    "<a href=\"\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a827e09",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-11-05T17:59:38.335093Z",
     "iopub.status.busy": "2023-11-05T17:59:38.334737Z",
     "iopub.status.idle": "2023-11-05T18:01:02.444324Z",
     "shell.execute_reply": "2023-11-05T18:01:02.443308Z"
    },
    "papermill": {
     "duration": 84.119909,
     "end_time": "2023-11-05T18:01:02.447010",
     "exception": false,
     "start_time": "2023-11-05T17:59:38.327101",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "import dotenv\n",
    "%load_ext dotenv\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "IS_KAGGLE = os.getenv('IS_KAGGLE', 'True') == 'True'\n",
    "\n",
    "if IS_KAGGLE:\n",
    "    # Kaggle confgs\n",
    "    print('Running in Kaggle...')\n",
    "    %pip install yfinance\n",
    "    %pip install statsmodels\n",
    "    %pip install seaborn\n",
    "    %pip install itertools\n",
    "    %pip install scikit-learn\n",
    "\n",
    "    for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "        for filename in filenames:\n",
    "            print(os.path.join(dirname, filename))\n",
    "else:\n",
    "    print('Running Local...')\n",
    "\n",
    "import yfinance as yf\n",
    "from analysis_utils import load_ticker_prices_ts_df, load_ticker_ts_df\n",
    "\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f226f2c",
   "metadata": {},
   "source": [
    "# Financial Data\n",
    "\n",
    "Collect market time series data (like stock prices, trading volumes, etc.).\n",
    "Clean the data to handle missing values, outliers, or anomalies.\n",
    "Ensure the data is in a time series format, typically with a timestamp.\n",
    "\n",
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38727fc7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-05T18:01:02.470424Z",
     "iopub.status.busy": "2023-11-05T18:01:02.469635Z",
     "iopub.status.idle": "2023-11-05T18:01:03.256388Z",
     "shell.execute_reply": "2023-11-05T18:01:03.255137Z"
    },
    "papermill": {
     "duration": 0.801372,
     "end_time": "2023-11-05T18:01:03.258937",
     "exception": false,
     "start_time": "2023-11-05T18:01:02.457565",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "START_DATE = \"1994-01-01\"\n",
    "END_DATE = \"2024-01-31\"\n",
    "DATA_DIR = \"data\"\n",
    "INDEX = \"Date\"\n",
    "TARGET_ETF = \"SPY\"  # S&P 500\n",
    "RATES_INDEX = \"^TNX\"  # 10 Year Treasury Note Yield\n",
    "VOLATILITY_INDEX = \"^VIX\"  # CBOE Volatility Index\n",
    "MACRO_INDEX = \"VEU\"  # Vanguard FTSE All-World ex-US ETF\n",
    "TECH_INDEX = \"^IXIC\"\n",
    "SMALLCAP_INDEX = \"R2US.MI\"\n",
    "GOLD_INDEX = \"GC=F\"\n",
    "UK_INDEX = \"^FTSE\"\n",
    "MEME_INDEX=\"BTC-USD\"\n",
    "JAP_INDEX = \"^N225\"\n",
    "RUS_INDEX = \"IMOEX.ME\"\n",
    "CHINA_INDEX = \"399001.SZ\"\n",
    "tickers_symbols = [\n",
    "    TARGET_ETF,\n",
    "    VOLATILITY_INDEX,\n",
    "    RATES_INDEX,\n",
    "    MACRO_INDEX,\n",
    "    TECH_INDEX,\n",
    "    SMALLCAP_INDEX,\n",
    "    GOLD_INDEX,\n",
    "    MEME_INDEX,\n",
    "    UK_INDEX,\n",
    "    JAP_INDEX,\n",
    "    RUS_INDEX,\n",
    "    CHINA_INDEX,\n",
    "]\n",
    "INTERVAL = \"1d\"\n",
    "\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "tickers = {}\n",
    "for symbol in tickers_symbols:\n",
    "    cached_file_path = f\"{DATA_DIR}/{symbol}-{START_DATE}-{END_DATE}-{INTERVAL}.csv\"\n",
    "\n",
    "    try:\n",
    "        if os.path.exists(cached_file_path):\n",
    "            df = pd.read_csv(cached_file_path, index_col=INDEX)\n",
    "            df.index = pd.to_datetime(df.index)\n",
    "            assert len(df) > 0\n",
    "        else:\n",
    "            df = yf.download(\n",
    "                symbol,\n",
    "                start=START_DATE,\n",
    "                end=END_DATE,\n",
    "                progress=False,\n",
    "                interval=INTERVAL,\n",
    "            )\n",
    "            assert len(df) > 0\n",
    "            df.to_csv(cached_file_path)\n",
    "        df.fillna(0, inplace=True)\n",
    "        tickers[symbol] = df\n",
    "    except Exception as e:\n",
    "        print(f\"Error with {symbol}: {e}\")\n",
    "\n",
    "tickers.get(TARGET_ETF).tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122af07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage_returns = {}\n",
    "\n",
    "for ticker in tickers_symbols:\n",
    "    df = tickers.get(ticker)\n",
    "    assert not df[\"Close\"].isna().any().any()\n",
    "\n",
    "    percentage_returns[ticker] = ((1 + df[\"Close\"].pct_change()).cumprod() - 1) * 100\n",
    "plt.figure(figsize=(16, 6))\n",
    "for ticker, pr in percentage_returns.items():\n",
    "    plt.plot(pr, label=ticker, alpha=0.75)\n",
    "\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c42ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_etf = tickers.get(TARGET_ETF)\n",
    "\n",
    "days_rising = target_etf[target_etf[\"Close\"] > target_etf[\"Open\"]]\n",
    "days_not_rising = target_etf[target_etf[\"Close\"] <= target_etf[\"Open\"]]\n",
    "days_rising_count = days_rising.groupby(days_rising.index.year).size()\n",
    "days_not_rising_count = days_not_rising.groupby(days_not_rising.index.year).size()\n",
    "\n",
    "total_days = target_etf.groupby(target_etf.index.year).size()\n",
    "percentage_rising = (days_rising_count / total_days) * 100\n",
    "\n",
    "yearly_counts = pd.DataFrame(\n",
    "    {\"Rising\": days_rising_count, \"Not Rising\": days_not_rising_count}\n",
    ")\n",
    "yearly_counts.plot(kind=\"bar\", color=[\"green\", \"red\"], figsize=(16, 4))\n",
    "plt.title(\"Days Rising vs Not Rising per Year\")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.tick_params(axis=\"x\", rotation=45)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Baseline Accuracy: {percentage_rising.mean():0.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfdd0638",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n",
    "![alt](./images/time_sincos.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272a09ee",
   "metadata": {},
   "source": [
    "## Feature Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5751b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer, MinMaxScaler, normalize\n",
    "\n",
    "MONTH_SINE = \"month_sin\"\n",
    "MONTH_COS = \"month_cos\"\n",
    "MONTH_RBF = \"month_rbf\"\n",
    "DAY_SINE = \"day_sin\"\n",
    "DAY_COS = \"day_cos\"\n",
    "DAY_RBF = \"day_rbf\"\n",
    "Q_SINE = \"quart_sin\"\n",
    "Q_COS = \"quart_cos\"\n",
    "Q_RBF = \"quart_rbf\"\n",
    "BIZ_SINE = \"biz_sin\"\n",
    "BIZ_COS = \"biz_cos\"\n",
    "BIZ_RBF = \"biz_rbf\"\n",
    "TARGET_FACTOR = 0.0075  # assume 75BP move on average\n",
    "TARGET_LABEL = \"Close_target\"\n",
    "TARGET_TS = \"Close\"  # The TS which we will condition.\n",
    "EXOG_TS = tickers_symbols # The TS to use for conditioning\n",
    "PRICE_FEATURES = [\"Open\", \"High\", \"Low\", \"Close\"]\n",
    "TIME_FEATURES = [\n",
    "    DAY_RBF,\n",
    "    MONTH_RBF,\n",
    "    Q_RBF,\n",
    "    BIZ_RBF,\n",
    "]\n",
    "TS_FEATURES = [\"Volume\"]\n",
    "TARGET_TS_FEATURES_NOTIME = EXOG_TS + TS_FEATURES + PRICE_FEATURES\n",
    "TARGET_TS_FEATURES = TARGET_TS_FEATURES_NOTIME + TIME_FEATURES\n",
    "WINDOW_SIZE = 252 // 4  # 1 years trading, sampled across days\n",
    "PREDICTION_HORIZON = 1  # next 1 trading day\n",
    "\n",
    "print(f\"target factor used: {TARGET_FACTOR} for window: {WINDOW_SIZE} predicting: {PREDICTION_HORIZON} step\")\n",
    "\n",
    "def create_time_features(data_df):\n",
    "    \"\"\"\n",
    "    Encodes time cyclic features for a dataset with monthly sampling.\n",
    "    Including cyclic encoding for day and year.\n",
    "    :param data_df: The timeseries with a date in the format YYYY-MM-DD as index.\n",
    "    :return: data_df with added wave features for month, day, and year.\n",
    "    \"\"\"\n",
    "    if not isinstance(data_df.index, pd.DatetimeIndex):\n",
    "        raise ValueError(\"The DataFrame index must be a DateTimeIndex.\")\n",
    "\n",
    "    def _sin_transformer(period):\n",
    "        return FunctionTransformer(lambda x: np.sin(x / period * 2 * np.pi))\n",
    "\n",
    "    def _cos_transformer(period):\n",
    "        return FunctionTransformer(lambda x: np.cos(x / period * 2 * np.pi))\n",
    "\n",
    "    def rbf_transform(x, period, input_range):\n",
    "        x_normalized = (x - input_range[0]) / (input_range[1] - input_range[0]) * period\n",
    "        return np.exp(-0.5 * ((x_normalized - period / 2) / 1.0) ** 2)\n",
    "\n",
    "    data_df[DAY_RBF] = rbf_transform(data_df.index.day, 31, (1, 31))\n",
    "    data_df[MONTH_RBF] = rbf_transform(data_df.index.month, 12, (1, 12))\n",
    "    data_df[Q_RBF] = rbf_transform((data_df.index.month - 1) // 3 + 1, 4, (1, 4))\n",
    "    min_year, max_year = data_df.index.year.min(), data_df.index.year.max()\n",
    "    data_df[BIZ_RBF] = rbf_transform(\n",
    "        data_df.index.year, max_year - min_year, (min_year, max_year)\n",
    "    )\n",
    "\n",
    "    return data_df\n",
    "\n",
    "def create_features_df(tickers, data_df):\n",
    "    \"\"\"\n",
    "    Create all exogenous features that lead to our target etf.\n",
    "        - if the trading day close is higher than the open.\n",
    "        - price log returns or 1D integrations.\n",
    "    :param tickers: All the timeseries with a date in the format YYYY-MM-DD as index.\n",
    "    :param data_df: Any pre-engineered features.\n",
    "    :return: data_df With TARGET_LABEL timeseries at column 0, and the rest are for conditioning.\n",
    "    \"\"\"\n",
    "\n",
    "    def _get_first_difference(data_df):\n",
    "        return data_df.pct_change().fillna(0)\n",
    "\n",
    "    def _get_log_returns(data_df):\n",
    "        return np.log(data_df / data_df.shift(1)).fillna(0)\n",
    "\n",
    "    IDX_COL = \"Open\"\n",
    "    price_transform = FunctionTransformer(_get_first_difference)\n",
    "\n",
    "    data_df[PRICE_FEATURES] = price_transform.fit_transform(data_df[PRICE_FEATURES])\n",
    "    rates_df = tickers.get(RATES_INDEX)\n",
    "\n",
    "    for index in EXOG_TS:\n",
    "        index_df = tickers.get(index)\n",
    "        transformed_data = price_transform.fit_transform(index_df[[IDX_COL]])\n",
    "        data_df[index] = transformed_data\n",
    "\n",
    "    data_df = data_df.fillna(0)\n",
    "\n",
    "    return data_df\n",
    "\n",
    "IS_CLASSIFICATION = True\n",
    "\n",
    "def prepare_data(tickers, classify=IS_CLASSIFICATION, to_normalize=True):\n",
    "    \"\"\"\n",
    "    Utility function to prepare the data.\n",
    "    :data_df dataframe: dataframe with `window_size` months of data to predict the `window_size`+`horizon`.\n",
    "    :param window_size: int, length of the input sequence\n",
    "    :param horizon: int, forecasting horizon, defaults to 1\n",
    "    :return: Array in the shape of (n_samples, n_steps, n_features)\n",
    "    \"\"\"\n",
    "    y_scaler = None, None\n",
    "    data_df = tickers.get(TARGET_ETF).copy()\n",
    "    close_df = data_df['Close'].shift(-1)\n",
    "    if classify:\n",
    "        open_df = data_df['Open'].shift(-1)\n",
    "        # Calculate the target label: 1 if next day's close is higher than its open.\n",
    "        data_df[TARGET_LABEL] = (close_df > (open_df * (1 - TARGET_FACTOR))).astype(int)\n",
    "    else:\n",
    "        # A target variable with a large spread of values, in turn, may result in large error gradient values\n",
    "        # causing weight values to change dramatically. We predict tomorrows close.\n",
    "        y_scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "        y_scaled = y_scaler.fit_transform(\n",
    "            close_df.values * (1 - TARGET_FACTOR)\n",
    "        )\n",
    "        data_df[TARGET_LABEL] = y_scaled.flatten()\n",
    "\n",
    "    data_df = create_features_df(tickers, data_df)\n",
    "    data_df_normalized = None\n",
    "    if to_normalize:\n",
    "        data_df_normalized = normalize(data_df[TARGET_TS_FEATURES_NOTIME], norm=\"l2\")\n",
    "    else:\n",
    "        data_df_normalized = data_df[TARGET_TS_FEATURES_NOTIME]\n",
    "    data_df_normalized = pd.DataFrame(\n",
    "        data_df_normalized, columns=TARGET_TS_FEATURES_NOTIME\n",
    "    )\n",
    "    data_df_normalized = pd.concat(\n",
    "        [\n",
    "            data_df[TARGET_LABEL].reset_index(drop=True),\n",
    "            data_df_normalized.reset_index(drop=True),\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    "    if any(feature in TIME_FEATURES for feature in TARGET_TS_FEATURES):\n",
    "        # Don't normalize these.\n",
    "        data_df = create_time_features(data_df)\n",
    "        data_df_normalized = pd.concat(\n",
    "            [\n",
    "                data_df[TIME_FEATURES].reset_index(drop=True),\n",
    "                data_df_normalized.reset_index(drop=True),\n",
    "            ],\n",
    "            axis=1,\n",
    "        )\n",
    "    # we might drop the first and last row of data (shifts and difference => NAs)\n",
    "    return data_df_normalized.dropna(axis=0), y_scaler\n",
    "\n",
    "data_df, y_scaler = prepare_data(tickers, to_normalize=True)\n",
    "data_df.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c200b8",
   "metadata": {},
   "source": [
    "## DIM Reduction & Feature Selection\n",
    "\n",
    "### Dataset Balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bce56bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_counts = data_df[TARGET_LABEL].value_counts()\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.bar(label_counts.index.astype(str), label_counts.values, color=['blue', 'red'])\n",
    "plt.xlabel('Target Label')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of Target Labels (Close>Open)')\n",
    "plt.xticks([0, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abad8572",
   "metadata": {},
   "source": [
    "### PCA Analysis\n",
    "\n",
    "We carry out a PCA to assert which features have most info to our target.\n",
    "\n",
    "We later have a choice to use the components themselves instead of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0262feac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "MAX_VARIANCE = 0.95\n",
    "\n",
    "pca = PCA()\n",
    "data_df_pca = data_df.drop(columns=[TARGET_LABEL])\n",
    "xdata = pca.fit_transform(data_df_pca)\n",
    "\n",
    "cum_var_exp = np.cumsum(pca.explained_variance_ratio_)\n",
    "num_components = np.argmax(cum_var_exp >= MAX_VARIANCE) + 1 # type: ignore\n",
    "print(\n",
    "    f\"Max components for {MAX_VARIANCE*100}% variance: {num_components} out of {data_df.shape[1]}\"\n",
    ")\n",
    "\n",
    "feature_importance = pca.components_\n",
    "loadings_df = pd.DataFrame(\n",
    "    feature_importance[:num_components,:num_components],\n",
    "    columns=data_df_pca.columns[:num_components],\n",
    "    index=[f\"PC{i+1}\" for i in range(len(data_df_pca.columns[:num_components]))],\n",
    ").T\n",
    "summed_loadings = np.sum(abs(feature_importance), axis=0)\n",
    "summed_loadings_df = pd.DataFrame(\n",
    "    summed_loadings, index=data_df_pca.columns, columns=[\"Sum\"]\n",
    ").sort_values(ascending=False, by=\"Sum\")\n",
    "\n",
    "fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(18, 6))\n",
    "\n",
    "summed_loadings_df.plot(kind=\"bar\", legend=False, ax=axes[0])\n",
    "axes[0].set_title(\"Summed Loadings Across All Principal Components\")\n",
    "axes[0].set_ylabel(\"Summed Loadings\")\n",
    "axes[0].set_xlabel(\"Features\")\n",
    "axes[0].tick_params(axis=\"x\", labelrotation=45)\n",
    "axes[0].legend()\n",
    "loadings_df.plot(kind=\"bar\", legend=False, ax=axes[1])\n",
    "axes[1].set_title(f\"Loadings for {num_components} Principal Components\")\n",
    "axes[1].set_ylabel(\"Loadings\")\n",
    "axes[1].set_xlabel(\"Features\")\n",
    "axes[1].tick_params(axis=\"x\", labelrotation=45)\n",
    "axes[1].legend(loc='upper left', bbox_to_anchor=(1, 1))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b3d01c",
   "metadata": {},
   "source": [
    "## Feature Evaluation\n",
    "\n",
    "The Information Coefficient (IC), calculated here using Spearman's rank correlation. It measures the strength and direction of a monotonic relationship between two variables.\n",
    "\n",
    "IC is useful because it can capture nonlinear relationships between variables. A high absolute value of the IC indicates a strong relationship, which could be either positive or negative.\n",
    "\n",
    "The Mutual Information (MI) quantifies the amount of information obtained about one random variable through observing the other random variable. It's measuring how much information each feature in our dataset provides about the target variable.\n",
    "\n",
    "Unlike correlation, MI can capture any kind of relationship between variables, not just linear or monotonic. It's particularly useful in feature selection for machine learning because it can identify features that are informative about the target variable.\n",
    "\n",
    "Effective feature selection can improve model performance by reducing overfitting, improving accuracy, and reducing training time. It helps in simplifying models to make them more interpretable, and in focusing on the most relevant data, which can be particularly important in datasets with a large number of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f5cc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "\n",
    "ic = {}\n",
    "for column in data_df.columns:\n",
    "    if column != TARGET_LABEL:\n",
    "        corr, p_val = spearmanr(data_df[TARGET_LABEL], data_df[column])\n",
    "        ic[column] = [corr, p_val]\n",
    "\n",
    "ic_df = pd.DataFrame(ic, index=[\"IC\", \"p-value\"]).T\n",
    "\n",
    "mi = mutual_info_regression(X=data_df.drop(columns=[TARGET_LABEL]), y=data_df[TARGET_LABEL])\n",
    "mi_series = pd.Series(mi, index=data_df.drop(columns=[TARGET_LABEL]).columns)\n",
    "metrics = pd.concat(\n",
    "    [\n",
    "        mi_series.to_frame(\"Mutual Information\"),\n",
    "        ic_df[\"IC\"].to_frame(\"Information Coefficient\"),\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "metrics = metrics.sort_values(by=\"Mutual Information\", ascending=False)\n",
    "ax = metrics.plot.bar(figsize=(18, 4), rot=45)\n",
    "ax.set_xlabel(\"Features\")\n",
    "ax.set_ylabel(\"Scores\")\n",
    "ax.set_title(\"Feature Evaluation: MI & IC\")\n",
    "sns.despine()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fee3767",
   "metadata": {},
   "source": [
    "## Feature Selection\n",
    "\n",
    "Let's do a union of the important features from both PCA and IC/MI methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fcad60",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_FEATURES_COUNT = 10\n",
    "\n",
    "features_pca = summed_loadings_df.head(MAX_FEATURES_COUNT).index.tolist()\n",
    "features_miic = (metrics.head(MAX_FEATURES_COUNT).index.tolist())\n",
    "print(F\"Top {MAX_FEATURES_COUNT} PCA Loadings: {features_pca}\")\n",
    "print(F\"Top {MAX_FEATURES_COUNT} MI/IC: {features_miic}\")\n",
    "SELECTED_FEATURES = list(set(features_pca) & set(features_miic))\n",
    "\n",
    "print(f\"Selected {len(SELECTED_FEATURES)} features: {SELECTED_FEATURES}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc9a383",
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECTED_FEATURES =  data_df.drop(columns=EXOG_TS+[\"Open\", \"Volume\"]).columns.to_list()\n",
    "EXOG_TS.remove(TARGET_ETF)\n",
    "SELECTED_EXOG = EXOG_TS\n",
    "\n",
    "SELECTED_EXOG, SELECTED_FEATURES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3cb19b",
   "metadata": {},
   "source": [
    "### Multicolinearity and Variance Inflation Factor (VIF)\n",
    "\n",
    "For feature not to be colinear, they have to score between 1 to 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489e9928",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.tools.tools import add_constant\n",
    "\n",
    "data_with_constant = add_constant(data_df[SELECTED_FEATURES + SELECTED_EXOG])\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"feature\"] = data_with_constant.columns\n",
    "vif_data[\"VIF\"] = [\n",
    "    variance_inflation_factor(data_with_constant.values, i)\n",
    "    for i in range(data_with_constant.shape[1])\n",
    "]\n",
    "\n",
    "print(vif_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a29993",
   "metadata": {},
   "source": [
    "## Time Windows Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4a97a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_windows(\n",
    "    data_df,\n",
    "    target_df,\n",
    "    prime_ts=SELECTED_FEATURES,\n",
    "    exog_ts=SELECTED_EXOG,\n",
    "    window_size=WINDOW_SIZE,\n",
    "    horizon=PREDICTION_HORIZON,\n",
    "):\n",
    "    \"\"\"\n",
    "    Create input and target windows suitable for TCN model.\n",
    "    :param data: DataFrame with shape (n_samples, n_features)\n",
    "    :param features: List of strings, names of the feature columns\n",
    "    :param target: String, name of the target column\n",
    "    :param window_size: int, length of the input sequence.\n",
    "    :param horizon: int, forecasting horizon.\n",
    "    :return: Array in the shape of (n_samples, n_steps, n_features)\n",
    "    \"\"\"\n",
    "    X, Xexog, y = [], [], []\n",
    "    for i in tqdm(\n",
    "        range(len(data_df) - window_size - horizon + 1), desc=f\"Encoding Widows of {window_size}\"\n",
    "    ):\n",
    "        input_window = data_df[prime_ts].iloc[i : i + window_size].values\n",
    "        X.append(input_window)\n",
    "        input_window = data_df[exog_ts].iloc[i : i + window_size].values\n",
    "        Xexog.append(input_window)\n",
    "        target_window = target_df.iloc[i  : i + window_size].values\n",
    "        y.append(target_window)\n",
    "    return np.array(X), np.array(Xexog), np.array(y)\n",
    "\n",
    "X, Xexog, y = prepare_windows(data_df, data_df[TARGET_LABEL])\n",
    "\n",
    "assert not np.any(pd.isna(X)) and not np.any(pd.isna(Xexog))\n",
    "\n",
    "print(f\"Label shape encoded: {y.shape}\")\n",
    "print(f\"Data shapes for prime TS: {X.shape}, exog TS: {Xexog.shape}\")\n",
    "print(f\"First window: {X[:1]}\")\n",
    "print(f\"First window targets: {y[:1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e574d6",
   "metadata": {},
   "source": [
    "# TCN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421953b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.activations import relu, tanh, sigmoid\n",
    "from tensorflow.keras.layers import (\n",
    "    SpatialDropout1D,\n",
    "    Dropout,\n",
    "    Dense,\n",
    "    Conv1D,\n",
    "    Layer,\n",
    "    Add,\n",
    "    Input,\n",
    "    Concatenate,\n",
    "    Flatten,\n",
    "    LeakyReLU,\n",
    "    ReLU,\n",
    "    Lambda,\n",
    "    BatchNormalization,\n",
    "    Reshape,\n",
    ")\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "assert tf.__version__ >= \"2.0\"\n",
    "\n",
    "NONLINEAR_ACTIVATION = LeakyReLU(alpha=0.01)\n",
    "\n",
    "\n",
    "class GatedActivationBlock(Layer):\n",
    "    \"\"\"\n",
    "    This layer applies a gated activation mechanism to its input.\n",
    "    The input tensor is expected to have its last dimension divisible by 2.\n",
    "    The first half of the channels are passed through a tanh activation,\n",
    "    and the second half through a sigmoid to create a gating mechanism.\n",
    "    The final output is the product of the above.\n",
    "    \"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        n_filters = inputs.shape[-1] // 2\n",
    "        linear_output = tanh(inputs[..., :n_filters])\n",
    "        gate = sigmoid(inputs[..., n_filters:])\n",
    "        return linear_output * gate\n",
    "\n",
    "class TCNBlock(Layer):\n",
    "    \"\"\"\n",
    "    TCN Residual Block that uses zero-padding to maintain `steps` value of the ouput equal to the one in the input.\n",
    "    Residual Block is obtained by stacking togeather (2x) the following:\n",
    "        - 1D Dilated Convolution\n",
    "        - ReLu\n",
    "        - Spatial Dropout\n",
    "    And adding the input after trasnforming it with a 1x1 Conv\n",
    "    forked and extended from: https://github.com/albertogaspar/dts/blob/master/dts/models/TCN.py\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        filters=1,\n",
    "        kernel_size=2,\n",
    "        dilation_rate=1,\n",
    "        kernel_initializer=\"glorot_normal\",\n",
    "        bias_initializer=\"glorot_normal\",\n",
    "        kernel_regularizer=None,\n",
    "        bias_regularizer=None,\n",
    "        use_bias=False,\n",
    "        dropout_rate=0.0,\n",
    "        layer_id=None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\" \"\n",
    "        Arguments\n",
    "            filters: Integer, the dimensionality of the output space\n",
    "                (i.e. the number of output filters in the convolution).\n",
    "            kernel_size: An integer or tuple/list of a single integer,\n",
    "                specifying the length of the 1D convolution window.\n",
    "            dilation_rate: an integer or tuple/list of a single integer, specifying\n",
    "                the dilation rate to use for dilated convolution.\n",
    "                Usually dilation rate increases exponentially with the depth of the network.\n",
    "            activation: Activation function to use\n",
    "                If you don't specify anything, no activation is applied\n",
    "                (ie. \"linear\" activation: `a(x) = x`).\n",
    "            use_bias: Boolean, whether the layer uses a bias vector.\n",
    "            kernel_initializer: Initializer for the `kernel` weights matrix\n",
    "            bias_initializer: Initializer for the bias vector\n",
    "            kernel_regularizer: Regularizer function applied to the `kernel` weights matrix\n",
    "            bias_regularizer: Regularizer function applied to the bias vector\n",
    "                (see [regularizer](../regularizers.md)).\n",
    "        # Input shape\n",
    "            3D tensor with shape: `(batch, steps, n_features)`\n",
    "        # Output shape\n",
    "            3D tensor with shape: `(batch, steps, filters)`\n",
    "        \"\"\"\n",
    "        super(TCNBlock, self).__init__(**kwargs)\n",
    "        assert dilation_rate is not None and dilation_rate > 0 and filters > 0 and kernel_size > 0\n",
    "\n",
    "        self.filters = filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.dilation_rate = dilation_rate\n",
    "        self.kernel_initializer = kernel_initializer\n",
    "        self.bias_initializer = bias_initializer\n",
    "        self.kernel_regularizer = kernel_regularizer\n",
    "        self.bias_regularizer = bias_regularizer\n",
    "        self.use_bias = use_bias\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.layer_id = str(layer_id)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(TCNBlock, self).get_config()\n",
    "        config.update({\n",
    "            'filters': self.filters,\n",
    "            'kernel_size': self.kernel_size,\n",
    "            'dilation_rate': self.dilation_rate,\n",
    "            'kernel_initializer': self.kernel_initializer,\n",
    "            'bias_initializer': self.bias_initializer,\n",
    "            'kernel_regularizer': self.kernel_regularizer,\n",
    "            'bias_regularizer': self.bias_regularizer,\n",
    "            'use_bias': self.use_bias,\n",
    "            'dropout_rate': self.dropout_rate,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    def build(self, inputs):\n",
    "        # Capture feature set from the input\n",
    "        self.conv1 = Conv1D(\n",
    "            filters=self.filters,\n",
    "            kernel_size=self.kernel_size,\n",
    "            use_bias=self.use_bias,\n",
    "            bias_initializer=self.bias_initializer,\n",
    "            bias_regularizer=self.bias_regularizer,\n",
    "            kernel_initializer=self.kernel_initializer,\n",
    "            kernel_regularizer=self.kernel_regularizer,\n",
    "            padding=\"causal\",\n",
    "            dilation_rate=self.dilation_rate,\n",
    "            activation=NONLINEAR_ACTIVATION,\n",
    "            name=f\"Conv1D_1_{self.layer_id}\"\n",
    "        )\n",
    "        # Spatial dropout is specific to convolutions by dropping an entire timewindow,\n",
    "        # not to rely too heavily on specific features detected by the kernels.\n",
    "        self.dropout1 = SpatialDropout1D(\n",
    "            self.dropout_rate, trainable=True, name=f\"SpatialDropout1D_1_{self.layer_id}\"\n",
    "        )\n",
    "        # Capture a higher order feature set from the previous convolution\n",
    "        self.conv2 = Conv1D(\n",
    "            filters=self.filters,\n",
    "            kernel_size=self.kernel_size,\n",
    "            use_bias=self.use_bias,\n",
    "            bias_initializer=self.bias_initializer,\n",
    "            bias_regularizer=self.bias_regularizer,\n",
    "            kernel_initializer=self.kernel_initializer,\n",
    "            kernel_regularizer=self.kernel_regularizer,\n",
    "            padding=\"causal\",\n",
    "            dilation_rate=self.dilation_rate,\n",
    "            activation=NONLINEAR_ACTIVATION,\n",
    "            name=f\"Conv1D_2_{self.layer_id}\"\n",
    "        )\n",
    "        self.dropout2 = SpatialDropout1D(\n",
    "            self.dropout_rate, trainable=True, name=f\"SpatialDropout1D_2_{self.layer_id}\"\n",
    "        )\n",
    "        # The skip connection is an addition of the input to the block with the output of the second dropout layer.\n",
    "        # Solves vanishing gradient, carries info from earlier layers to later layers, allowing gradients to flow across this alternative path.\n",
    "        # Does not learn direct mappings, but differences (residuals) while keeping temporal context.\n",
    "        # Note how it keeps dims intact with kernel 1.\n",
    "        self.skip_out = Conv1D(\n",
    "            filters=self.filters,\n",
    "            kernel_size=1,\n",
    "            activation=\"relu\",\n",
    "            name=f\"Conv1D_skipconnection_{self.layer_id}\",\n",
    "        )\n",
    "        # This is the elementwise add for the residual connection and Conv1d 2's output\n",
    "        self.residual_out = Add(name=f\"residual_Add_{self.layer_id}\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.dropout1(x)\n",
    "        x = GatedActivationBlock()(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        # Residual output by adding the inputs back\n",
    "        skip_out_x = self.skip_out(inputs)\n",
    "        x = self.residual_out([x, skip_out_x])\n",
    "        return x, skip_out_x\n",
    "\n",
    "class ConditionalBlock(Layer):\n",
    "    \"\"\"\n",
    "    TCN condtioning Block that conditions a target timeseries to exogenous timeserieses.\n",
    "    The Block is obtained by stacking togeather the following:\n",
    "        - 1D Dilated Convolution for the main TS.\n",
    "        - 1D Dilated Convolution for the exog TSs.\n",
    "        - 1D Dilated skip layer for both to retain history.\n",
    "        - ReLu\n",
    "        - Spatial Dropout\n",
    "    And adding the input after trasnforming it with a 1x1 Conv\n",
    "    forked and extended from: https://github.com/albertogaspar/dts/blob/master/dts/models/TCN.py\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        filters=1,\n",
    "        kernel_size=2,\n",
    "        kernel_initializer=\"glorot_normal\",\n",
    "        bias_initializer=\"glorot_normal\",\n",
    "        kernel_regularizer=None,\n",
    "        bias_regularizer=None,\n",
    "        use_bias=False,\n",
    "        dropout_rate=0.01,\n",
    "        layer_id=None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super(ConditionalBlock, self).__init__(**kwargs)\n",
    "\n",
    "        assert filters > 0 and kernel_size > 0\n",
    "\n",
    "        self.filters = filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.kernel_initializer = kernel_initializer\n",
    "        self.bias_initializer = bias_initializer\n",
    "        self.kernel_regularizer = kernel_regularizer\n",
    "        self.bias_regularizer = bias_regularizer\n",
    "        self.use_bias = use_bias\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.layer_id = str(layer_id)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(ConditionalBlock, self).get_config()\n",
    "        config.update({\n",
    "            'filters': self.filters,\n",
    "            'kernel_size': self.kernel_size,\n",
    "            'kernel_initializer': self.kernel_initializer,\n",
    "            'bias_initializer': self.bias_initializer,\n",
    "            'kernel_regularizer': self.kernel_regularizer,\n",
    "            'bias_regularizer': self.bias_regularizer,\n",
    "            'use_bias': self.use_bias,\n",
    "            'dropout_rate': self.dropout_rate,\n",
    "            'id': self.layer_id\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    def build(self, inputs):\n",
    "        self.main_conv = Conv1D(\n",
    "            filters=self.filters,\n",
    "            kernel_size=self.kernel_size,\n",
    "            use_bias=self.use_bias,\n",
    "            bias_initializer=self.bias_initializer,\n",
    "            bias_regularizer=self.bias_regularizer,\n",
    "            kernel_initializer=self.kernel_initializer,\n",
    "            kernel_regularizer=self.kernel_regularizer,\n",
    "            padding=\"causal\",\n",
    "            activation=NONLINEAR_ACTIVATION,\n",
    "            name=f\"Conv1D_Conditional_1\",\n",
    "        )\n",
    "        self.dropout1 = SpatialDropout1D(\n",
    "            self.dropout_rate, trainable=True, name=f\"SpatialDropout1D_1_{self.layer_id}\"\n",
    "        )\n",
    "        self.main_skip_conn = Conv1D(\n",
    "            filters=self.filters,\n",
    "            kernel_size=1,\n",
    "            activation=\"relu\",\n",
    "            name=f\"Skip_Conditional_1\",\n",
    "        )\n",
    "        self.cond_conv = Conv1D(\n",
    "            filters=self.filters,\n",
    "            kernel_size=self.kernel_size,\n",
    "            use_bias=self.use_bias,\n",
    "            bias_initializer=self.bias_initializer,\n",
    "            bias_regularizer=self.bias_regularizer,\n",
    "            kernel_initializer=self.kernel_initializer,\n",
    "            kernel_regularizer=self.kernel_regularizer,\n",
    "            padding=\"causal\",\n",
    "            activation=NONLINEAR_ACTIVATION,\n",
    "            name=f\"Conv1D_Conditional_2\",\n",
    "        )\n",
    "        self.cond_skip_conn = Conv1D(\n",
    "            filters=self.filters,\n",
    "            kernel_size=1,\n",
    "            activation=\"relu\",\n",
    "            name=f\"Skip_Conditional_2\",\n",
    "        )\n",
    "        self.dropout2 = SpatialDropout1D(\n",
    "            self.dropout_rate, trainable=True, name=f\"SpatialDropout1D_2_{self.layer_id}\"\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        Will apply causal convolutions to every TS and concatenate the results.\n",
    "        :param inputs: Array\n",
    "            A list where inputs[0] is the main input and inputs[1] are the conditional inputs\n",
    "        :return: Array\n",
    "            Tensor of concatenated results.\n",
    "        \"\"\"\n",
    "        main_input, cond_input = inputs[0], inputs[1] if len(inputs) > 1 else None\n",
    "\n",
    "        x = self.main_conv(main_input)\n",
    "        x = self.dropout1(x)\n",
    "        skip_out_x = self.main_skip_conn(main_input)\n",
    "        x = Add()([x, skip_out_x])\n",
    "        if cond_input is not None:\n",
    "            cond_x = self.cond_conv(cond_input)\n",
    "            x = self.dropout2(x)\n",
    "            cond_skip_out_x = self.cond_skip_conn(cond_input)\n",
    "            cond_x = Add()([cond_x, cond_skip_out_x])\n",
    "\n",
    "            x = Concatenate(axis=-1)([x, cond_x])\n",
    "        return x\n",
    "\n",
    "\n",
    "def TCN(\n",
    "    input_shape,\n",
    "    dense_units=None,\n",
    "    conditioning_shapes=None,\n",
    "    output_horizon=1,\n",
    "    num_filters=32,\n",
    "    num_layers=1,\n",
    "    kernel_size=2,\n",
    "    dilation_rate=2,\n",
    "    kernel_initializer=\"glorot_normal\",\n",
    "    bias_initializer=\"glorot_normal\",\n",
    "    kernel_regularizer=None,\n",
    "    bias_regularizer=None,\n",
    "    use_bias=False,\n",
    "    dropout_rate=0.01,\n",
    "):\n",
    "    \"\"\"\n",
    "    Tensorflow TCN Model builder.\n",
    "    forked and extended from: https://github.com/albertogaspar/dts/blob/master/dts/models/TCN.py\n",
    "    see: https://www.tensorflow.org/api_docs/python/tf/keras/Model\n",
    "    see: https://www.tensorflow.org/guide/keras/making_new_layers_and_models_via_subclassing#the_model_class\n",
    "    see: https://www.tensorflow.org/api_docs/python/tf/keras/regularizers/L2\n",
    "\n",
    "    :param layers: int\n",
    "        Number of layers for the network. Defaults to 1 layer.\n",
    "    :param filters: int\n",
    "        the number of output filters in the convolution. Defaults to 32.\n",
    "    :param kernel_size: int or tuple\n",
    "        the length of the 1D convolution window\n",
    "    :param dilation_rate: int\n",
    "        the dilation rate to use for dilated convolution. Defaults to 1.\n",
    "    :param output_horizon: int\n",
    "        the output horizon.\n",
    "    \"\"\"\n",
    "    MAX_FILTER = 512\n",
    "\n",
    "    main_input = Input(shape=input_shape, name=\"main_input\")\n",
    "    cond_input = (\n",
    "        Input(shape=conditioning_shapes, name=\"exog_input\")\n",
    "        if conditioning_shapes is not None and len(conditioning_shapes) > 0\n",
    "        else None\n",
    "    )\n",
    "    cond_x = None\n",
    "    x = main_input\n",
    "    if cond_input is not None:\n",
    "        x = cond_x = ConditionalBlock(\n",
    "            filters=num_filters,\n",
    "            kernel_size=kernel_size,\n",
    "            kernel_initializer=kernel_initializer,\n",
    "            bias_initializer=bias_initializer,\n",
    "            kernel_regularizer=kernel_regularizer,\n",
    "            bias_regularizer=bias_regularizer,\n",
    "            use_bias=use_bias,\n",
    "            dropout_rate=dropout_rate,\n",
    "        )([main_input] + [cond_input])\n",
    "    skip_to_last = []\n",
    "    for i in range(num_layers):\n",
    "        x, x_skip = TCNBlock(\n",
    "            filters=num_filters,\n",
    "            kernel_size=kernel_size,\n",
    "            dilation_rate=dilation_rate ** (i + 1),\n",
    "            kernel_initializer=kernel_initializer,\n",
    "            bias_initializer=bias_initializer,\n",
    "            kernel_regularizer=kernel_regularizer,\n",
    "            bias_regularizer=bias_regularizer,\n",
    "            use_bias=use_bias,\n",
    "            dropout_rate=dropout_rate,\n",
    "            layer_id=i,\n",
    "        )(x)\n",
    "        # Last x can be dropped\n",
    "        skip_to_last.append(x_skip)\n",
    "    x = relu(Add()(skip_to_last))\n",
    "    x = Conv1D(num_filters, kernel_size=1, padding=\"causal\", activation=\"relu\")(x)\n",
    "    x = Conv1D(output_horizon, kernel_size=1, activation=\"sigmoid\", padding=\"causal\", name=f\"Conv_Classifier\")(x)\n",
    "\n",
    "    # Drop the channels, get last lag, 1 freature: direction.\n",
    "    # [batch_size, time_steps, features]\n",
    "    # x = Lambda(lambda x: x[:, :, 0])(x)\n",
    "\n",
    "    model = Model(\n",
    "        inputs=[main_input, cond_input] if cond_input is not None else [main_input],\n",
    "        outputs=x,\n",
    "        name=\"TCN_Conditional_Model\",\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e8329e",
   "metadata": {},
   "source": [
    "# Train Model\n",
    "Using tensorbards: `tensorboard --logdir logs/hparam_tuning`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba82142",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from tensorflow.config.experimental import list_physical_devices, set_memory_growth\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# https://learn.microsoft.com/en-us/windows/ai/directml/gpu-tensorflow-plugin\n",
    "gpus = list_physical_devices(\"GPU\")\n",
    "print(\"Num GPUs Available: \", len(gpus))\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "\n",
    "DELETE_OLD_LOGS = True\n",
    "\n",
    "LOG_BASEPATH = \"./logs\"\n",
    "if DELETE_OLD_LOGS and os.path.exists(LOG_BASEPATH):\n",
    "    assert os.path.isdir(LOG_BASEPATH)\n",
    "    shutil.rmtree(LOG_BASEPATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8ff228",
   "metadata": {},
   "source": [
    "## Hyper Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d53654",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboard.plugins.hparams import api as hp\n",
    "from tensorflow.keras.losses import BinaryCrossentropy, BinaryFocalCrossentropy\n",
    "from tensorflow.keras.metrics import AUC\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import ParameterGrid, TimeSeriesSplit\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "OOS_SPLIT = 0.1\n",
    "VAL_SPLIT = 0.20\n",
    "EPOCHS = 300\n",
    "PATIENCE_EPOCHS = 15\n",
    "BATCH_SIZE = 124\n",
    "FILTER = 128\n",
    "HIDDEN_DENSE = [124]\n",
    "BIAS = True\n",
    "DROPRATE = 0.5\n",
    "POOL_SIZE = 8\n",
    "KERNEL_SIZE = 2\n",
    "DILATION_RATE = 1\n",
    "MAX_LAYERS = 5\n",
    "REG_WEIGHTS = 0.05\n",
    "LEARN_RATE = 0.01\n",
    "MODEL_LOG_DIR = f'{LOG_BASEPATH}/{datetime.now().strftime(\"%d%H%M%S\")}'\n",
    "CV_SPLITS = 5\n",
    "TARGET_METRIC = \"auc\" if IS_CLASSIFICATION else \"mae\"\n",
    "LOSS = BinaryFocalCrossentropy(apply_class_balancing=True, from_logits=True) if IS_CLASSIFICATION else TARGET_METRIC\n",
    "METRICS = [AUC(name=TARGET_METRIC),\n",
    "           BinaryCrossentropy(from_logits=True), 'accuracy'] if IS_CLASSIFICATION else [\"mae\", \"mse\", \"mape\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82617c46",
   "metadata": {},
   "source": [
    "### Build, CV and Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f481f001",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.regularizers import L2, L1L2\n",
    "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard,ModelCheckpoint,ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.summary import create_file_writer\n",
    "from tensorflow.debugging.experimental import enable_dump_debug_info\n",
    "\n",
    "# enable_dump_debug_info(LOG_BASEPATH, tensor_debug_mode=\"FULL_HEALTH\", circular_buffer_size=-1)\n",
    "\n",
    "# See: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.ParameterGrid.html\n",
    "# See paper: https://www.mdpi.com/2076-3417/10/7/2322\n",
    "HP_NUM_FILTERS = hp.HParam(\"num_filters\", hp.Discrete([FILTER * 3, FILTER * 2, FILTER]))\n",
    "HP_KERNEL_SIZE = hp.HParam(\"kernel_size\", hp.Discrete([KERNEL_SIZE * 2, KERNEL_SIZE]))\n",
    "HP_BATCH_SIZE = hp.HParam(\"batch_size\", hp.Discrete([BATCH_SIZE]))\n",
    "HP_EPOCHS = hp.HParam(\"epochs\", hp.Discrete([EPOCHS]))\n",
    "HP_DILATION_RATE = hp.HParam(\"dilation_rate\", hp.Discrete([DILATION_RATE]))\n",
    "HP_DROPOUT_RATE = hp.HParam(\"dropout_rate\", hp.Discrete([DROPRATE, 0.5]))\n",
    "HP_NUM_LAYERS = hp.HParam(\"num_layers\", hp.Discrete([MAX_LAYERS * 2, MAX_LAYERS, 1]))\n",
    "HP_REG_WEIGHTS = hp.HParam(\"reg_weight\", hp.Discrete([REG_WEIGHTS, 0.0005]))\n",
    "HP_LEARNING_RATE = hp.HParam(\"learning_rate\", hp.Discrete([LEARN_RATE]))\n",
    "HP_PATIENCE = hp.HParam(\"patience\", hp.Discrete([PATIENCE_EPOCHS]))\n",
    "HP_BIAS = hp.HParam(\"bias\", hp.Discrete([BIAS]))\n",
    "HP_HIDDEN_DENSE = hp.HParam(\"dense_units\", hp.Discrete([\n",
    "    \"0\",\n",
    "    \"8\",\n",
    "    \"128_32\",\n",
    "    \"64_32_16\",\n",
    "    \"128_128_64_32\",\n",
    "]))\n",
    "HPARAMS = [\n",
    "    HP_NUM_FILTERS,\n",
    "    HP_KERNEL_SIZE,\n",
    "    HP_BATCH_SIZE,\n",
    "    HP_EPOCHS,\n",
    "    HP_DILATION_RATE,\n",
    "    HP_DROPOUT_RATE,\n",
    "    HP_NUM_LAYERS,\n",
    "    HP_REG_WEIGHTS,\n",
    "    HP_LEARNING_RATE,\n",
    "    HP_PATIENCE,\n",
    "    HP_BIAS,\n",
    "    HP_HIDDEN_DENSE\n",
    "]\n",
    "\n",
    "\n",
    "def build_tcn(\n",
    "    input_shape, X, y, Xt=None, yt=None,\n",
    "    conditioning_shapes=None,\n",
    "    val_split=VAL_SPLIT,\n",
    "    output_horizon=PREDICTION_HORIZON,\n",
    "    num_filters=FILTER,\n",
    "    kernel_size=KERNEL_SIZE,\n",
    "    num_layers=MAX_LAYERS,\n",
    "    dilation_rate=DILATION_RATE,\n",
    "    kernel_regularizer=L1L2(l1=REG_WEIGHTS, l2=REG_WEIGHTS//10),\n",
    "    bias_regularizer=L1L2(l1=REG_WEIGHTS, l2=REG_WEIGHTS//10),\n",
    "    dropout_rate=DROPRATE,\n",
    "    dense_units=HIDDEN_DENSE,\n",
    "    lr=LEARN_RATE,\n",
    "    patience=PATIENCE_EPOCHS,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    use_bias=BIAS,\n",
    "    loss=LOSS,\n",
    "):\n",
    "    model = TCN(\n",
    "        input_shape=input_shape,\n",
    "        conditioning_shapes=conditioning_shapes,\n",
    "        dense_units=dense_units,\n",
    "        output_horizon=output_horizon,\n",
    "        num_filters=num_filters,\n",
    "        kernel_size=kernel_size,\n",
    "        num_layers=num_layers,\n",
    "        dilation_rate=dilation_rate,\n",
    "        kernel_regularizer=kernel_regularizer,\n",
    "        bias_regularizer=bias_regularizer,\n",
    "        use_bias=use_bias,\n",
    "        dropout_rate=dropout_rate,\n",
    "    )\n",
    "    model.compile(loss=loss, optimizer=Adam(lr), metrics=METRICS)\n",
    "    callbacks = [EarlyStopping(\n",
    "                    patience=patience,\n",
    "                    monitor=f\"val_{TARGET_METRIC}\",\n",
    "                    restore_best_weights=True,\n",
    "                ),\n",
    "                TensorBoard(log_dir=MODEL_LOG_DIR, profile_batch=10),\n",
    "                ReduceLROnPlateau(\n",
    "                    monitor=f\"val_{TARGET_METRIC}\",\n",
    "                    factor=0.3,\n",
    "                    patience=PATIENCE_EPOCHS//2,\n",
    "                    verbose=1,\n",
    "                    min_delta=0.00001,\n",
    "                )]\n",
    "    if Xt is not None:\n",
    "        history = model.fit(\n",
    "            X,\n",
    "            y,\n",
    "            validation_data=(Xt, yt),\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            callbacks=callbacks,\n",
    "            verbose=1,\n",
    "        )\n",
    "    else:\n",
    "        history = model.fit(\n",
    "            X,\n",
    "            y,\n",
    "            validation_split=val_split,\n",
    "            epochs=EPOCHS,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            callbacks=callbacks,\n",
    "            verbose=1,\n",
    "        )\n",
    "    return model, history\n",
    "\n",
    "def grid_search_build_tcn(\n",
    "    input_shape, X, y, hparams=HPARAMS, file_name=\"best_params.json\"\n",
    "):\n",
    "    def decode_dense_units(config_str):\n",
    "        return [int(unit) for unit in config_str.split('_')]\n",
    "\n",
    "    def _save_best_params(best_params, best_loss, best_metric, file_name=\"best_params.json\"):\n",
    "        with open(file_name, \"w\") as file:\n",
    "            json.dump({\"best_params\": best_params, \"best_loss\": best_loss, \"best_metric\": best_metric}, file)\n",
    "\n",
    "    with create_file_writer(f\"{MODEL_LOG_DIR}/hparam_tuning\").as_default():\n",
    "        hp.hparams_config(\n",
    "            hparams=hparams,\n",
    "            metrics=[hp.Metric(TARGET_METRIC, display_name=TARGET_METRIC)],\n",
    "        )\n",
    "    grid = list(ParameterGrid({h.name: h.domain.values for h in hparams}))\n",
    "    best_model = None\n",
    "    best_loss = np.inf\n",
    "    best_metric = np.inf if not IS_CLASSIFICATION else -np.inf\n",
    "    best_params = None\n",
    "    best_history = None\n",
    "    for hp_values in tqdm(grid, desc=\"Grid Search..\"):\n",
    "        try:\n",
    "            dense_units = decode_dense_units(hp_values[\"dense_units\"])\n",
    "            model, history = build_tcn(input_shape, X, y,\n",
    "                                        output_horizon=PREDICTION_HORIZON,\n",
    "                                        num_filters=hp_values[\"num_filters\"],\n",
    "                                        kernel_size=hp_values[\"kernel_size\"],\n",
    "                                        num_layers=hp_values[\"num_layers\"],\n",
    "                                        dilation_rate=hp_values[\"dilation_rate\"],\n",
    "                                        kernel_regularizer=L1L2(l1=hp_values[\"reg_weight\"], l2=hp_values[\"reg_weight\"]),\n",
    "                                        bias_regularizer=L1L2(l1=hp_values[\"reg_weight\"], l2=hp_values[\"reg_weight\"]),\n",
    "                                        dropout_rate=hp_values[\"dropout_rate\"],\n",
    "                                        epochs=hp_values[\"epochs\"],\n",
    "                                        lr=hp_values[\"learning_rate\"],\n",
    "                                        dense_units=dense_units,\n",
    "                                        use_bias=hp_values[\"bias\"],)\n",
    "            loss = np.min(history.history[f\"val_loss\"])\n",
    "            metric = np.min(history.history[f\"val_{TARGET_METRIC}\"])\n",
    "            if (loss < best_loss) or ((best_metric < metric and not IS_CLASSIFICATION) or (best_metric > metric and IS_CLASSIFICATION)):\n",
    "                print(f\"best metric: {metric}\")\n",
    "                print(f\"best loss: {loss}\")\n",
    "                print(f\"best params: {hp_values}\")\n",
    "                best_history = history\n",
    "                best_loss = loss\n",
    "                best_metric = metric\n",
    "                best_model = model\n",
    "                best_params = hp_values\n",
    "                _save_best_params(best_params, best_loss, best_metric, file_name)\n",
    "        except Exception as e:\n",
    "            print(f\"Grid Search ERROR on params: {hp_values}\\n\", e)\n",
    "\n",
    "    return best_model, best_history\n",
    "\n",
    "\n",
    "def train_cv_model(X, y, input_shape, n_splits=CV_SPLITS):\n",
    "    results = []\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "    for train_index, test_index in tscv.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        try:\n",
    "            model = build_tcn(input_shape, X_train, y_train, X_test, y_test)\n",
    "            result = model.evaluate(X_test, y_test)\n",
    "            results.append(result)\n",
    "        except Exception as e:\n",
    "            print(f\"CV error!\", e)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8007549",
   "metadata": {},
   "source": [
    "### Shape and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92eb87e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "MODEL_DIR = f\"./models/{datetime.now().strftime('%Y%m%d-%H')}\"\n",
    "\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "assert os.path.exists(MODEL_DIR)\n",
    "\n",
    "LOAD_MODEL_CACHE = False\n",
    "GRID_SEARCH_TRAIN = False\n",
    "\n",
    "OOS_SIZE = round(len(X) * OOS_SPLIT)\n",
    "\n",
    "x_oos = X[-OOS_SIZE:]\n",
    "X = X[:-OOS_SIZE]\n",
    "Xexog_oos = Xexog[-OOS_SIZE:]\n",
    "Xexog = Xexog[:-OOS_SIZE]\n",
    "y_oos = y[-OOS_SIZE:]\n",
    "y = y[:-OOS_SIZE]\n",
    "\n",
    "if LOAD_MODEL_CACHE:\n",
    "    model = load_model(f\"{MODEL_DIR}/tcn.h5\")\n",
    "    with open(f\"{MODEL_DIR}/history.pkl\", 'rb') as file:\n",
    "        loaded_history = pickle.load(file)\n",
    "else:\n",
    "    input_shape = (\n",
    "        WINDOW_SIZE,\n",
    "        1 if len(X.shape) < 3 else X.shape[2],\n",
    "    )  # if we have no additonal features X.shape[1]\n",
    "    conditioning_shapes = (WINDOW_SIZE, Xexog.shape[2])\n",
    "    print(f\"Model logs for Tensorboard available here: {MODEL_LOG_DIR}\")\n",
    "    print(f\"Input Shape: {input_shape}\")\n",
    "\n",
    "    assert not np.any(np.isnan(X))\n",
    "    assert not np.any(np.isnan(Xexog))\n",
    "    assert not np.any(np.isnan(y))\n",
    "\n",
    "    if GRID_SEARCH_TRAIN:\n",
    "        model, history = grid_search_build_tcn(input_shape, X, y)\n",
    "    else:\n",
    "        model, history = build_tcn( # Xexog\n",
    "            input_shape, [X], y, # conditioning_shapes=conditioning_shapes\n",
    "        )\n",
    "    model.save(f\"{MODEL_DIR}/tcn.h5\")\n",
    "    with open(f\"{MODEL_DIR}/history.pkl\", \"wb\") as file:\n",
    "        pickle.dump(history.history, file)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba472f4c",
   "metadata": {},
   "source": [
    "### Tensorboard for profiling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0601dd19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboard import program\n",
    "\n",
    "tb = program.TensorBoard()\n",
    "tb.configure(argv=[None, '--logdir', LOG_BASEPATH, '--bind_all'])\n",
    "url = tb.launch()\n",
    "print(f\"TensorBoard started at {url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34348cd",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b862085f",
   "metadata": {},
   "outputs": [],
   "source": [
    "VAL_SIZE = round(len(X) * VAL_SPLIT)\n",
    "\n",
    "train_data = X[:-VAL_SIZE]\n",
    "test_data = X[-VAL_SIZE:]\n",
    "train_exog_data = Xexog[:-VAL_SIZE]\n",
    "test_exog_data = Xexog[-VAL_SIZE:]\n",
    "ytrain_data = y[:-VAL_SIZE]\n",
    "ytest_data = y[-VAL_SIZE:]\n",
    "if not IS_CLASSIFICATION and y_scaler is not None:\n",
    "    ytrain_data = y_scaler.inverse_transform(ytrain_data.reshape(-1, 1))\n",
    "    ytest_data = y_scaler.inverse_transform(ytest_data.reshape(-1, 1))\n",
    "\n",
    "y_pred = model.predict([train_data])\n",
    "print(f\"y_pred shape: {y_pred.shape}\")\n",
    "yt_pred = model.predict([test_data])\n",
    "if IS_CLASSIFICATION:\n",
    "    y_pred_orig = y_pred.copy()\n",
    "    yt_pred_orig = yt_pred.copy()\n",
    "    y_pred = (y_pred > 0.5).astype(int)\n",
    "    yt_pred = (yt_pred > 0.5).astype(int)\n",
    "else:\n",
    "    y_pred = y_scaler.inverse_transform(y_pred.reshape(-1, 1))\n",
    "    yt_pred = y_scaler.inverse_transform(yt_pred.reshape(-1, 1))\n",
    "print(f\"Prediction shape: {yt_pred.shape} vs test data shape: {ytest_data.shape}\")\n",
    "print(f\"Test data 1 horizon sample: {ytest_data[0]}\")\n",
    "print(f\"Prediction data 1 horizon sample: {yt_pred[0].T}\")\n",
    "print(f\"Prediction 1 horizon sample: {yt_pred.flatten()[0]} VS {ytest_data.flatten()[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088dd6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if IS_CLASSIFICATION:\n",
    "    proba = np.round((yt_pred_orig.flatten()*100).astype(float), decimals=5)\n",
    "    print(proba[:15])\n",
    "\n",
    "    unique, counts = np.unique(y_pred, return_counts=True)\n",
    "    print(dict(zip(unique, counts)))\n",
    "\n",
    "    try:\n",
    "        pred_df = pd.DataFrame({'True': ytest_data.flatten(), 'Proba': proba})\n",
    "\n",
    "        plt.figure(figsize=(8, 4))\n",
    "        sns.histplot(data=pred_df, x='Proba', hue='True', bins=2, element='bars', stat=\"density\")\n",
    "\n",
    "        plt.xlabel('Predicted Proba')\n",
    "        plt.ylabel('Density')\n",
    "        plt.title('Distribution of Probabilities by Labels')\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(f\"Error plotting histogram: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9ee892",
   "metadata": {},
   "source": [
    "### Confusion Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbb7061",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.math import confusion_matrix\n",
    "if IS_CLASSIFICATION:\n",
    "    cm_train = confusion_matrix(ytrain_data.flatten(), y_pred.flatten())\n",
    "    cm_test = confusion_matrix(ytest_data.flatten(), yt_pred.flatten())\n",
    "\n",
    "    cm_train_np = cm_train.numpy()\n",
    "    cm_test_np = cm_test.numpy()\n",
    "\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.heatmap(cm_train_np, annot=True, fmt='d', cmap='Blues', xticklabels=[0, 1], yticklabels=[0, 1])\n",
    "    plt.xlabel('Predicted Labels')\n",
    "    plt.ylabel('True Labels')\n",
    "    plt.title('Training Set Confusion Matrix')\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.heatmap(cm_test_np, annot=True, fmt='d', cmap='Reds', xticklabels=[0, 1], yticklabels=[0, 1])\n",
    "    plt.xlabel('Predicted Labels')\n",
    "    plt.title('Test Set Confusion Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf9703b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    mean_squared_error,\n",
    "    r2_score,\n",
    "    mean_absolute_percentage_error,\n",
    "    mean_absolute_error,\n",
    "    accuracy_score, precision_score, recall_score,\n",
    "    f1_score, roc_auc_score, roc_curve, auc\n",
    ")\n",
    "\n",
    "def directional_accuracy(y, y_pred):\n",
    "    a = np.array(y).flatten()\n",
    "    p = np.array(y_pred).flatten()\n",
    "\n",
    "    a_dir = np.sign(np.diff(a))\n",
    "    p_dir = np.sign(np.diff(p))\n",
    "    correct_dirs = np.sum(a_dir == p_dir)\n",
    "    acc = correct_dirs / len(a_dir)\n",
    "\n",
    "    return acc\n",
    "\n",
    "\n",
    "print(f\"shapes y_pred: {y_pred.shape} and yt_pred: {yt_pred.shape}\")\n",
    "\n",
    "if not IS_CLASSIFICATION:\n",
    "    mae_train = mean_absolute_error(ytrain_data, y_pred)\n",
    "    mae_test = mean_absolute_error(ytest_data, yt_pred)\n",
    "    mse_train = mean_squared_error(ytrain_data, y_pred)\n",
    "    mse_test = mean_squared_error(ytest_data, yt_pred)\n",
    "    rmse_train = mean_squared_error(ytrain_data, y_pred, squared=False)\n",
    "    rmse_test = mean_squared_error(ytest_data, yt_pred, squared=False)\n",
    "    mape_train = mean_absolute_percentage_error(ytrain_data, y_pred) * 100\n",
    "    mape_test = mean_absolute_percentage_error(ytest_data, yt_pred) * 100\n",
    "    r2 = r2_score(\n",
    "        ytest_data,\n",
    "        yt_pred,\n",
    "    )\n",
    "    da_test = directional_accuracy(ytest_data, yt_pred)\n",
    "    metrics_df = pd.DataFrame(\n",
    "        {\n",
    "            \"MAE\": [mae_test],\n",
    "            \"MSE\": [mse_test],\n",
    "            \"RMSE\": [rmse_test],\n",
    "            \"MAPE\": [mape_test],\n",
    "            \"R2\": [r2],\n",
    "            \"DA\": [da_test],\n",
    "        }\n",
    "    )\n",
    "else:\n",
    "    accuracy_train = accuracy_score(ytrain_data.flatten(), y_pred.flatten())\n",
    "    accuracy_test = accuracy_score(ytest_data.flatten(), yt_pred.flatten())\n",
    "    precision_test = precision_score(ytest_data.flatten(), yt_pred.flatten())\n",
    "    recall_test = recall_score(ytest_data.flatten(), yt_pred.flatten())\n",
    "    f1_test = f1_score(ytest_data.flatten(), yt_pred.flatten())\n",
    "    roc_auc_test = roc_auc_score(ytest_data.flatten(), yt_pred_orig.flatten())\n",
    "    metrics_df = pd.DataFrame({\n",
    "        \"Accuracy\": [accuracy_test],\n",
    "        \"Precision\": [precision_test],\n",
    "        \"Recall\": [recall_test],\n",
    "        \"F1 Score\": [f1_test],\n",
    "        \"ROC AUC\": [roc_auc_test],\n",
    "    }, index=[\"Test\"])\n",
    "\n",
    "fig, axs = plt.subplots(3 if IS_CLASSIFICATION else 2, 1, figsize=(12, 10))\n",
    "axs[0].plot(history.history[\"loss\"], label=\"Train loss\")\n",
    "axs[0].plot(history.history[\"val_loss\"], label=\"Validation loss\")\n",
    "axs[0].set_xlabel(\"Epochs\")\n",
    "axs[0].set_ylabel(\"Loss\")\n",
    "axs[0].legend()\n",
    "axs[1].plot(\n",
    "    history.history[TARGET_METRIC],\n",
    "    label=f\"Train {TARGET_METRIC}\",\n",
    ")\n",
    "axs[1].plot(\n",
    "    history.history[f\"val_{TARGET_METRIC}\"],\n",
    "    label=f\"Test {TARGET_METRIC}\",\n",
    "    color=\"k\",\n",
    ")\n",
    "if not IS_CLASSIFICATION:\n",
    "    axs[1].axhline(mae_test, color=\"b\", linestyle=\"--\", label=\"Train Sample MAE\")\n",
    "    axs[1].axhline(mae_train, color=\"b\", linestyle=\"-\", label=\"Test Sample MAE\")\n",
    "    axs[1].axhline(rmse_train, color=\"r\", linestyle=\"--\", label=\"Train Sample RMSE\")\n",
    "    axs[1].axhline(rmse_test, color=\"r\", linestyle=\"-\", label=\"Test Sample RMSE\")\n",
    "else:\n",
    "    axs[1].axhline(accuracy_train, color=\"r\", linestyle=\"--\", label=\"Train Acc\")\n",
    "    axs[1].axhline(accuracy_test, color=\"r\", linestyle=\"-\", label=\"Test Acc\")\n",
    "    axs[1].axhline(precision_test, color=\"g\", linestyle=\"--\", label=\"Test Prec\")\n",
    "    axs[1].axhline(recall_test, color=\"g\", linestyle=\"-\", label=\"Test Rec\")\n",
    "axs[1].set_xlabel(\"Epochs\")\n",
    "axs[1].set_ylabel(\"Score\")\n",
    "axs[1].legend()\n",
    "\n",
    "if IS_CLASSIFICATION:\n",
    "    fpr, tpr, _ = roc_curve(ytest_data.flatten(), yt_pred_orig.flatten())\n",
    "    roc_auc_test = auc(fpr, tpr)\n",
    "    axs[2].plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc_test)\n",
    "    axs[2].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='ROC Line')\n",
    "    axs[2].set_xlabel('False Positive Rate')\n",
    "    axs[2].set_ylabel('True Positive Rate')\n",
    "    axs[2].legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4b4c45",
   "metadata": {},
   "source": [
    "# Walk Forwards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf8ad22",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_oos_pred_raw = model.predict([x_oos])\n",
    "y_oos_pred = (y_oos_pred_raw > 0.5).astype(int)\n",
    "\n",
    "print(f\"Prediction shape: {y_oos_pred.shape} vs test data shape: {y_oos.shape}\")\n",
    "print(f\"Test data 1 horizon sample: {y_oos[0]}\")\n",
    "print(f\"Predicted data 1 horizon sample: {y_oos_pred[0]}\")\n",
    "print(f\"Prediction 1 horizon sample: {y_oos_pred.flatten()[0]} VS {y_oos.flatten()[0]}\")\n",
    "\n",
    "metrics_oos_df = None\n",
    "if IS_CLASSIFICATION:\n",
    "    accuracy_test = accuracy_score(y_oos.flatten(), y_oos_pred.flatten())\n",
    "    precision_test = precision_score(y_oos.flatten(), y_oos_pred.flatten())\n",
    "    recall_test = recall_score(y_oos.flatten(), y_oos_pred.flatten())\n",
    "    f1_test = f1_score(y_oos.flatten(), y_oos_pred.flatten())\n",
    "    roc_auc_test = roc_auc_score(y_oos.flatten(), y_oos_pred_raw.flatten())\n",
    "    metrics_oos_df = pd.DataFrame({\n",
    "        \"Accuracy\": [accuracy_test],\n",
    "        \"Precision\": [precision_test],\n",
    "        \"Recall\": [recall_test],\n",
    "        \"F1 Score\": [f1_test],\n",
    "        \"ROC AUC\": [roc_auc_test],\n",
    "    }, index=[\"Test\"])\n",
    "\n",
    "metrics_oos_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e9a9c5",
   "metadata": {
    "papermill": {
     "duration": 0.037345,
     "end_time": "2023-11-05T18:01:12.947046",
     "exception": false,
     "start_time": "2023-11-05T18:01:12.909701",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "![]()\n",
    "\n",
    "## References\n",
    "\n",
    "- [YFinance Github](https://github.com/ranaroussi/yfinance)\n",
    "- [Vanguard All World excluding US](https://investor.vanguard.com/investment-products/etfs/profile/veu)\n",
    "\n",
    "\n",
    "## Github\n",
    "\n",
    "Article here is also available on [Github]()\n",
    "\n",
    "Kaggle notebook available [here]()\n",
    "\n",
    "\n",
    "## Media\n",
    "\n",
    "All media used (in the form of code or images) are either solely owned by me, acquired through licensing, or part of the Public Domain and granted use through Creative Commons License.\n",
    "\n",
    "## CC Licensing and Use\n",
    "\n",
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc/4.0/\"><img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by-nc/4.0/88x31.png\" /></a><br />This work is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc/4.0/\">Creative Commons Attribution-NonCommercial 4.0 International License</a>."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 98.905855,
   "end_time": "2023-11-05T18:01:13.607303",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-11-05T17:59:34.701448",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
