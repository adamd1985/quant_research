{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4f2af90",
   "metadata": {
    "papermill": {
     "duration": 0.00682,
     "end_time": "2023-11-05T17:59:38.297125",
     "exception": false,
     "start_time": "2023-11-05T17:59:38.290305",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Machine Learning with Broad Stock Market Timeseries - a SARIMA Rollercoaster\n",
    "\n",
    "\n",
    "<a href=\"\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a827e09",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-11-05T17:59:38.335093Z",
     "iopub.status.busy": "2023-11-05T17:59:38.334737Z",
     "iopub.status.idle": "2023-11-05T18:01:02.444324Z",
     "shell.execute_reply": "2023-11-05T18:01:02.443308Z"
    },
    "papermill": {
     "duration": 84.119909,
     "end_time": "2023-11-05T18:01:02.447010",
     "exception": false,
     "start_time": "2023-11-05T17:59:38.327101",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "import dotenv\n",
    "%load_ext dotenv\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "IS_KAGGLE = os.getenv('IS_KAGGLE', 'True') == 'True'\n",
    "\n",
    "if IS_KAGGLE:\n",
    "    # Kaggle confgs\n",
    "    print('Running in Kaggle...')\n",
    "    %pip install yfinance\n",
    "    %pip install statsmodels\n",
    "    %pip install seaborn\n",
    "    %pip install itertools\n",
    "    %pip install scikit-learn\n",
    "\n",
    "    for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "        for filename in filenames:\n",
    "            print(os.path.join(dirname, filename))\n",
    "else:\n",
    "    print('Running Local...')\n",
    "\n",
    "import yfinance as yf\n",
    "from analysis_utils import load_ticker_prices_ts_df, load_ticker_ts_df\n",
    "\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f226f2c",
   "metadata": {},
   "source": [
    "# Financial Data\n",
    "\n",
    "Collect market time series data (like stock prices, trading volumes, etc.).\n",
    "Clean the data to handle missing values, outliers, or anomalies.\n",
    "Ensure the data is in a time series format, typically with a timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38727fc7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-05T18:01:02.470424Z",
     "iopub.status.busy": "2023-11-05T18:01:02.469635Z",
     "iopub.status.idle": "2023-11-05T18:01:03.256388Z",
     "shell.execute_reply": "2023-11-05T18:01:03.255137Z"
    },
    "papermill": {
     "duration": 0.801372,
     "end_time": "2023-11-05T18:01:03.258937",
     "exception": false,
     "start_time": "2023-11-05T18:01:02.457565",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "START_DATE = \"2009-01-01\"\n",
    "END_DATE = \"2023-12-31\"\n",
    "DATA_DIR = \"data\"\n",
    "PRICE_FEATURES = [\"Open\", \"High\", \"Low\"]\n",
    "INDEX = \"Date\"\n",
    "\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "TARGET_ETF = \"SPY\"  # S&P 500\n",
    "RATES_INDEX = \"^TNX\"  # 10 Year Treasury Note Yield\n",
    "VOLATILITY_INDEX = \"^VIX\"  # CBOE Volatility Index\n",
    "MACRO_INDEX = \"VEU\"  # Vanguard FTSE All-World ex-US ETF\n",
    "\n",
    "tickers_symbols = [\n",
    "    TARGET_ETF,\n",
    "    VOLATILITY_INDEX,\n",
    "    RATES_INDEX,\n",
    "    MACRO_INDEX,\n",
    "]\n",
    "\n",
    "tickers = {}\n",
    "for symbol in tickers_symbols:\n",
    "    cached_file_path = f\"{DATA_DIR}/{symbol}-{START_DATE}-{END_DATE}.csv\"\n",
    "\n",
    "    try:\n",
    "        if os.path.exists(cached_file_path):\n",
    "            df = pd.read_csv(cached_file_path, index_col=INDEX)\n",
    "            df.index = pd.to_datetime(df.index)\n",
    "            assert len(df) > 0\n",
    "        else:\n",
    "            df = yf.download(\n",
    "                symbol, start=START_DATE, end=END_DATE, progress=False, interval=\"1d\"\n",
    "            )\n",
    "            assert len(df) > 0\n",
    "            df.to_csv(cached_file_path)\n",
    "        df.fillna(0, inplace=True)\n",
    "        tickers[symbol] = df\n",
    "    except Exception as e:\n",
    "        print(f\"Error with {symbol}: {e}\")\n",
    "\n",
    "\n",
    "sp500_df = tickers.get(TARGET_ETF)\n",
    "vix_df = tickers.get(VOLATILITY_INDEX)\n",
    "t10ytt_df = tickers.get(RATES_INDEX)\n",
    "veu_df = tickers.get(MACRO_INDEX)\n",
    "\n",
    "assert not sp500_df[PRICE_FEATURES].isna().any().any()\n",
    "\n",
    "sp500_df.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122af07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp500_pr = ((1 + sp500_df[\"Close\"].pct_change()).cumprod() - 1) * 100\n",
    "veu_pr = ((1 + veu_df[\"Close\"].pct_change()).cumprod() - 1) * 100\n",
    "vix_pr = ((1 + vix_df[\"Close\"].pct_change()).cumprod() - 1) * 100\n",
    "tnx_pr = ((1 + t10ytt_df[\"Close\"].pct_change()).cumprod() - 1) * 100\n",
    "\n",
    "plt.figure(figsize=(16, 6))\n",
    "plt.plot(sp500_pr, label=TARGET_ETF)\n",
    "plt.plot(veu_pr, label=MACRO_INDEX, alpha=0.5)\n",
    "plt.plot(vix_pr, label=VOLATILITY_INDEX)\n",
    "plt.plot(tnx_pr, label=RATES_INDEX)\n",
    "\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c42ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "days_rising = sp500_df[sp500_df[\"Close\"] > sp500_df[\"Open\"]]\n",
    "days_not_rising = sp500_df[sp500_df[\"Close\"] <= sp500_df[\"Open\"]]\n",
    "days_rising_count = days_rising.groupby(days_rising.index.year).size()\n",
    "days_not_rising_count = days_not_rising.groupby(days_not_rising.index.year).size()\n",
    "\n",
    "total_days = sp500_df.groupby(sp500_df.index.year).size()\n",
    "percentage_rising = (days_rising_count / total_days) * 100\n",
    "\n",
    "yearly_counts = pd.DataFrame(\n",
    "    {\"Rising\": days_rising_count, \"Not Rising\": days_not_rising_count}\n",
    ")\n",
    "yearly_counts.plot(kind=\"bar\", color=[\"green\", \"red\"], figsize=(16, 6))\n",
    "plt.title(\"Days Rising vs Not Rising per Year\")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.tick_params(axis=\"x\", rotation=45)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Baseline Accuracy: {percentage_rising.mean():0.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47100631",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import (\n",
    "    SpatialDropout1D,\n",
    "    Dense,\n",
    "    Conv1D,\n",
    "    Layer,\n",
    "    Normalization,\n",
    "    Add,\n",
    "    Input,\n",
    "    Lambda,\n",
    ")\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "\n",
    "class TCNBlock(Layer):\n",
    "    \"\"\"\n",
    "    TCN Residual Block that uses zero-padding to maintain `steps` value of the ouput equal to the one in the input.\n",
    "    Residual Block is obtained by stacking togeather (2x) the following:\n",
    "        - 1D Dilated Convolution\n",
    "        - ReLu\n",
    "        - Spatial Dropout\n",
    "    And adding the input after trasnforming it with a 1x1 Conv\n",
    "    forked and extended from: https://github.com/albertogaspar/dts/blob/master/dts/models/TCN.py\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        filters=1,\n",
    "        kernel_size=2,\n",
    "        dilation_rate=None,\n",
    "        kernel_initializer=\"glorot_normal\",\n",
    "        bias_initializer=\"glorot_normal\",\n",
    "        kernel_regularizer=None,\n",
    "        bias_regularizer=None,\n",
    "        use_bias=False,\n",
    "        dropout_rate=0.0,\n",
    "        id=None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\" \"\n",
    "        Arguments\n",
    "            filters: Integer, the dimensionality of the output space\n",
    "                (i.e. the number of output filters in the convolution).\n",
    "            kernel_size: An integer or tuple/list of a single integer,\n",
    "                specifying the length of the 1D convolution window.\n",
    "            dilation_rate: an integer or tuple/list of a single integer, specifying\n",
    "                the dilation rate to use for dilated convolution.\n",
    "                Usually dilation rate increases exponentially with the depth of the network.\n",
    "            activation: Activation function to use\n",
    "                If you don't specify anything, no activation is applied\n",
    "                (ie. \"linear\" activation: `a(x) = x`).\n",
    "            use_bias: Boolean, whether the layer uses a bias vector.\n",
    "            kernel_initializer: Initializer for the `kernel` weights matrix\n",
    "            bias_initializer: Initializer for the bias vector\n",
    "            kernel_regularizer: Regularizer function applied to the `kernel` weights matrix\n",
    "            bias_regularizer: Regularizer function applied to the bias vector\n",
    "                (see [regularizer](../regularizers.md)).\n",
    "        # Input shape\n",
    "            3D tensor with shape: `(batch, steps, n_features)`\n",
    "        # Output shape\n",
    "            3D tensor with shape: `(batch, steps, filters)`\n",
    "        \"\"\"\n",
    "        super(TCNBlock, self).__init__(**kwargs)\n",
    "        self.filters = filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.dilation_rate = dilation_rate\n",
    "\n",
    "        # Capture feature set from the input\n",
    "        self.conv1 = Conv1D(\n",
    "            filters=filters,\n",
    "            kernel_size=kernel_size,\n",
    "            use_bias=use_bias,\n",
    "            bias_initializer=bias_initializer,\n",
    "            bias_regularizer=bias_regularizer,\n",
    "            kernel_initializer=kernel_initializer,\n",
    "            kernel_regularizer=kernel_regularizer,\n",
    "            padding=\"causal\",\n",
    "            dilation_rate=dilation_rate,\n",
    "            activation=\"relu\",\n",
    "            name=f\"Conv1D_1_{id}\",\n",
    "        )\n",
    "\n",
    "        # Spatial dropout is specific to convolutions by dropping an entire timewindow,\n",
    "        # not to rely too heavily on specific features detected by the kernels.\n",
    "        self.dropout1 = SpatialDropout1D(\n",
    "            dropout_rate, trainable=True, name=f\"SpatialDropout1D_1_{id}\"\n",
    "        )\n",
    "        # Capture a higher order feature set from the previous convolution\n",
    "        self.conv2 = Conv1D(\n",
    "            filters=filters,\n",
    "            kernel_size=kernel_size,\n",
    "            use_bias=use_bias,\n",
    "            bias_initializer=bias_initializer,\n",
    "            bias_regularizer=bias_regularizer,\n",
    "            kernel_initializer=kernel_initializer,\n",
    "            kernel_regularizer=kernel_regularizer,\n",
    "            padding=\"causal\",\n",
    "            dilation_rate=dilation_rate,\n",
    "            activation=\"relu\",\n",
    "            name=f\"Conv1D_2_{id}\",\n",
    "        )\n",
    "        self.dropout2 = SpatialDropout1D(\n",
    "            dropout_rate, trainable=True, name=f\"SpatialDropout1D_2_{id}\"\n",
    "        )\n",
    "\n",
    "        # The skip connection is an addition of the input to the block with the output of the second dropout layer.\n",
    "        # Solves vanishing gradient, carries info from earlier layers to later layers, allowing gradients to flow across this alternative path.\n",
    "        # Does not learn direct mappings, but differences (residuals) while keeping temporal context.\n",
    "        # Note how it keeps dims intact with kernel 1.\n",
    "        self.skip_out = Conv1D(\n",
    "            filters=filters,\n",
    "            kernel_size=1,\n",
    "            activation=\"linear\",\n",
    "            name=f\"Conv1D_skipconnection_{id}\",\n",
    "        )\n",
    "        # This is the elementwise add for the residual connection and Conv1d 2's output\n",
    "        self.residual_out = Add(name=f\"residual_Add_{id}\")\n",
    "\n",
    "    def apply_block(self, inputs):\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        # Residual output by adding the inputs back:\n",
    "        skip_out_x = self.skip_out(inputs)\n",
    "        x = self.residual_out([x, skip_out_x])\n",
    "        return x\n",
    "\n",
    "\n",
    "def TCN(\n",
    "    input_shape,\n",
    "    output_horizon=1,\n",
    "    num_filters=32,\n",
    "    num_layers=1,\n",
    "    kernel_size=2,\n",
    "    dilation_rate=2,\n",
    "    kernel_initializer=\"glorot_normal\",\n",
    "    bias_initializer=\"glorot_normal\",\n",
    "    kernel_regularizer=None,\n",
    "    bias_regularizer=None,\n",
    "    use_bias=False,\n",
    "    dropout_rate=0.0,\n",
    "):\n",
    "    \"\"\"\n",
    "    Tensorflow TCN Model builder.\n",
    "    forked and extended from: https://github.com/albertogaspar/dts/blob/master/dts/models/TCN.py\n",
    "    see: https://www.tensorflow.org/api_docs/python/tf/keras/Model\n",
    "    see: https://www.tensorflow.org/guide/keras/making_new_layers_and_models_via_subclassing#the_model_class\n",
    "    see: https://www.tensorflow.org/api_docs/python/tf/keras/regularizers/L2\n",
    "\n",
    "    :param layers: int\n",
    "        Number of layers for the network. Defaults to 1 layer.\n",
    "    :param filters: int\n",
    "        the number of output filters in the convolution. Defaults to 32.\n",
    "    :param kernel_size: int or tuple\n",
    "        the length of the 1D convolution window\n",
    "    :param dilation_rate: int\n",
    "        the dilation rate to use for dilated convolution. Defaults to 1.\n",
    "    :param output_horizon: int\n",
    "        the output horizon.\n",
    "    \"\"\"\n",
    "    x = inputs = Input(shape=input_shape)\n",
    "    for i in range(num_layers):\n",
    "        block = TCNBlock(\n",
    "            filters=num_filters,\n",
    "            kernel_size=kernel_size,\n",
    "            dilation_rate=dilation_rate**i,\n",
    "            kernel_initializer=kernel_initializer,\n",
    "            bias_initializer=bias_initializer,\n",
    "            kernel_regularizer=kernel_regularizer,\n",
    "            bias_regularizer=bias_regularizer,\n",
    "            use_bias=use_bias,\n",
    "            dropout_rate=dropout_rate,\n",
    "            id=i,\n",
    "        )\n",
    "        x = block.apply_block(x)\n",
    "    # Selects the last timestep and predict in the 1 DIM layer.\n",
    "    x = Lambda(lambda x: x[:, -output_horizon:, 0], name=\"lambda_last_timestep\")(x)\n",
    "    outputs = Dense(output_horizon, name=\"Dense_singleoutput\", activation=\"sigmoid\")(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs, name=\"TCN\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfdd0638",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5751b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "MONTH_SINE = \"month_sin\"\n",
    "MONTH_COS = \"month_cos\"\n",
    "DAY_SINE = \"day_sin\"\n",
    "DAY_COS = \"day_cos\"\n",
    "Q_SINE = \"quart_sin\"\n",
    "Q_COS = \"quart_cos\"\n",
    "BIZ_SINE = \"biz_sin\"\n",
    "BIZ_COS = \"biz_cos\"\n",
    "FACTOR_COSTS = 0.0025  # Assume 2.5% commissions.\n",
    "FACTOR_SPREAD = 0.0003  # assume 0.3% spread & slippage.\n",
    "FACTOR_AVG_MOVE = tickers.get(TARGET_ETF)[\"Close\"].tail(365 * 2).pct_change().mean() / 4\n",
    "TARGET_FACTOR = FACTOR_COSTS + FACTOR_SPREAD + FACTOR_AVG_MOVE\n",
    "TARGET = \"open_lt_close\"\n",
    "TIME_FEATURES = [\n",
    "    MONTH_SINE,\n",
    "    MONTH_COS,\n",
    "]  # [DAY_SINE, DAY_COS, MONTH_SINE, MONTH_COS, BIZ_SINE, BIZ_COS, Q_SINE, Q_COS]\n",
    "FEATURES = [\n",
    "    \"Volume\",\n",
    "    VOLATILITY_INDEX,\n",
    "]  # =  [\"Volume\", RATES_INDEX, MACRO_INDEX, VOLATILITY_INDEX]\n",
    "EXT_FEATURES = FEATURES + PRICE_FEATURES + TIME_FEATURES\n",
    "WINDOW_SIZE = int(252 / 4)  # 1 years trading, sampled across days\n",
    "PREDICTION_HORIZON = 1  # next 1 trading day\n",
    "\n",
    "print(f\"target factor used: {TARGET_FACTOR}\")\n",
    "\n",
    "\n",
    "def encode_timewindows(data_df, features, target, window_size, horizon):\n",
    "    \"\"\"\n",
    "    Create input and target windows suitable for TCN model.\n",
    "    :param data: DataFrame with shape (n_samples, n_features)\n",
    "    :param features: List of strings, names of the feature columns\n",
    "    :param target: String, name of the target column\n",
    "    :param window_size: int, length of the input sequence.\n",
    "    :param horizon: int, forecasting horizon.\n",
    "    :return: Array in the shape of (n_samples, n_steps, n_features)\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    for i in tqdm(\n",
    "        range(len(data_df) - window_size - horizon + 1), desc=\"Encoding Widows\"\n",
    "    ):\n",
    "        input_window = data_df[features].iloc[i : i + window_size].values\n",
    "        X.append(input_window)\n",
    "        if horizon == 1:\n",
    "            target_value = data_df[target].iloc[i + window_size]\n",
    "        else:\n",
    "            target_value = (\n",
    "                data_df[target].iloc[i + window_size : i + window_size + horizon].values\n",
    "            )\n",
    "        y.append(target_value)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "\n",
    "def create_time_features(data_df):\n",
    "    \"\"\"\n",
    "    Encodes time cyclic features for a dataset with monthly sampling.\n",
    "    Including cyclic encoding for day and year.\n",
    "    :param data_df: The timeseries with a date in the format YYYY-MM-DD as index.\n",
    "    :return: data_df with added wave features for month, day, and year.\n",
    "    \"\"\"\n",
    "    if not isinstance(data_df.index, pd.DatetimeIndex):\n",
    "        raise ValueError(\"The DataFrame index must be a DateTimeIndex.\")\n",
    "\n",
    "    def _sin_transformer(period):\n",
    "        return FunctionTransformer(lambda x: np.sin(x / period * 2 * np.pi))\n",
    "\n",
    "    def _cos_transformer(period):\n",
    "        return FunctionTransformer(lambda x: np.cos(x / period * 2 * np.pi))\n",
    "\n",
    "    months = data_df.index.month\n",
    "    data_df[MONTH_SINE] = _sin_transformer(12).fit_transform(months)\n",
    "    data_df[MONTH_COS] = _cos_transformer(12).fit_transform(months)\n",
    "    data_df[Q_SINE] = _sin_transformer(12 / 4).fit_transform(months)\n",
    "    data_df[Q_COS] = _cos_transformer(12 / 4).fit_transform(months)\n",
    "    data_df[BIZ_SINE] = _sin_transformer(12 * 5).fit_transform(months)\n",
    "    data_df[BIZ_COS] = _cos_transformer(12 * 5).fit_transform(months)\n",
    "    days = data_df.index.day\n",
    "    data_df[DAY_SINE] = _sin_transformer(365).fit_transform(days)\n",
    "    data_df[DAY_COS] = _cos_transformer(365).fit_transform(days)\n",
    "    return data_df\n",
    "\n",
    "\n",
    "def get_log_returns(data_df):\n",
    "    return np.log(data_df / data_df.shift(1)).fillna(0)\n",
    "\n",
    "\n",
    "def create_features_df(tickers):\n",
    "    \"\"\"\n",
    "    Create all exogenous features that lead to our target etf.\n",
    "        - if the trading day close is higher than the open.\n",
    "        - price log returns\n",
    "    \"\"\"\n",
    "    data_df = tickers.get(TARGET_ETF).copy()\n",
    "    data_df[TARGET] = (\n",
    "        (data_df[\"Open\"] * (1 + TARGET_FACTOR)) < (data_df[\"Close\"])\n",
    "    ).astype(int)\n",
    "\n",
    "    data_df[PRICE_FEATURES] = get_log_returns(data_df[PRICE_FEATURES])\n",
    "\n",
    "    rates_df = tickers.get(RATES_INDEX)\n",
    "    data_df[RATES_INDEX] = get_log_returns(rates_df[\"High\"])\n",
    "    macro_df = tickers.get(MACRO_INDEX)\n",
    "    data_df[MACRO_INDEX] = get_log_returns(macro_df[\"High\"])\n",
    "    vix_df = tickers.get(VOLATILITY_INDEX)\n",
    "    data_df[VOLATILITY_INDEX] = get_log_returns(vix_df[\"High\"])\n",
    "    data_df = data_df.fillna(0)\n",
    "\n",
    "    return data_df\n",
    "\n",
    "\n",
    "def prepare_data_and_windows(tickers, window=WINDOW_SIZE, horizon=PREDICTION_HORIZON):\n",
    "    \"\"\"\n",
    "    Utility function to prepare the data.\n",
    "    :data_df dataframe: dataframe with `window_size` months of data to predict the `window_size`+`horizon`.\n",
    "    :param window_size: int, length of the input sequence\n",
    "    :param horizon: int, forecasting horizon, defaults to 1\n",
    "    :return: Array in the shape of (n_samples, n_steps, n_features)\n",
    "    \"\"\"\n",
    "    data_df = create_features_df(tickers)\n",
    "\n",
    "    normalizer = Normalization(axis=-1)\n",
    "    normalizer.adapt(data_df[FEATURES])\n",
    "    data_df_normalized = normalizer(data_df[FEATURES])\n",
    "\n",
    "    data_df_normalized = pd.DataFrame(\n",
    "        data_df_normalized.numpy(), columns=FEATURES, index=data_df.index\n",
    "    )\n",
    "    data_df_normalized = pd.concat(\n",
    "        [data_df[TARGET], data_df[PRICE_FEATURES], data_df_normalized], axis=1\n",
    "    )\n",
    "    data_df_normalized = create_time_features(data_df_normalized)\n",
    "    X, y = encode_timewindows(data_df_normalized, EXT_FEATURES, TARGET, window, horizon)\n",
    "    print(\n",
    "        f\"FEATURES: {EXT_FEATURES}, TARGET: '{TARGET}', window: {WINDOW_SIZE}, horizon: {PREDICTION_HORIZON}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Shape unencoded (including target label and superflous features): {data_df.shape}\"\n",
    "    )\n",
    "    print(f\"Shape encoded (window and selected exog features only): {X.shape}\")\n",
    "    return X, y, normalizer\n",
    "\n",
    "\n",
    "# Yes, we are using the whole dataset not the training dataset.\n",
    "# See: https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit\n",
    "# we will tell keras to do a validation split, it will not fit on the validation data.\n",
    "\n",
    "X, y, normalizer = prepare_data_and_windows(tickers)\n",
    "print(f\"Label shape encoded: {y.shape}\")\n",
    "print(f\"Data shape: {X.shape}\")\n",
    "print(f\"First window exog normalized: {X[0,  :]}\")\n",
    "print(f\"First window targets: {y[:WINDOW_SIZE]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5555aaf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Last window exog normalized: {X[-1,  :]}\")\n",
    "print(f\"Last window targets: {y[-WINDOW_SIZE:]}\")\n",
    "\n",
    "input_shape = (WINDOW_SIZE, X.shape[2])\n",
    "input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecaabe9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique, counts = np.unique(y, return_counts=True)\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.bar(unique, counts, tick_label=unique)\n",
    "plt.title(\"Distribution of Close > Open == 1\")\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e8329e",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f481f001",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.regularizers import L2\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from tensorflow.keras.metrics import Precision\n",
    "\n",
    "from tensorflow_addons.metrics import F1Score\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.model_selection import ParameterGrid, TimeSeriesSplit\n",
    "\n",
    "import json\n",
    "\n",
    "\n",
    "VAL_SPLIT = 0.15\n",
    "\n",
    "EPOCHS = 100\n",
    "\n",
    "PATIENCE_EPOCHS = 10\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "FILTER = 128\n",
    "\n",
    "DROPRATE = 0.5\n",
    "\n",
    "POOL_SIZE = 5\n",
    "\n",
    "KERNEL_SIZE = 5\n",
    "\n",
    "DILATION_RATE = KERNEL_SIZE\n",
    "\n",
    "MAX_LAYERS = 5\n",
    "\n",
    "L2_REG = 0.005\n",
    "\n",
    "LEARN_RATE = 0.0001\n",
    "\n",
    "MODEL_LOG_DIR = f'./logs/{datetime.now().strftime(\"%m%d-%H%M%S\")}'\n",
    "\n",
    "CV_SPLITS = 5\n",
    "\n",
    "# See: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.ParameterGrid.html\n",
    "\n",
    "# See paper: https://www.mdpi.com/2076-3417/10/7/2322\n",
    "\n",
    "GRID = {\n",
    "\n",
    "    \"num_filters\": [32, 64, 128],\n",
    "\n",
    "    \"kernel_size\": [2, 3, 4],\n",
    "\n",
    "    \"batch_size\": [64, 128, 255],\n",
    "\n",
    "    \"epochs\": [25, 50, 100, 300],\n",
    "\n",
    "    \"dilation_rate\": [1, 2, 4],\n",
    "\n",
    "    \"dropout_rate\": [0.1, 0.2, 0.3],\n",
    "\n",
    "    \"num_layers\": [6, 5, 3],\n",
    "\n",
    "    \"l2_reg\": [0.005, 0.001, 0.01],\n",
    "\n",
    "    \"learning_rate\": [0.001, 0.01, 0.1],\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "print(f\"Model logs for Tensorboard available here: {MODEL_LOG_DIR}\")\n",
    "\n",
    "\n",
    "\n",
    "def grid_search(X, y, param_grid=GRID, file_name=\"best_params.json\"):\n",
    "\n",
    "    \"\"\"Runs for 3 days!!!\"\"\"\n",
    "\n",
    "\n",
    "    def _create_model(hyperparams):\n",
    "\n",
    "        model = TCN(\n",
    "\n",
    "            input_shape=input_shape,\n",
    "\n",
    "            output_horizon=PREDICTION_HORIZON,\n",
    "\n",
    "            num_filters=hyperparams[\"num_filters\"],\n",
    "\n",
    "            kernel_size=hyperparams[\"kernel_size\"],\n",
    "\n",
    "            num_layers=hyperparams[\"num_layers\"],\n",
    "\n",
    "            dilation_rate=hyperparams[\"dilation_rate\"],\n",
    "\n",
    "            kernel_regularizer=L2(l2=hyperparams[\"l2_reg\"]),\n",
    "\n",
    "            bias_regularizer=L2(l2=hyperparams[\"l2_reg\"]),\n",
    "\n",
    "            dropout_rate=hyperparams[\"dropout_rate\"],\n",
    "\n",
    "        )\n",
    "\n",
    "        optimizer = Adam(hyperparams[\"learning_rate\"])\n",
    "\n",
    "        model.compile(loss=\"mse\", optimizer=optimizer, metrics=[\"mse\", \"mae\", \"mape\"])\n",
    "\n",
    "        return model\n",
    "\n",
    "\n",
    "    def _save_best_params(best_params, best_loss, file_name=\"best_params.json\"):\n",
    "\n",
    "        with open(file_name, \"w\") as file:\n",
    "\n",
    "            json.dump({\"best_params\": best_params, \"best_loss\": best_loss}, file)\n",
    "\n",
    "\n",
    "    grid = list(ParameterGrid(param_grid))\n",
    "\n",
    "    best_model = None\n",
    "\n",
    "    best_loss = np.inf\n",
    "\n",
    "    best_params = None\n",
    "\n",
    "\n",
    "    for params in tqdm(grid, desc=\"Grid Search..\"):\n",
    "\n",
    "        model = _create_model(params)\n",
    "\n",
    "        callbacks = [EarlyStopping(patience=PATIENCE_EPOCHS, monitor=\"val_loss\")]\n",
    "\n",
    "        history = model.fit(\n",
    "\n",
    "            X,\n",
    "\n",
    "            y,\n",
    "\n",
    "            epochs=EPOCHS,\n",
    "\n",
    "            batch_size=BATCH_SIZE,\n",
    "\n",
    "            validation_split=VAL_SPLIT,\n",
    "\n",
    "            verbose=0,\n",
    "\n",
    "            callbacks=callbacks,\n",
    "\n",
    "        )\n",
    "\n",
    "        val_loss = np.min(history.history[\"val_loss\"])\n",
    "\n",
    "\n",
    "        if val_loss < best_loss:\n",
    "\n",
    "            best_loss = val_loss\n",
    "\n",
    "            best_model = model\n",
    "\n",
    "            best_params = params\n",
    "\n",
    "            _save_best_params(best_params, best_loss, file_name)\n",
    "\n",
    "\n",
    "    return best_model, best_loss\n",
    "\n",
    "\n",
    "\n",
    "def build_tcn(input_shape, X, y, Xt=None, yt=None, val_split=VAL_SPLIT):\n",
    "\n",
    "    model = TCN(\n",
    "\n",
    "        input_shape=input_shape,\n",
    "\n",
    "        output_horizon=PREDICTION_HORIZON,\n",
    "\n",
    "        num_filters=FILTER,\n",
    "\n",
    "        kernel_size=KERNEL_SIZE,\n",
    "\n",
    "        num_layers=MAX_LAYERS,\n",
    "\n",
    "        dilation_rate=DILATION_RATE,\n",
    "\n",
    "        kernel_regularizer=L2(l2=L2_REG),\n",
    "\n",
    "        bias_regularizer=L2(l2=L2_REG),\n",
    "\n",
    "    )\n",
    "\n",
    "    optimizer = Adam(LEARN_RATE)\n",
    "\n",
    "    metrics = [Precision()]\n",
    "\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=metrics)\n",
    "\n",
    "    callbacks = [\n",
    "\n",
    "        EarlyStopping(patience=25, monitor=\"val_loss\", restore_best_weights=True),\n",
    "\n",
    "        TensorBoard(log_dir=MODEL_LOG_DIR),\n",
    "\n",
    "    ]\n",
    "\n",
    "    history = model.fit(\n",
    "\n",
    "        X,\n",
    "\n",
    "        y,\n",
    "\n",
    "        validation_data=(Xt, yt) if Xt is not None else None,\n",
    "\n",
    "        validation_split=val_split if Xt is None else None,\n",
    "\n",
    "        epochs=EPOCHS,\n",
    "\n",
    "        batch_size=BATCH_SIZE,\n",
    "\n",
    "        callbacks=callbacks,\n",
    "\n",
    "        verbose=1,\n",
    "\n",
    "    )\n",
    "\n",
    "    return model, history\n",
    "\n",
    "\n",
    "\n",
    "def train_cv_model(X, y, input_shape, n_splits=CV_SPLITS):\n",
    "\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "\n",
    "    for train_index, test_index in tscv.split(X):\n",
    "\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        model = build_tcn(input_shape, X_train, y_train, X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "MODEL_DIR = f\"./models/{datetime.now().strftime('%Y%m%d-%H')}\"\n",
    "\n",
    "\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "assert os.path.exists(MODEL_DIR)\n",
    "\n",
    "\n",
    "# model = load_model(f\"{MODEL_DIR}/tcn.h5\")\n",
    "\n",
    "model, history = build_tcn(input_shape, X, y)\n",
    "\n",
    "model = load_model(f\"{MODEL_DIR}/tcn.h5\")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "84ed0e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "model.save(f\"{MODEL_DIR}/tcn.h5\")\n",
    "with open(f\"{MODEL_DIR}/history.pkl\", \"wb\") as file:\n",
    "    pickle.dump(history.history, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34348cd",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b862085f",
   "metadata": {},
   "outputs": [],
   "source": [
    "VAL_SIZE = round(len(X) * VAL_SPLIT)\n",
    "\n",
    "train_data = X[:-VAL_SIZE]\n",
    "test_data = X[-VAL_SIZE:]\n",
    "ytrain_data = y[:-VAL_SIZE]\n",
    "ytest_data = y[-VAL_SIZE:]\n",
    "print(f\"Test data shape: {ytest_data.shape}\")\n",
    "print(f\"Test data 1 horizon sample: {ytest_data[0]}\")\n",
    "\n",
    "y_pred = model.predict(train_data)\n",
    "yt_pred = model.predict(test_data)\n",
    "\n",
    "print(f\"Prediction shape: {yt_pred.shape}\")\n",
    "print(f\"Predition 1 horizon sample: {yt_pred[0].flatten()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf9703b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "y_pred_b = (y_pred >= 0.5).astype(int).flatten()\n",
    "yt_pred_b = (yt_pred >= 0.5).astype(int).flatten()\n",
    "\n",
    "y_flat = ytrain_data.flatten()\n",
    "yt_flat = ytest_data.flatten()\n",
    "\n",
    "print(f\"shapes y: {ytrain_data.shape} and yt_pred: {ytest_data.shape}\")\n",
    "print(f\"shapes y_pred: {y_pred.shape} and yt_pred: {yt_pred.shape}\")\n",
    "print(f\"Next + binary shapes y_pred: {y_pred_b.shape} and yt_pred: {yt_pred_b.shape}\")\n",
    "\n",
    "accuracy_train = accuracy_score(y_flat, y_pred_b)\n",
    "accuracy_test = accuracy_score(yt_flat, yt_pred_b)\n",
    "precision_train = precision_score(y_flat, y_pred_b)\n",
    "precision_test = precision_score(yt_flat, yt_pred_b)\n",
    "recall_train = recall_score(y_flat, y_pred_b)\n",
    "recall_test = recall_score(yt_flat, yt_pred_b)\n",
    "f1_train = f1_score(y_flat, y_pred_b)\n",
    "f1_test = f1_score(yt_flat, yt_pred_b)\n",
    "\n",
    "\n",
    "metrics_df = pd.DataFrame(\n",
    "    {\n",
    "        \"Accuracy\": [accuracy_train, accuracy_test],\n",
    "        \"Precision\": [precision_train, precision_test],\n",
    "        \"Recall\": [recall_train, recall_test],\n",
    "        \"F1 Score\": [f1_train, f1_test],\n",
    "    },\n",
    "    index=[\"Train\", \"Test\"],\n",
    ")\n",
    "\n",
    "fig, axs = plt.subplots(2, 1, figsize=(10, 10))\n",
    "axs[0].plot(history.history[\"loss\"], label=\"Train loss\")\n",
    "axs[0].plot(history.history[\"val_loss\"], label=\"Validation loss\")\n",
    "axs[0].set_xlabel(\"Epochs\")\n",
    "axs[0].set_ylabel(\"Loss\")\n",
    "axs[0].legend()\n",
    "\n",
    "axs[1].plot(\n",
    "    history.history[\"precision\"],\n",
    "    label=\"Train precision\",\n",
    "    linestyle=\"-.\",\n",
    ")\n",
    "axs[1].plot(\n",
    "    history.history[\"val_precision\"],\n",
    "    label=\"Test precision\",\n",
    "    linestyle=\"-\",\n",
    ")\n",
    "axs[1].axhline(\n",
    "    precision_train, linestyle=\"--\", label=\"Train Precision\", alpha=0.5, color=\"r\"\n",
    ")\n",
    "axs[1].axhline(\n",
    "    precision_test, linestyle=\"-\", label=\"Test Precision\", alpha=0.5, color=\"r\"\n",
    ")\n",
    "axs[1].axhline(f1_train, linestyle=\"--\", label=\"Train F1\", alpha=0.5, color=\"g\")\n",
    "axs[1].axhline(f1_test, linestyle=\"-\", label=\"Test F1\", alpha=0.5, color=\"g\")\n",
    "axs[1].set_xlabel(\"Epochs\")\n",
    "axs[1].set_ylabel(\"Accuracy/Precision\")\n",
    "axs[1].legend()\n",
    "\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e9a9c5",
   "metadata": {
    "papermill": {
     "duration": 0.037345,
     "end_time": "2023-11-05T18:01:12.947046",
     "exception": false,
     "start_time": "2023-11-05T18:01:12.909701",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "![]()\n",
    "\n",
    "## References\n",
    "\n",
    "- [YFinance Github](https://github.com/ranaroussi/yfinance)\n",
    "- [Vanguard All World excluding US](https://investor.vanguard.com/investment-products/etfs/profile/veu)\n",
    "\n",
    "\n",
    "## Github\n",
    "\n",
    "Article here is also available on [Github]()\n",
    "\n",
    "Kaggle notebook available [here]()\n",
    "\n",
    "\n",
    "## Media\n",
    "\n",
    "All media used (in the form of code or images) are either solely owned by me, acquired through licensing, or part of the Public Domain and granted use through Creative Commons License.\n",
    "\n",
    "## CC Licensing and Use\n",
    "\n",
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc/4.0/\"><img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by-nc/4.0/88x31.png\" /></a><br />This work is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc/4.0/\">Creative Commons Attribution-NonCommercial 4.0 International License</a>."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 98.905855,
   "end_time": "2023-11-05T18:01:13.607303",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-11-05T17:59:34.701448",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
