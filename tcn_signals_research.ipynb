{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4f2af90",
   "metadata": {
    "papermill": {
     "duration": 0.00682,
     "end_time": "2023-11-05T17:59:38.297125",
     "exception": false,
     "start_time": "2023-11-05T17:59:38.290305",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Machine Learning with Broad Stock Market Timeseries - a SARIMA Rollercoaster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eafa184f",
   "metadata": {
    "papermill": {
     "duration": 0.005778,
     "end_time": "2023-11-05T17:59:38.309438",
     "exception": false,
     "start_time": "2023-11-05T17:59:38.303660",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a href=\"\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fef99d6",
   "metadata": {
    "papermill": {
     "duration": 0.005624,
     "end_time": "2023-11-05T17:59:38.321019",
     "exception": false,
     "start_time": "2023-11-05T17:59:38.315395",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<!-- @import \"[TOC]\" {cmd=\"toc\" depthFrom=1 depthTo=6 orderedList=false} -->\n",
    "\n",
    "![]()\n",
    "\n",
    "\n",
    "## Prepare your Environment\n",
    "\n",
    "Have a jupyter environment ready, and `pip install` these libraries:\n",
    "- numpy\n",
    "- pandas\n",
    "- yfinance\n",
    "\n",
    "You'll need access to [analysis_utils](./analysis_utils.py) library for common functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a827e09",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-11-05T17:59:38.335093Z",
     "iopub.status.busy": "2023-11-05T17:59:38.334737Z",
     "iopub.status.idle": "2023-11-05T18:01:02.444324Z",
     "shell.execute_reply": "2023-11-05T18:01:02.443308Z"
    },
    "papermill": {
     "duration": 84.119909,
     "end_time": "2023-11-05T18:01:02.447010",
     "exception": false,
     "start_time": "2023-11-05T17:59:38.327101",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adamd\\AppData\\Local\\Temp\\ipykernel_22756\\3477214583.py:2: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Local...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\adamd\\\\workspace\\\\quant_research'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "import dotenv\n",
    "%load_ext dotenv\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "IS_KAGGLE = os.getenv('IS_KAGGLE', 'True') == 'True'\n",
    "\n",
    "if IS_KAGGLE:\n",
    "    # Kaggle confgs\n",
    "    print('Running in Kaggle...')\n",
    "    %pip install yfinance\n",
    "    %pip install statsmodels\n",
    "    %pip install seaborn\n",
    "    %pip install itertools\n",
    "    %pip install scikit-learn\n",
    "\n",
    "    for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "        for filename in filenames:\n",
    "            print(os.path.join(dirname, filename))\n",
    "else:\n",
    "    print('Running Local...')\n",
    "\n",
    "import yfinance as yf\n",
    "from analysis_utils import load_ticker_prices_ts_df, load_ticker_ts_df\n",
    "\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection and Preprocessing\n",
    "\n",
    "Collect market time series data (like stock prices, trading volumes, etc.).\n",
    "Clean the data to handle missing values, outliers, or anomalies.\n",
    "Ensure the data is in a time series format, typically with a timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38727fc7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-05T18:01:02.470424Z",
     "iopub.status.busy": "2023-11-05T18:01:02.469635Z",
     "iopub.status.idle": "2023-11-05T18:01:03.256388Z",
     "shell.execute_reply": "2023-11-05T18:01:03.255137Z"
    },
    "papermill": {
     "duration": 0.801372,
     "end_time": "2023-11-05T18:01:03.258937",
     "exception": false,
     "start_time": "2023-11-05T18:01:02.457565",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2023-12-22</th>\n",
       "      <td>4753.919922</td>\n",
       "      <td>4772.939941</td>\n",
       "      <td>4736.770020</td>\n",
       "      <td>4754.629883</td>\n",
       "      <td>4754.629883</td>\n",
       "      <td>3046770000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-26</th>\n",
       "      <td>4758.859863</td>\n",
       "      <td>4784.720215</td>\n",
       "      <td>4758.450195</td>\n",
       "      <td>4774.750000</td>\n",
       "      <td>4774.750000</td>\n",
       "      <td>2513910000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-27</th>\n",
       "      <td>4773.450195</td>\n",
       "      <td>4785.390137</td>\n",
       "      <td>4768.899902</td>\n",
       "      <td>4781.580078</td>\n",
       "      <td>4781.580078</td>\n",
       "      <td>2748450000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-28</th>\n",
       "      <td>4786.439941</td>\n",
       "      <td>4793.299805</td>\n",
       "      <td>4780.979980</td>\n",
       "      <td>4783.350098</td>\n",
       "      <td>4783.350098</td>\n",
       "      <td>2698860000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-29</th>\n",
       "      <td>4782.879883</td>\n",
       "      <td>4788.430176</td>\n",
       "      <td>4751.990234</td>\n",
       "      <td>4769.830078</td>\n",
       "      <td>4769.830078</td>\n",
       "      <td>3126060000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Open         High          Low        Close    Adj Close  \\\n",
       "Date                                                                          \n",
       "2023-12-22  4753.919922  4772.939941  4736.770020  4754.629883  4754.629883   \n",
       "2023-12-26  4758.859863  4784.720215  4758.450195  4774.750000  4774.750000   \n",
       "2023-12-27  4773.450195  4785.390137  4768.899902  4781.580078  4781.580078   \n",
       "2023-12-28  4786.439941  4793.299805  4780.979980  4783.350098  4783.350098   \n",
       "2023-12-29  4782.879883  4788.430176  4751.990234  4769.830078  4769.830078   \n",
       "\n",
       "                Volume  \n",
       "Date                    \n",
       "2023-12-22  3046770000  \n",
       "2023-12-26  2513910000  \n",
       "2023-12-27  2748450000  \n",
       "2023-12-28  2698860000  \n",
       "2023-12-29  3126060000  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "START_DATE = \"2013-01-01\"\n",
    "END_DATE = \"2023-12-31\"\n",
    "DATA_DIR = \"data\"\n",
    "\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "tickers = {\n",
    "    \"SP5\": \"^GSPC\",  # S&P 500\n",
    "    # \"VTW\": \"VT\",  # Vanguard Total World Stock ETF\n",
    "    # \"VIX\": \"^VIX\",  # CBOE Volatility Index\n",
    "    # \"10YTT\": \"^TNX\",  # 10 Year Treasury Note Yield\n",
    "    # \"VEU\": \"VEU\",  # Vanguard FTSE All-World ex-US ETF\n",
    "}\n",
    "\n",
    "dataframes = {}\n",
    "for symbol, ticker in tickers.items():\n",
    "    cached_file_path = f\"{DATA_DIR}/{symbol}-{END_DATE}.pkl\"\n",
    "\n",
    "    try:\n",
    "        if os.path.exists(cached_file_path):\n",
    "            df = pd.read_pickle(cached_file_path)\n",
    "        else:\n",
    "            df = yf.download(\n",
    "                ticker, start=START_DATE, end=END_DATE, progress=False, interval=\"1d\"\n",
    "            )\n",
    "            assert len(df) > 0\n",
    "            df.to_pickle(cached_file_path)\n",
    "        dataframes[symbol] = df\n",
    "    except Exception as e:\n",
    "        print(f\"Error with {symbol}: {e}\")\n",
    "\n",
    "sp500_df = dataframes.get(\"SP5\")\n",
    "\n",
    "sp500_df.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47100631",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import (\n",
    "    SpatialDropout1D,\n",
    "    Dense,\n",
    "    Conv1D,\n",
    "    Layer,\n",
    "    Normalization,\n",
    "    Add,\n",
    "    Input,\n",
    "    Lambda,\n",
    ")\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "\n",
    "class TCNBlock(Layer):\n",
    "    \"\"\"\n",
    "    TCN Residual Block that uses zero-padding to maintain `steps` value of the ouput equal to the one in the input.\n",
    "    Residual Block is obtained by stacking togeather (2x) the following:\n",
    "        - 1D Dilated Convolution\n",
    "        - ReLu\n",
    "        - Spatial Dropout\n",
    "    And adding the input after trasnforming it with a 1x1 Conv\n",
    "    forked and extended from: https://github.com/albertogaspar/dts/blob/master/dts/models/TCN.py\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        filters=1,\n",
    "        kernel_size=2,\n",
    "        dilation_rate=None,\n",
    "        kernel_initializer=\"glorot_normal\",\n",
    "        bias_initializer=\"glorot_normal\",\n",
    "        kernel_regularizer=None,\n",
    "        bias_regularizer=None,\n",
    "        use_bias=False,\n",
    "        dropout_rate=0.0,\n",
    "        id=None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\" \"\n",
    "        Arguments\n",
    "            filters: Integer, the dimensionality of the output space\n",
    "                (i.e. the number of output filters in the convolution).\n",
    "            kernel_size: An integer or tuple/list of a single integer,\n",
    "                specifying the length of the 1D convolution window.\n",
    "            dilation_rate: an integer or tuple/list of a single integer, specifying\n",
    "                the dilation rate to use for dilated convolution.\n",
    "                Usually dilation rate increases exponentially with the depth of the network.\n",
    "            activation: Activation function to use\n",
    "                If you don't specify anything, no activation is applied\n",
    "                (ie. \"linear\" activation: `a(x) = x`).\n",
    "            use_bias: Boolean, whether the layer uses a bias vector.\n",
    "            kernel_initializer: Initializer for the `kernel` weights matrix\n",
    "            bias_initializer: Initializer for the bias vector\n",
    "            kernel_regularizer: Regularizer function applied to the `kernel` weights matrix\n",
    "            bias_regularizer: Regularizer function applied to the bias vector\n",
    "                (see [regularizer](../regularizers.md)).\n",
    "        # Input shape\n",
    "            3D tensor with shape: `(batch, steps, n_features)`\n",
    "        # Output shape\n",
    "            3D tensor with shape: `(batch, steps, filters)`\n",
    "        \"\"\"\n",
    "        super(TCNBlock, self).__init__(**kwargs)\n",
    "        self.filters = filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.dilation_rate = dilation_rate\n",
    "\n",
    "        # Capture feature set from the input\n",
    "        self.conv1 = Conv1D(\n",
    "            filters=filters,\n",
    "            kernel_size=kernel_size,\n",
    "            use_bias=use_bias,\n",
    "            bias_initializer=bias_initializer,\n",
    "            bias_regularizer=bias_regularizer,\n",
    "            kernel_initializer=kernel_initializer,\n",
    "            kernel_regularizer=kernel_regularizer,\n",
    "            padding=\"causal\",\n",
    "            dilation_rate=dilation_rate,\n",
    "            activation=\"relu\",\n",
    "            name=f\"Conv1D_1_{id}\",\n",
    "        )\n",
    "\n",
    "        # Spatial dropout is specific to convolutions by dropping an entire timewindow,\n",
    "        # not to rely too heavily on specific features detected by the kernels.\n",
    "        self.dropout1 = SpatialDropout1D(\n",
    "            dropout_rate, trainable=True, name=f\"SpatialDropout1D_1_{id}\"\n",
    "        )\n",
    "        # Capture a higher order feature set from the previous convolution\n",
    "        self.conv2 = Conv1D(\n",
    "            filters=filters,\n",
    "            kernel_size=kernel_size,\n",
    "            use_bias=use_bias,\n",
    "            bias_initializer=bias_initializer,\n",
    "            bias_regularizer=bias_regularizer,\n",
    "            kernel_initializer=kernel_initializer,\n",
    "            kernel_regularizer=kernel_regularizer,\n",
    "            padding=\"causal\",\n",
    "            dilation_rate=dilation_rate,\n",
    "            activation=\"relu\",\n",
    "            name=f\"Conv1D_2_{id}\",\n",
    "        )\n",
    "        self.dropout2 = SpatialDropout1D(\n",
    "            dropout_rate, trainable=True, name=f\"SpatialDropout1D_2_{id}\"\n",
    "        )\n",
    "\n",
    "        # The skip connection is an addition of the input to the block with the output of the second dropout layer.\n",
    "        # Solves vanishing gradient, carries info from earlier layers to later layers, allowing gradients to flow across this alternative path.\n",
    "        # Does not learn direct mappings, but differences (residuals) while keeping temporal context.\n",
    "        # Note how it keeps dims intact with kernel 1.\n",
    "        self.skip_out = Conv1D(\n",
    "            filters=filters,\n",
    "            kernel_size=1,\n",
    "            activation=\"linear\",\n",
    "            name=f\"Conv1D_skipconnection_{id}\",\n",
    "        )\n",
    "        # This is the elementwise add for the residual connection and Conv1d 2's output\n",
    "        self.residual_out = Add(name=f\"residual_Add_{id}\")\n",
    "\n",
    "    def apply_block(self, inputs):\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        # Residual output by adding the inputs back:\n",
    "        skip_out_x = self.skip_out(inputs)\n",
    "        x = self.residual_out([x, skip_out_x])\n",
    "        return x\n",
    "\n",
    "\n",
    "def TCN(\n",
    "    input_shape,\n",
    "    output_horizon=1,\n",
    "    num_filters=32,\n",
    "    num_layers=1,\n",
    "    kernel_size=2,\n",
    "    dilation_rate=2,\n",
    "    kernel_initializer=\"glorot_normal\",\n",
    "    bias_initializer=\"glorot_normal\",\n",
    "    kernel_regularizer=None,\n",
    "    bias_regularizer=None,\n",
    "    use_bias=False,\n",
    "    dropout_rate=0.0,\n",
    "):\n",
    "    \"\"\"\n",
    "    Tensorflow TCN Model builder.\n",
    "    forked and extended from: https://github.com/albertogaspar/dts/blob/master/dts/models/TCN.py\n",
    "    see: https://www.tensorflow.org/api_docs/python/tf/keras/Model\n",
    "    see: https://www.tensorflow.org/guide/keras/making_new_layers_and_models_via_subclassing#the_model_class\n",
    "    see: https://www.tensorflow.org/api_docs/python/tf/keras/regularizers/L2\n",
    "\n",
    "    :param layers: int\n",
    "        Number of layers for the network. Defaults to 1 layer.\n",
    "    :param filters: int\n",
    "        the number of output filters in the convolution. Defaults to 32.\n",
    "    :param kernel_size: int or tuple\n",
    "        the length of the 1D convolution window\n",
    "    :param dilation_rate: int\n",
    "        the dilation rate to use for dilated convolution. Defaults to 1.\n",
    "    :param output_horizon: int\n",
    "        the output horizon.\n",
    "    \"\"\"\n",
    "    x = inputs = Input(shape=input_shape)\n",
    "    for i in range(num_layers):\n",
    "        block = TCNBlock(\n",
    "            filters=num_filters,\n",
    "            kernel_size=kernel_size,\n",
    "            dilation_rate=dilation_rate**i,\n",
    "            kernel_initializer=kernel_initializer,\n",
    "            bias_initializer=bias_initializer,\n",
    "            kernel_regularizer=kernel_regularizer,\n",
    "            bias_regularizer=bias_regularizer,\n",
    "            use_bias=use_bias,\n",
    "            dropout_rate=dropout_rate,\n",
    "            id=i,\n",
    "        )\n",
    "        x = block.apply_block(x)\n",
    "    # Selects the last timestep and predict the demand in the 1 DIM layer.\n",
    "    x = Lambda(lambda x: x[:, -1:, 0], name=\"lambda_last_timestep\")(x)\n",
    "    outputs = Dense(output_horizon, name=\"Dense_singleoutput\")(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs, name=\"TCN\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b86ab8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume'], dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp500_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e5751b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding Widows: 100%|██████████| 2763/2763 [00:01<00:00, 1766.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FEATURES: ['Adj Close', 'Open', 'High', 'Low', 'Volume', 'month_sin', 'month_cos'], TARGET: 'Adj Close', window: 5, horizon: 1\n",
      "Shape unencoded (including target label and superflous features): (2768, 6)\n",
      "Shape encoded (window and selected exog features only): (2763, 5, 7)\n",
      "Label shape encoded: (2763,)\n",
      "Data shape: (2763, 5, 7)\n",
      "First window exog normalized: [[-1.50244939 -1.54041171 -1.50813103 -1.53370547  0.32189605  0.5\n",
      "   0.8660254 ]\n",
      " [-1.5056777  -1.50205755 -1.50493467 -1.50242615 -0.07017848  0.5\n",
      "   0.8660254 ]\n",
      " [-1.49816263 -1.50528634 -1.50233769 -1.49873757 -0.49650061  0.5\n",
      "   0.8660254 ]\n",
      " [-1.50301039 -1.49777019 -1.50388324 -1.50126421 -0.62196624  0.5\n",
      "   0.8660254 ]\n",
      " [-1.50802755 -1.50261867 -1.50869894 -1.50657332 -0.31005836  0.5\n",
      "   0.8660254 ]]\n",
      "First window targets: [-1.5039313]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5, 7)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def encode_timewindows(data_df, features, target, window_size, horizon):\n",
    "    \"\"\"\n",
    "    Create input and target windows suitable for TCN model.\n",
    "\n",
    "    :param data: DataFrame with shape (n_samples, n_features)\n",
    "    :param features: List of strings, names of the feature columns\n",
    "    :param target: String, name of the target column\n",
    "    :param window_size: int, length of the input sequence.\n",
    "    :param horizon: int, forecasting horizon.\n",
    "    :return: Array in the shape of (n_samples, n_steps, n_features)\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    for i in tqdm(\n",
    "        range(len(data_df) - window_size - horizon + 1), desc=\"Encoding Widows\"\n",
    "    ):\n",
    "        input_window = data_df[features].iloc[i : i + window_size].values\n",
    "        X.append(input_window)\n",
    "\n",
    "        # Target window, note it predicts {horizon} steps ahead\n",
    "        if horizon == 1:\n",
    "            target_value = data_df[target].iloc[i + window_size]\n",
    "        else:\n",
    "            target_value = (\n",
    "                data_df[target].iloc[i + window_size : i + window_size + horizon].values\n",
    "            )\n",
    "        y.append(target_value)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "\n",
    "MONTH_SINE = \"month_sin\"\n",
    "MONTH_COS = \"month_cos\"\n",
    "\n",
    "\n",
    "def encode_cyclics(data_df):\n",
    "    \"\"\"\n",
    "    Encodes time cyclic features for a dataset with monthly sampling.\n",
    "    Assuming we can capture the yearly periodicity by encoding the month as a wave.\n",
    "    See: https://www.tensorflow.org/tutorials/structured_data/time_series#time\n",
    "    :param data_df: The timeseries with a date in the format YYYY-DD-mm as index.\n",
    "    :return: data_df with 2 new wave features.\n",
    "    \"\"\"\n",
    "    months = data_df.index.month\n",
    "\n",
    "    data_df[MONTH_SINE] = np.sin(2 * np.pi * months / 12)\n",
    "    data_df[MONTH_COS] = np.cos(2 * np.pi * months / 12)\n",
    "    return data_df\n",
    "\n",
    "\n",
    "TARGET = \"Adj Close\"\n",
    "FEATURES = [TARGET, \"Open\", \"High\", \"Low\", \"Volume\"]\n",
    "INDEX = \"Date\"\n",
    "WINDOW_SIZE = 1 * 5  # 1 week 8hrs trading\n",
    "EXT_FEATURES = FEATURES + [MONTH_SINE, MONTH_COS]\n",
    "PREDICTION_HORIZON = 1  # next 1 day\n",
    "\n",
    "\n",
    "def prepare_data_and_windows(data_df, window=WINDOW_SIZE, horizon=PREDICTION_HORIZON):\n",
    "    \"\"\"\n",
    "    Utility function to prepare the data.\n",
    "    :data_df dataframe: dataframe with `window_size` months of data to predict the `window_size`+`horizon`.\n",
    "    :param window_size: int, length of the input sequence\n",
    "    :param horizon: int, forecasting horizon, defaults to 1\n",
    "    :return: Array in the shape of (n_samples, n_steps, n_features)\n",
    "\n",
    "    \"\"\"\n",
    "    normalizer = Normalization(axis=-1)\n",
    "\n",
    "    normalizer.adapt(data_df[FEATURES])\n",
    "    data_df_normalized = normalizer(data_df[FEATURES])\n",
    "    data_df_normalized = pd.DataFrame(\n",
    "        data_df_normalized, columns=FEATURES, index=data_df.index\n",
    "    )\n",
    "    data_df_normalized = encode_cyclics(data_df_normalized)\n",
    "    X, y = encode_timewindows(\n",
    "        data_df_normalized,\n",
    "        EXT_FEATURES,\n",
    "        TARGET,\n",
    "        window,\n",
    "        horizon,\n",
    "    )\n",
    "    print(\n",
    "        f\"FEATURES: {EXT_FEATURES}, TARGET: '{TARGET}', window: {WINDOW_SIZE}, horizon: {PREDICTION_HORIZON}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Shape unencoded (including target label and superflous features): {data_df.shape}\"\n",
    "    )\n",
    "    print(f\"Shape encoded (window and selected exog features only): {X.shape}\")\n",
    "\n",
    "    return X, y, normalizer\n",
    "\n",
    "\n",
    "# Yes, we are using the whole dataset not the training dataset.\n",
    "# See: https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit\n",
    "# we will tell keras to do a validation split, it will not fit on the validation data.\n",
    "X, y, normalizer = prepare_data_and_windows(sp500_df)\n",
    "print(f\"Label shape encoded: {y.shape}\")\n",
    "print(f\"Data shape: {X.shape}\")\n",
    "print(f\"First window exog normalized: {X[0,  :]}\")\n",
    "print(f\"First window targets: {y[:1]}\")\n",
    "\n",
    "input_shape = (WINDOW_SIZE, X.shape[2])\n",
    "input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f481f001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model logs for Tensorboard available here: ./logs/0126-204333\n",
      "X: (2763, 5, 7), y: (2763,), input: (5, 7)\n",
      "Epoch 1/300\n",
      "78/78 [==============================] - 16s 167ms/step - loss: 4.2254 - mse: 0.1528 - mae: 0.2589 - mape: 152.7421 - val_loss: 3.7808 - val_mse: 0.1359 - val_mae: 0.3511 - val_mape: 24.0082\n",
      "Epoch 2/300\n",
      "78/78 [==============================] - 11s 143ms/step - loss: 3.3534 - mse: 0.0413 - mae: 0.1524 - mape: 78.3344 - val_loss: 3.0064 - val_mse: 0.0114 - val_mae: 0.0882 - val_mape: 6.1925\n",
      "Epoch 3/300\n",
      "78/78 [==============================] - 12s 150ms/step - loss: 2.7695 - mse: 0.0344 - mae: 0.1371 - mape: 60.1703 - val_loss: 2.5305 - val_mse: 0.0435 - val_mae: 0.1888 - val_mape: 13.1734\n",
      "Epoch 4/300\n",
      "78/78 [==============================] - 13s 173ms/step - loss: 2.2947 - mse: 0.0136 - mae: 0.0894 - mape: 52.6781 - val_loss: 2.1178 - val_mse: 0.0342 - val_mae: 0.1710 - val_mape: 12.0132\n",
      "Epoch 5/300\n",
      "78/78 [==============================] - 14s 181ms/step - loss: 1.9313 - mse: 0.0128 - mae: 0.0857 - mape: 48.5192 - val_loss: 1.7724 - val_mse: 0.0125 - val_mae: 0.0981 - val_mape: 6.9895\n",
      "Epoch 6/300\n",
      "78/78 [==============================] - 13s 172ms/step - loss: 1.6305 - mse: 0.0039 - mae: 0.0472 - mape: 35.2710 - val_loss: 1.5044 - val_mse: 0.0064 - val_mae: 0.0673 - val_mape: 4.8361\n",
      "Epoch 7/300\n",
      "78/78 [==============================] - 13s 162ms/step - loss: 1.3922 - mse: 0.0026 - mae: 0.0366 - mape: 33.3004 - val_loss: 1.2890 - val_mse: 0.0041 - val_mae: 0.0526 - val_mape: 3.8179\n",
      "Epoch 8/300\n",
      "78/78 [==============================] - 12s 153ms/step - loss: 1.1983 - mse: 0.0021 - mae: 0.0315 - mape: 32.0264 - val_loss: 1.1135 - val_mse: 0.0031 - val_mae: 0.0449 - val_mape: 3.2835\n",
      "Epoch 9/300\n",
      "78/78 [==============================] - 13s 160ms/step - loss: 1.0393 - mse: 0.0020 - mae: 0.0297 - mape: 32.7816 - val_loss: 0.9693 - val_mse: 0.0027 - val_mae: 0.0411 - val_mape: 3.0124\n",
      "Epoch 10/300\n",
      "78/78 [==============================] - 12s 158ms/step - loss: 0.9081 - mse: 0.0019 - mae: 0.0295 - mape: 32.1242 - val_loss: 0.8502 - val_mse: 0.0028 - val_mae: 0.0418 - val_mape: 3.0308\n",
      "Epoch 11/300\n",
      "78/78 [==============================] - 14s 174ms/step - loss: 0.7992 - mse: 0.0021 - mae: 0.0310 - mape: 33.5590 - val_loss: 0.7524 - val_mse: 0.0044 - val_mae: 0.0534 - val_mape: 3.7637\n",
      "Epoch 12/300\n",
      "78/78 [==============================] - 14s 177ms/step - loss: 0.7085 - mse: 0.0028 - mae: 0.0351 - mape: 35.5196 - val_loss: 0.6741 - val_mse: 0.0098 - val_mae: 0.0836 - val_mape: 5.7536\n",
      "Epoch 13/300\n",
      "78/78 [==============================] - 13s 164ms/step - loss: 0.6330 - mse: 0.0044 - mae: 0.0435 - mape: 40.5583 - val_loss: 0.6130 - val_mse: 0.0195 - val_mae: 0.1237 - val_mape: 8.4604\n",
      "Epoch 14/300\n",
      "78/78 [==============================] - 13s 169ms/step - loss: 0.5707 - mse: 0.0075 - mae: 0.0570 - mape: 43.8456 - val_loss: 0.5580 - val_mse: 0.0245 - val_mae: 0.1405 - val_mape: 9.6021\n",
      "Epoch 15/300\n",
      "78/78 [==============================] - 13s 163ms/step - loss: 0.5213 - mse: 0.0138 - mae: 0.0793 - mape: 46.5532 - val_loss: 0.4912 - val_mse: 0.0090 - val_mae: 0.0756 - val_mape: 5.2500\n",
      "Epoch 16/300\n",
      "78/78 [==============================] - 13s 168ms/step - loss: 0.4826 - mse: 0.0227 - mae: 0.1064 - mape: 53.1549 - val_loss: 0.4450 - val_mse: 0.0066 - val_mae: 0.0674 - val_mape: 4.8116\n",
      "Epoch 17/300\n",
      "78/78 [==============================] - 14s 176ms/step - loss: 0.4459 - mse: 0.0267 - mae: 0.1162 - mape: 57.9041 - val_loss: 0.4426 - val_mse: 0.0419 - val_mae: 0.1942 - val_mape: 13.5969\n",
      "Epoch 18/300\n",
      "78/78 [==============================] - 13s 160ms/step - loss: 0.4020 - mse: 0.0181 - mae: 0.0989 - mape: 58.2210 - val_loss: 0.3986 - val_mse: 0.0311 - val_mae: 0.1675 - val_mape: 11.7976\n",
      "Epoch 19/300\n",
      "78/78 [==============================] - 14s 178ms/step - loss: 0.3592 - mse: 0.0064 - mae: 0.0607 - mape: 46.7103 - val_loss: 0.3473 - val_mse: 0.0093 - val_mae: 0.0858 - val_mape: 6.1600\n",
      "Epoch 20/300\n",
      "78/78 [==============================] - 13s 169ms/step - loss: 0.3277 - mse: 0.0031 - mae: 0.0420 - mape: 38.8547 - val_loss: 0.3156 - val_mse: 0.0044 - val_mae: 0.0554 - val_mape: 4.0361\n",
      "Epoch 21/300\n",
      "78/78 [==============================] - 16s 206ms/step - loss: 0.3010 - mse: 0.0019 - mae: 0.0314 - mape: 35.5781 - val_loss: 0.2898 - val_mse: 0.0028 - val_mae: 0.0423 - val_mape: 3.1253\n",
      "Epoch 22/300\n",
      "78/78 [==============================] - 15s 198ms/step - loss: 0.2776 - mse: 0.0016 - mae: 0.0280 - mape: 33.3377 - val_loss: 0.2671 - val_mse: 0.0022 - val_mae: 0.0376 - val_mape: 2.7841\n",
      "Epoch 23/300\n",
      "78/78 [==============================] - 14s 183ms/step - loss: 0.2562 - mse: 0.0015 - mae: 0.0257 - mape: 31.2987 - val_loss: 0.2466 - val_mse: 0.0020 - val_mae: 0.0358 - val_mape: 2.6605\n",
      "Epoch 24/300\n",
      "78/78 [==============================] - 13s 171ms/step - loss: 0.2367 - mse: 0.0014 - mae: 0.0244 - mape: 30.4627 - val_loss: 0.2279 - val_mse: 0.0019 - val_mae: 0.0348 - val_mape: 2.5740\n",
      "Epoch 25/300\n",
      "78/78 [==============================] - 15s 187ms/step - loss: 0.2188 - mse: 0.0014 - mae: 0.0240 - mape: 28.9264 - val_loss: 0.2107 - val_mse: 0.0019 - val_mae: 0.0347 - val_mape: 2.5624\n",
      "Epoch 26/300\n",
      "78/78 [==============================] - 14s 176ms/step - loss: 0.2024 - mse: 0.0014 - mae: 0.0241 - mape: 29.5879 - val_loss: 0.1949 - val_mse: 0.0018 - val_mae: 0.0345 - val_mape: 2.5425\n",
      "Epoch 27/300\n",
      "78/78 [==============================] - 12s 154ms/step - loss: 0.1872 - mse: 0.0015 - mae: 0.0247 - mape: 30.3997 - val_loss: 0.1803 - val_mse: 0.0019 - val_mae: 0.0348 - val_mape: 2.5436\n",
      "Epoch 28/300\n",
      "78/78 [==============================] - 12s 156ms/step - loss: 0.1733 - mse: 0.0016 - mae: 0.0261 - mape: 31.4026 - val_loss: 0.1670 - val_mse: 0.0020 - val_mae: 0.0358 - val_mape: 2.5957\n",
      "Epoch 29/300\n",
      "78/78 [==============================] - 12s 159ms/step - loss: 0.1606 - mse: 0.0019 - mae: 0.0286 - mape: 32.2221 - val_loss: 0.1548 - val_mse: 0.0023 - val_mae: 0.0383 - val_mape: 2.7457\n",
      "Epoch 30/300\n",
      "78/78 [==============================] - 24s 303ms/step - loss: 0.1490 - mse: 0.0022 - mae: 0.0315 - mape: 31.9512 - val_loss: 0.1440 - val_mse: 0.0029 - val_mae: 0.0438 - val_mape: 3.0963\n",
      "Epoch 31/300\n",
      "78/78 [==============================] - 17s 223ms/step - loss: 0.1385 - mse: 0.0028 - mae: 0.0356 - mape: 32.6318 - val_loss: 0.1342 - val_mse: 0.0038 - val_mae: 0.0506 - val_mape: 3.5339\n",
      "Epoch 32/300\n",
      "71/78 [==========================>...] - ETA: 1s - loss: 0.1283 - mse: 0.0024 - mae: 0.0343 - mape: 35.4309"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 125\u001b[0m\n\u001b[0;32m    112\u001b[0m     history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[0;32m    113\u001b[0m         X,\n\u001b[0;32m    114\u001b[0m         y,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    120\u001b[0m         verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m    121\u001b[0m     )\n\u001b[0;32m    122\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model, history\n\u001b[1;32m--> 125\u001b[0m model, history \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_tcn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_shape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    126\u001b[0m model\u001b[38;5;241m.\u001b[39msummary()\n",
      "Cell \u001b[1;32mIn[6], line 112\u001b[0m, in \u001b[0;36mbuild_tcn\u001b[1;34m(X, y, input_shape)\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;66;03m# Paper's `patience` was 50, we limited to 25 and watch the MAPE\u001b[39;00m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;66;03m# see: https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping\u001b[39;00m\n\u001b[0;32m    108\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    109\u001b[0m     EarlyStopping(patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m25\u001b[39m, monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_mape\u001b[39m\u001b[38;5;124m\"\u001b[39m, restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[0;32m    110\u001b[0m     TensorBoard(log_dir\u001b[38;5;241m=\u001b[39mMODEL_LOG_DIR),\n\u001b[0;32m    111\u001b[0m ]\n\u001b[1;32m--> 112\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    113\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    114\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    115\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mVAL_SPLIT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    116\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    117\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    119\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    120\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    121\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model, history\n",
      "File \u001b[1;32mc:\\Users\\adamd\\.conda\\envs\\quant\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\adamd\\.conda\\envs\\quant\\lib\\site-packages\\keras\\engine\\training.py:1564\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1556\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1558\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1561\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1562\u001b[0m ):\n\u001b[0;32m   1563\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1564\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1565\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1566\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\adamd\\.conda\\envs\\quant\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\adamd\\.conda\\envs\\quant\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\adamd\\.conda\\envs\\quant\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateless_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\adamd\\.conda\\envs\\quant\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2493\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m   2494\u001b[0m   (graph_function,\n\u001b[0;32m   2495\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2496\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\adamd\\.conda\\envs\\quant\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1858\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1859\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1860\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1861\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1862\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1863\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1864\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1865\u001b[0m     args,\n\u001b[0;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1867\u001b[0m     executing_eagerly)\n\u001b[0;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\adamd\\.conda\\envs\\quant\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\Users\\adamd\\.conda\\envs\\quant\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.regularizers import L2\n",
    "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "import json\n",
    "\n",
    "VAL_SPLIT = 0.1\n",
    "EPOCHS = 300\n",
    "BATCH_SIZE = 32\n",
    "FILTER = 128\n",
    "DROPRATE = 0.5\n",
    "POOL_SIZE = 2\n",
    "KERNEL_SIZE = 4\n",
    "DILATION_RATE = 4\n",
    "MAX_LAYERS = 4\n",
    "L2_REG = 0.005\n",
    "LEARN_RATE = 0.0001\n",
    "MODEL_LOG_DIR = f'./logs/{datetime.now().strftime(\"%m%d-%H%M%S\")}'\n",
    "# See: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.ParameterGrid.html\n",
    "# See paper: https://www.mdpi.com/2076-3417/10/7/2322\n",
    "GRID = {\n",
    "    \"num_filters\": [32, 64, 128],\n",
    "    \"kernel_size\": [2, 3, 4],\n",
    "    \"batch_size\": [64, 128, 255],\n",
    "    \"epochs\": [25, 50, 100, 300],\n",
    "    \"dilation_rate\": [1, 2, 4],\n",
    "    \"dropout_rate\": [0.1, 0.2, 0.3],\n",
    "    \"num_layers\": [6, 5, 3],\n",
    "    \"l2_reg\": [0.005, 0.001, 0.01],\n",
    "    \"learning_rate\": [0.001, 0.01, 0.1],\n",
    "}\n",
    "\n",
    "print(f\"Model logs for Tensorboard available here: {MODEL_LOG_DIR}\")\n",
    "\n",
    "\n",
    "def grid_search(X, y, param_grid=GRID, file_name=\"best_params.json\"):\n",
    "    \"\"\"Runs for 3 days!!!\"\"\"\n",
    "\n",
    "    def _create_model(hyperparams):\n",
    "        model = TCN(\n",
    "            input_shape=input_shape,\n",
    "            output_horizon=PREDICTION_HORIZON,\n",
    "            num_filters=hyperparams[\"num_filters\"],\n",
    "            kernel_size=hyperparams[\"kernel_size\"],\n",
    "            num_layers=hyperparams[\"num_layers\"],\n",
    "            dilation_rate=hyperparams[\"dilation_rate\"],\n",
    "            kernel_regularizer=L2(l2=hyperparams[\"l2_reg\"]),\n",
    "            bias_regularizer=L2(l2=hyperparams[\"l2_reg\"]),\n",
    "            dropout_rate=hyperparams[\"dropout_rate\"],\n",
    "        )\n",
    "        optimizer = Adam(hyperparams[\"learning_rate\"])\n",
    "        model.compile(loss=\"mse\", optimizer=optimizer, metrics=[\"mse\", \"mae\", \"mape\"])\n",
    "        return model\n",
    "\n",
    "    def _save_best_params(best_params, best_loss, file_name=\"best_params.json\"):\n",
    "        with open(file_name, \"w\") as file:\n",
    "            json.dump({\"best_params\": best_params, \"best_loss\": best_loss}, file)\n",
    "\n",
    "    grid = list(ParameterGrid(param_grid))\n",
    "    best_model = None\n",
    "    best_loss = np.inf\n",
    "    best_params = None\n",
    "\n",
    "    for params in tqdm(grid, desc=\"Grid Search..\"):\n",
    "        model = _create_model(params)\n",
    "        callbacks = [EarlyStopping(patience=10, monitor=\"val_loss\")]\n",
    "        history = model.fit(\n",
    "            X,\n",
    "            y,\n",
    "            epochs=EPOCHS,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            validation_split=VAL_SPLIT,\n",
    "            verbose=0,\n",
    "            callbacks=callbacks,\n",
    "        )\n",
    "        val_loss = np.min(history.history[\"val_loss\"])\n",
    "\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            best_model = model\n",
    "            best_params = params\n",
    "            _save_best_params(best_params, best_loss, file_name)\n",
    "\n",
    "    return best_model, best_loss\n",
    "\n",
    "\n",
    "def build_tcn(X, y, input_shape):\n",
    "    print(f\"X: {X.shape}, y: {y.shape}, input: {input_shape}\")\n",
    "    model = TCN(\n",
    "        input_shape=input_shape,\n",
    "        output_horizon=PREDICTION_HORIZON,\n",
    "        num_filters=FILTER,\n",
    "        kernel_size=KERNEL_SIZE,\n",
    "        num_layers=MAX_LAYERS,\n",
    "        dilation_rate=DILATION_RATE,\n",
    "        kernel_regularizer=L2(l2=L2_REG),\n",
    "        bias_regularizer=L2(l2=L2_REG),\n",
    "    )\n",
    "\n",
    "    # See: https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam\n",
    "    optimizer = Adam(LEARN_RATE)\n",
    "    metrics = [\"mse\", \"mae\", \"mape\"]\n",
    "    model.compile(loss=\"mse\", optimizer=optimizer, metrics=metrics)\n",
    "\n",
    "    # Paper's `patience` was 50, we limited to 25 and watch the MAPE\n",
    "    # see: https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping\n",
    "    callbacks = [\n",
    "        EarlyStopping(patience=25, monitor=\"val_mape\", restore_best_weights=True),\n",
    "        TensorBoard(log_dir=MODEL_LOG_DIR),\n",
    "    ]\n",
    "    history = model.fit(\n",
    "        X,\n",
    "        y,\n",
    "        validation_split=VAL_SPLIT,\n",
    "        shuffle=False,\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1,\n",
    "    )\n",
    "    return model, history\n",
    "\n",
    "\n",
    "model, history = build_tcn(X, y, input_shape)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b862085f",
   "metadata": {},
   "outputs": [],
   "source": [
    "VAL_SIZE = round(len(X) * VAL_SPLIT)\n",
    "\n",
    "train_data = X[:-VAL_SIZE]\n",
    "test_data = X[-VAL_SIZE:]\n",
    "ytrain_data = y[:-VAL_SIZE]\n",
    "ytest_data = y[-VAL_SIZE:]\n",
    "print(ytest_data.shape)\n",
    "print(ytest_data)\n",
    "\n",
    "y_pred = model.predict(train_data)\n",
    "yt_pred = model.predict(test_data)\n",
    "\n",
    "print(yt_pred.shape)\n",
    "print(yt_pred.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf9703b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    r2_score,\n",
    "    mean_absolute_percentage_error,\n",
    "    mean_absolute_error,\n",
    "    mean_squared_error\n",
    ")\n",
    "\n",
    "def wape(actual_values, forecast_values):\n",
    "    \"\"\"\n",
    "    Calculate Weighted Absolute Percentage Error (WAPE).\n",
    "    see:  https://www.obviously.ai/post/introducing-wape\n",
    "    Parameters:\n",
    "    - actual_values: numpy array, actual values\n",
    "    - forecast_values: numpy array, predicted values\n",
    "\n",
    "    Returns:\n",
    "    - wape: float, WAPE value\n",
    "    \"\"\"\n",
    "    total_actual = np.sum(actual_values)\n",
    "    if total_actual == 0:\n",
    "        return np.nan\n",
    "    absolute_errors = np.abs(actual_values - forecast_values)\n",
    "    wape = np.sum(absolute_errors) / total_actual\n",
    "    return wape\n",
    "\n",
    "print(f\"shapes y_pred: {y_pred.shape} and yt_pred: {yt_pred.shape}\")\n",
    "mae_train = mean_absolute_error(ytrain_data, y_pred)\n",
    "mae_test = mean_absolute_error(ytest_data, yt_pred)\n",
    "mse_train = mean_squared_error(ytrain_data, y_pred)\n",
    "mse_test = mean_squared_error(ytest_data, yt_pred)\n",
    "rmse_train = mean_squared_error(ytrain_data, y_pred, squared=False)\n",
    "rmse_test = mean_squared_error(ytest_data, yt_pred, squared=False)\n",
    "mape_train = mean_absolute_percentage_error(ytrain_data, y_pred) * 100\n",
    "mape_test = mean_absolute_percentage_error(ytest_data, yt_pred) * 100\n",
    "wape_train = wape(ytrain_data, y_pred)\n",
    "wape_test = wape(ytest_data, yt_pred)\n",
    "r2 = r2_score(\n",
    "    ytest_data,\n",
    "    yt_pred,\n",
    ")\n",
    "\n",
    "metrics_df = pd.DataFrame(\n",
    "    {\n",
    "        \"MAE\": mae_test,\n",
    "        \"MSE\": mse_test,\n",
    "        \"RMSE\": rmse_test,\n",
    "        \"MAPE\": mape_test,\n",
    "        \"WAPE\": wape_test,\n",
    "        \"R2\": r2,\n",
    "    }\n",
    ")\n",
    "fig, axs = plt.subplots(3, 1, figsize=(10, 10))\n",
    "axs[0].plot(history.history[\"loss\"], label=\"Train loss\")\n",
    "axs[0].plot(history.history[\"val_loss\"], label=\"Validation loss\")\n",
    "axs[0].set_xlabel(\"Epochs\")\n",
    "axs[0].set_ylabel(\"Loss\")\n",
    "axs[0].legend()\n",
    "axs[1].plot(\n",
    "    history.history[\"mse\"],\n",
    "    label=\"Train MSE\",\n",
    ")\n",
    "\n",
    "axs[1].plot(\n",
    "    history.history[\"val_mse\"],\n",
    "    label=\"Test MSE\",\n",
    ")\n",
    "\n",
    "axs[1].set_ylim((0, 0.1))\n",
    "\n",
    "axs[1].axhline(rmse_train, color=\"r\", linestyle=\"--\", label=\"Train Sample RMSE\")\n",
    "axs[1].axhline(rmse_test, color=\"r\", linestyle=\"-\", label=\"Test Sample RMSE\")\n",
    "axs[1].set_xlabel(\"Epochs\")\n",
    "axs[1].set_ylabel(\"Error\")\n",
    "axs[1].legend()\n",
    "axs[2].plot(\n",
    "    history.history[\"mape\"],\n",
    "    label=\"Train MAPE\",\n",
    ")\n",
    "axs[2].plot(\n",
    "    history.history[\"val_mape\"],\n",
    "    label=\"Test MAPE\",\n",
    ")\n",
    "axs[2].axhline(\n",
    "    wape_train, color=\"b\", linestyle=\"--\", label=\"Train Sample WAPE\", alpha=0.5\n",
    ")\n",
    "\n",
    "axs[2].axhline(wape_test, color=\"b\", linestyle=\"-\", label=\"Test Sample WAPE\", alpha=0.5)\n",
    "axs[2].set_ylim((0, 40))\n",
    "axs[2].set_xlabel(\"Epochs\")\n",
    "axs[2].set_ylabel(\"Error\")\n",
    "axs[2].legend()\n",
    "plt.show()\n",
    "\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e9a9c5",
   "metadata": {
    "papermill": {
     "duration": 0.037345,
     "end_time": "2023-11-05T18:01:12.947046",
     "exception": false,
     "start_time": "2023-11-05T18:01:12.909701",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "![]()\n",
    "\n",
    "## References\n",
    "\n",
    "- [YFinance Github](https://github.com/ranaroussi/yfinance)\n",
    "- [Vanguard All World excluding US](https://investor.vanguard.com/investment-products/etfs/profile/veu)\n",
    "\n",
    "\n",
    "## Github\n",
    "\n",
    "Article here is also available on [Github]()\n",
    "\n",
    "Kaggle notebook available [here]()\n",
    "\n",
    "\n",
    "## Media\n",
    "\n",
    "All media used (in the form of code or images) are either solely owned by me, acquired through licensing, or part of the Public Domain and granted use through Creative Commons License.\n",
    "\n",
    "## CC Licensing and Use\n",
    "\n",
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc/4.0/\"><img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by-nc/4.0/88x31.png\" /></a><br />This work is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc/4.0/\">Creative Commons Attribution-NonCommercial 4.0 International License</a>."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 98.905855,
   "end_time": "2023-11-05T18:01:13.607303",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-11-05T17:59:34.701448",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
